# Nyaya Backend - Setup & Quick Start Guide

## âœ… Implementation Complete!

The production FastAPI backend with LangGraph multi-agent orchestration is now fully implemented.

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py                 âœ… Package initialization
â”‚   â”œâ”€â”€ main.py                     âœ… FastAPI application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py            âœ… API package
â”‚   â”‚   â””â”€â”€ routes.py              âœ… REST endpoints (/upload, /query, /search, /predict)
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py            âœ… Agents package
â”‚   â”‚   â”œâ”€â”€ orchestrator.py        âœ… LangGraph multi-agent coordinator
â”‚   â”‚   â”œâ”€â”€ classification_agent.py âœ… Document upload & classification
â”‚   â”‚   â”œâ”€â”€ similarity_agent.py    âœ… Find similar cases
â”‚   â”‚   â”œâ”€â”€ prediction_agent.py    âœ… Outcome prediction
â”‚   â”‚   â””â”€â”€ rag_agent.py           âœ… Role-aware Q&A
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py            âœ… Core package
â”‚   â”‚   â””â”€â”€ config.py              âœ… Pydantic settings
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py            âœ… Models package
â”‚   â”‚   â””â”€â”€ schemas.py             âœ… Pydantic models (26 models total)
â”‚   â”‚
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py            âœ… Services package
â”‚       â”œâ”€â”€ intent_detection.py    âœ… Rule-based intent routing (no LLM)
â”‚       â”œâ”€â”€ preprocessing.py       âœ… Document preprocessing
â”‚       â”œâ”€â”€ classification_service.py âœ… InLegalBERT wrapper
â”‚       â”œâ”€â”€ embedding_service.py   âœ… EmbeddingGemma wrapper
â”‚       â”œâ”€â”€ pinecone_service.py    âœ… Pinecone operations
â”‚       â””â”€â”€ context_manager.py     âœ… Session management
â”‚
â”œâ”€â”€ .env.example                    âœ… Environment template
â”œâ”€â”€ .env                            âš ï¸  YOU NEED TO CREATE THIS
â””â”€â”€ README.md                       âœ… Full documentation
```

## ğŸš€ Quick Start (3 Steps)

### Step 1: Setup Environment Variables

```bash
cd backend

# Copy template
cp .env.example .env

# Edit .env with your actual keys
nano .env  # or use your favorite editor
```

**Required keys in `.env`:**
```bash
# Pinecone (get from: https://app.pinecone.io/)
PINECONE_API_KEY=pcsk_xxxxx...

# Hugging Face (get from: https://huggingface.co/settings/tokens)
# Accept license at: https://huggingface.co/google/embeddinggemma-300M
HF_TOKEN=hf_xxxxx...

# Google Cloud (for Vertex AI Gemini)
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Model path (you need to provide trained model or train it)
CLASSIFIER_MODEL_PATH=./models/inlegalbert_classifier.pt
```

### Step 2: Ensure Virtual Environment is Activated

```bash
# Make sure you're in the project root
cd /home/uttam/B.Tech\ Major\ Project/nyaya

# Activate virtual environment
source .venv/bin/activate

# Verify dependencies are installed
python -c "import fastapi, langgraph, pinecone; print('âœ… Dependencies OK')"
```

### Step 3: Run the Server

```bash
cd backend

# Option 1: Using uvicorn directly (recommended)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Option 2: Using Python module
python -m app.main

# Option 3: If you want background mode
nohup uvicorn app.main:app --host 0.0.0.0 --port 8000 &
```

## ğŸŒ Access the API

Once running, you can access:

- **API Documentation (Swagger)**: http://localhost:8000/docs
- **Alternative Docs (ReDoc)**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/api/v1/health
- **Statistics**: http://localhost:8000/api/v1/stats

## ğŸ“ API Endpoints Overview

### Session Management
```bash
# Create a new session
curl -X POST http://localhost:8000/api/v1/sessions
# Returns: {"session_id": "uuid...", "message": "Session created successfully"}
```

### Document Upload & Classification
```bash
# Upload PDF or TXT
curl -X POST http://localhost:8000/api/v1/upload \
  -F "file=@judgment.pdf" \
  -F "session_id=your-session-id" \
  -F "case_id=case_001"
```

### Role-Aware Question Answering
```bash
# Ask questions about uploaded documents
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What were the facts of the case?",
    "session_id": "your-session-id"
  }'
```

### Find Similar Cases
```bash
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "breach of contract cases",
    "session_id": "your-session-id",
    "top_k": 5
  }'
```

### Predict Outcome
```bash
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{
    "case_description": "Petitioner challenges...",
    "relevant_laws": ["Article 14"],
    "session_id": "your-session-id"
  }'
```

## âš ï¸ Important: Model Requirements

### 1. InLegalBERT Classifier (Required)

You need a trained InLegalBERT model at `backend/models/inlegalbert_classifier.pt`

**Option A: Train your own model**
```bash
# Use the training notebooks in docs/
# See: docs/REMOTE_GPU_TRAINING_GUIDE.md
```

**Option B: Use a pre-trained model**
```bash
# If you have a pre-trained model:
mkdir -p backend/models
cp /path/to/your/inlegalbert_classifier.pt backend/models/
```

**Option C: Mock for testing (temporary)**
```python
# For testing API without classifier, you can modify:
# backend/app/services/classification_service.py
# to return mock classifications (not recommended for production)
```

### 2. EmbeddingGemma (Auto-downloaded)

- **First run will download ~1.2GB model from Hugging Face**
- Make sure you have accepted the license at: https://huggingface.co/google/embeddinggemma-300M
- Set `HF_TOKEN` in `.env`

### 3. Vertex AI Gemini (Cloud-based)

- Requires Google Cloud project with Vertex AI API enabled
- Service account with "Vertex AI User" role
- Set `GOOGLE_APPLICATION_CREDENTIALS` in `.env`

## ğŸ”§ Testing the Implementation

### Test 1: Health Check
```bash
# Should return: {"status": "healthy", ...}
curl http://localhost:8000/api/v1/health
```

### Test 2: Stats
```bash
# Should show Pinecone index stats
curl http://localhost:8000/api/v1/stats
```

### Test 3: Create Session
```bash
SESSION_ID=$(curl -s -X POST http://localhost:8000/api/v1/sessions | jq -r '.session_id')
echo "Session ID: $SESSION_ID"
```

### Test 4: Query (without upload)
```bash
# This will work if you have data in Pinecone from previous tests
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d "{
    \"query\": \"Hello, what can you help me with?\",
    \"session_id\": \"$SESSION_ID\"
  }"
```

## ğŸ› Troubleshooting

### Issue: Import errors in VS Code

**Cause:** VS Code Python extension not using the correct virtual environment

**Solution:**
```bash
# 1. Open Command Palette (Ctrl+Shift+P)
# 2. Type: "Python: Select Interpreter"
# 3. Choose: .venv/bin/python

# Or set in VS Code settings:
{
  "python.defaultInterpreterPath": "${workspaceFolder}/.venv/bin/python"
}
```

### Issue: "Model file not found"

**Solution:**
```bash
# You need to train or provide InLegalBERT model
# Temporary workaround: Comment out classifier loading for testing
# backend/app/services/classification_service.py line 39-84
```

### Issue: "Pinecone API key invalid"

**Solution:**
```bash
# Verify key in .env
cat backend/.env | grep PINECONE_API_KEY

# Test Pinecone connection
python -c "
from dotenv import load_dotenv
import os
from pinecone import Pinecone
load_dotenv('backend/.env')
pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
print('âœ… Pinecone connected:', pc.list_indexes())
"
```

### Issue: "Google Cloud credentials not found"

**Solution:**
```bash
# Set path to service account JSON
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

# Verify
gcloud auth application-default print-access-token
```

## ğŸ“Š Performance Expectations

- **Startup time:** 15-30 seconds (loading models)
- **Classification:** 5-10 sentences/sec (GPU), 1-2 sentences/sec (CPU)
- **Embedding:** ~100 sentences/sec
- **RAG query:** 2-5 seconds (depends on Gemini API latency)

## ğŸ”„ Development Workflow

### Hot Reload (Development)
```bash
# Uvicorn with --reload watches for file changes
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode
```bash
# No reload, multiple workers
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

## ğŸ“¦ What's Implemented

âœ… **Services (6 modules):**
- Intent detection (rule-based, no LLM)
- Document preprocessing (PDF/TXT, sentence splitting)
- Classification service (InLegalBERT wrapper)
- Embedding service (EmbeddingGemma with asymmetric encoding)
- Pinecone service (vector storage with role metadata)
- Context manager (session & conversation management)

âœ… **Agents (4 specialized agents + 1 orchestrator):**
- Classification Agent: Upload â†’ Classify â†’ Embed â†’ Store
- Similarity Agent: Find similar cases with role-weighted scoring
- Prediction Agent: Predict outcomes based on precedents
- RAG Agent: Role-aware question answering
- Orchestrator: LangGraph-based routing and coordination

âœ… **API (5 endpoints):**
- POST /api/v1/sessions - Create session
- POST /api/v1/upload - Upload & classify documents
- POST /api/v1/query - Role-aware Q&A
- POST /api/v1/search - Find similar cases
- POST /api/v1/predict - Predict outcomes

âœ… **Data Models (26 Pydantic models):**
- All request/response types
- Enum types (RhetoricalRole, Intent)
- Agent state management
- Session context

âœ… **Configuration:**
- Pydantic settings management
- Environment variable loading
- Production-ready defaults

## ğŸ¯ Next Steps

1. **Create `.env` file** with your actual API keys
2. **Provide InLegalBERT model** or train one
3. **Run the server**: `uvicorn app.main:app --reload`
4. **Test with Swagger UI**: http://localhost:8000/docs
5. **Connect frontend**: Update `client/src/services/` to call backend API

## ğŸ“š Documentation

- **Full README**: `backend/README.md` (detailed API documentation)
- **Architecture**: `backend/ARCHITECTURE.md` (system design)
- **Setup**: `backend/PINECONE_SETUP.md` (Pinecone configuration)
- **Embeddings**: `backend/README_EMBEDDINGS.md` (official patterns)

## ğŸ¤ Integration with Frontend

The frontend (`client/`) is currently using mock data. To connect:

1. **Update API base URL** in `client/src/services/api.ts`:
   ```typescript
   const API_BASE_URL = 'http://localhost:8000/api/v1';
   ```

2. **Replace mock data calls** with actual API requests:
   ```typescript
   // Replace mockData.ts calls with:
   const response = await fetch(`${API_BASE_URL}/query`, {
     method: 'POST',
     headers: {'Content-Type': 'application/json'},
     body: JSON.stringify({query, session_id})
   });
   ```

3. **Start both servers:**
   ```bash
   # Terminal 1: Backend
   cd backend && uvicorn app.main:app --reload

   # Terminal 2: Frontend
   cd client && npm run dev
   ```

## âœ¨ Key Features

- **No LLM for routing**: Intent detection uses fast keyword matching
- **Role-aware retrieval**: Filter by specific rhetorical roles
- **Asymmetric encoding**: Separate prompts for docs vs queries (+16.7% accuracy)
- **Multi-agent**: LangGraph orchestrates specialized agents
- **Session management**: Conversation history and context tracking
- **Production-ready**: FastAPI, Pydantic, proper error handling

---

**Status: READY FOR TESTING** ğŸ‰

All components implemented. Just add your API keys and models!
