{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a900ebd4",
   "metadata": {},
   "source": [
    "# InLegalBERT Role Classifier Training\n",
    "\n",
    "This notebook trains a rhetorical role classifier for Indian legal judgments using **InLegalBERT** as the base model.\n",
    "\n",
    "## Model Information\n",
    "- **Base Model:** `law-ai/InLegalBERT` (pre-trained on 5.4M Indian legal documents)\n",
    "- **Task:** Multi-class sentence classification into 7 rhetorical roles\n",
    "- **Dataset:** `train_final/` directory\n",
    "\n",
    "## The 7 Rhetorical Roles\n",
    "1. **Facts** - Background and case events\n",
    "2. **Issue** - Legal questions to resolve\n",
    "3. **Arguments of Petitioner (AoP)** - Petitioner's claims\n",
    "4. **Arguments of Respondent (AoR)** - Respondent's counter-arguments\n",
    "5. **Reasoning** - Court's legal analysis\n",
    "6. **Decision** - Final judgment\n",
    "7. **None** - Other content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7db019",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618eec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers>=4.35.0 torch>=2.0.0 datasets scikit-learn pandas numpy tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518f4aa",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ec69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_ROOT = Path(\"/root/dataset\")\n",
    "    TRAIN_DIR = DATASET_ROOT / \"train_final\"\n",
    "    VAL_DIR = DATASET_ROOT / \"val\"\n",
    "    TEST_DIR = DATASET_ROOT / \"test\"\n",
    "    OUTPUT_DIR = Path(\"./models/inlegalbert_classifier\")\n",
    "    LOGS_DIR = Path(\"./logs\")\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "    MAX_LENGTH = 256  # Maximum sequence length\n",
    "    \n",
    "    # Role labels\n",
    "    ROLES = [\n",
    "        \"Facts\",\n",
    "        \"Issue\",\n",
    "        \"Arguments of Petitioner\",\n",
    "        \"Arguments of Respondent\",\n",
    "        \"Reasoning\",\n",
    "        \"Decision\",\n",
    "        \"None\"\n",
    "    ]\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 10\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_STEPS = 500\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    \n",
    "    # Early stopping\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "    EARLY_STOPPING_THRESHOLD = 0.01\n",
    "    \n",
    "    # Evaluation\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training on: {config.DEVICE}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Number of roles: {len(config.ROLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5422c40",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "The dataset format is:\n",
    "```\n",
    "Sentence\\tRole\\tConfidence\n",
    "Sentence\\tRole\\tConfidence\n",
    "\n",
    "Sentence\\tRole\\tConfidence\n",
    "...\n",
    "```\n",
    "Blank lines separate different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(data_dir: Path) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load dataset from tab-separated text files.\n",
    "    \n",
    "    Format: Sentence\\tRole\\tConfidence (blank lines separate documents)\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        print(f\"Warning: Directory {data_dir} does not exist!\")\n",
    "        return samples\n",
    "    \n",
    "    txt_files = list(data_dir.glob(\"*.txt\"))\n",
    "    print(f\"Found {len(txt_files)} files in {data_dir.name}\")\n",
    "    \n",
    "    for file_path in tqdm(txt_files, desc=f\"Loading {data_dir.name}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip blank lines\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    sentence = parts[0].strip()\n",
    "                    role = parts[1].strip()\n",
    "                    \n",
    "                    # Skip empty sentences\n",
    "                    if sentence and role in config.ROLES:\n",
    "                        samples.append({\n",
    "                            'text': sentence,\n",
    "                            'label': role\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading training data...\")\n",
    "train_samples = load_dataset_from_files(config.TRAIN_DIR)\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_samples = load_dataset_from_files(config.VAL_DIR)\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_samples = load_dataset_from_files(config.TEST_DIR)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_samples):,}\")\n",
    "print(f\"  Validation samples: {len(val_samples):,}\")\n",
    "print(f\"  Test samples: {len(test_samples):,}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1af526",
   "metadata": {},
   "source": [
    "## 4. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(samples: List[Dict], split_name: str):\n",
    "    \"\"\"\n",
    "    Analyze and visualize dataset statistics.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(samples)\n",
    "    \n",
    "    print(f\"\\n{split_name} Dataset Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Role distribution\n",
    "    role_counts = df['label'].value_counts()\n",
    "    print(\"\\nRole Distribution:\")\n",
    "    for role, count in role_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {role:30s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Text length statistics\n",
    "    df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    print(\"\\nText Length Statistics (words):\")\n",
    "    print(f\"  Mean: {df['text_length'].mean():.2f}\")\n",
    "    print(f\"  Median: {df['text_length'].median():.2f}\")\n",
    "    print(f\"  Min: {df['text_length'].min()}\")\n",
    "    print(f\"  Max: {df['text_length'].max()}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Role distribution bar chart\n",
    "    role_counts.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title(f'{split_name} - Role Distribution')\n",
    "    axes[0].set_xlabel('Role')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Text length distribution\n",
    "    axes[1].hist(df['text_length'], bins=50, color='coral', edgecolor='black')\n",
    "    axes[1].set_title(f'{split_name} - Text Length Distribution')\n",
    "    axes[1].set_xlabel('Number of Words')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].axvline(df['text_length'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.LOGS_DIR / f'{split_name.lower()}_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze datasets\n",
    "if train_samples:\n",
    "    train_df = analyze_dataset(train_samples, \"Training\")\n",
    "if val_samples:\n",
    "    val_df = analyze_dataset(val_samples, \"Validation\")\n",
    "if test_samples:\n",
    "    test_df = analyze_dataset(test_samples, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ddf1f",
   "metadata": {},
   "source": [
    "## 5. Create Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(config.ROLES)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label Mappings:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {idx}: {label}\")\n",
    "\n",
    "# Save label mappings\n",
    "with open(config.OUTPUT_DIR / \"label_mappings.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nLabel mappings saved to {config.OUTPUT_DIR / 'label_mappings.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3be2e",
   "metadata": {},
   "source": [
    "## 6. Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff195ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(samples: List[Dict]) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert samples to HuggingFace Dataset format with label IDs.\n",
    "    \"\"\"\n",
    "    # Convert labels to IDs\n",
    "    data = {\n",
    "        'text': [s['text'] for s in samples],\n",
    "        'label': [label2id[s['label']] for s in samples]\n",
    "    }\n",
    "    \n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = prepare_dataset(train_samples) if train_samples else None\n",
    "val_dataset = prepare_dataset(val_samples) if val_samples else None\n",
    "test_dataset = prepare_dataset(test_samples) if test_samples else None\n",
    "\n",
    "print(\"Dataset Preparation Complete:\")\n",
    "if train_dataset:\n",
    "    print(f\"  Training: {len(train_dataset)} samples\")\n",
    "if val_dataset:\n",
    "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "if test_dataset:\n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Show example\n",
    "if train_dataset:\n",
    "    print(\"\\nExample training sample:\")\n",
    "    print(f\"  Text: {train_dataset[0]['text'][:100]}...\")\n",
    "    print(f\"  Label ID: {train_dataset[0]['label']}\")\n",
    "    print(f\"  Label Name: {id2label[train_dataset[0]['label']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721881c5",
   "metadata": {},
   "source": [
    "## 7. Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer: {config.MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {config.MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=len(config.ROLES),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(config.DEVICE)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4752f",
   "metadata": {},
   "source": [
    "## 8. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text inputs.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding=False,  # Dynamic padding in data collator\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "if train_dataset:\n",
    "    train_dataset = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing training data\"\n",
    "    )\n",
    "\n",
    "if val_dataset:\n",
    "    val_dataset = val_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing validation data\"\n",
    "    )\n",
    "\n",
    "if test_dataset:\n",
    "    test_dataset = test_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        desc=\"Tokenizing test data\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d046ea",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4292c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a090a",
   "metadata": {},
   "source": [
    "## 10. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63988c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(config.OUTPUT_DIR),\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_steps=config.WARMUP_STEPS,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config.EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=str(config.LOGS_DIR),\n",
    "    logging_steps=config.LOGGING_STEPS,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Total epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Gradient accumulation steps: {config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  FP16 training: {training_args.fp16}\")\n",
    "print(f\"  Warmup steps: {config.WARMUP_STEPS}\")\n",
    "print(f\"  Eval steps: {config.EVAL_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229535c",
   "metadata": {},
   "source": [
    "## 11. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=config.EARLY_STOPPING_PATIENCE,\n",
    "            early_stopping_threshold=config.EARLY_STOPPING_THRESHOLD\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "\n",
    "# Calculate training stats\n",
    "if train_dataset:\n",
    "    total_steps = (\n",
    "        len(train_dataset) // \n",
    "        (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS) * \n",
    "        config.NUM_EPOCHS\n",
    "    )\n",
    "    print(f\"\\nEstimated training steps: {total_steps:,}\")\n",
    "    print(f\"Evaluation every {config.EVAL_STEPS} steps\")\n",
    "    print(f\"Total evaluations: ~{total_steps // config.EVAL_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86c4cf",
   "metadata": {},
   "source": [
    "## 12. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Print training summary\n",
    "print(\"Training Summary:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67090711",
   "metadata": {},
   "source": [
    "## 13. Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(f\"\\nSaving model to {config.OUTPUT_DIR}...\")\n",
    "\n",
    "trainer.save_model(str(config.OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(config.OUTPUT_DIR))\n",
    "\n",
    "# Save training arguments\n",
    "with open(config.OUTPUT_DIR / \"training_config.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'model_name': config.MODEL_NAME,\n",
    "        'num_epochs': config.NUM_EPOCHS,\n",
    "        'batch_size': config.BATCH_SIZE,\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'max_length': config.MAX_LENGTH,\n",
    "        'num_labels': len(config.ROLES),\n",
    "        'roles': config.ROLES\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel files:\")\n",
    "for file in sorted(config.OUTPUT_DIR.glob(\"*\")):\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16930c1",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_dataset:\n",
    "    print(\"Evaluating on test set...\\n\")\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"Test Set Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    with open(config.OUTPUT_DIR / \"test_results.json\", 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "else:\n",
    "    print(\"No test dataset available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b3d85",
   "metadata": {},
   "source": [
    "## 15. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_dataset:\n",
    "    print(\"Generating detailed classification report...\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "    true_labels = predictions.label_ids\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 80)\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        target_names=config.ROLES,\n",
    "        digits=4\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(config.OUTPUT_DIR / \"classification_report.txt\", 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=config.ROLES,\n",
    "        yticklabels=config.ROLES,\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=16, pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nConfusion matrix saved to {config.OUTPUT_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1344d68",
   "metadata": {},
   "source": [
    "## 16. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a107699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on sample sentences\n",
    "print(\"Testing inference on sample sentences...\\n\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"The petitioner filed a writ petition challenging the constitutional validity of the Act.\",\n",
    "    \"The main issue in this case is whether the amendment violates Article 14 of the Constitution.\",\n",
    "    \"The learned counsel for the petitioner submitted that the impugned order is arbitrary.\",\n",
    "    \"The respondent contends that the petition is not maintainable in law.\",\n",
    "    \"After careful consideration of the submissions made by both parties, we are of the view that the law is well settled.\",\n",
    "    \"The writ petition is hereby dismissed with costs.\",\n",
    "    \"The court was informed about the procedural requirements.\"\n",
    "]\n",
    "\n",
    "# Load the trained model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    result = classifier(sentence)[0]\n",
    "    predicted_label = result['label']\n",
    "    confidence = result['score']\n",
    "    \n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Predicted: {predicted_label} (confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f93c68",
   "metadata": {},
   "source": [
    "## 17. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training log\n",
    "log_file = list(config.LOGS_DIR.glob(\"**/trainer_state.json\"))\n",
    "\n",
    "if log_file:\n",
    "    with open(log_file[0], 'r') as f:\n",
    "        trainer_state = json.load(f)\n",
    "    \n",
    "    log_history = trainer_state['log_history']\n",
    "    \n",
    "    # Extract metrics\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    eval_f1 = []\n",
    "    steps = []\n",
    "    eval_steps = []\n",
    "    \n",
    "    for entry in log_history:\n",
    "        if 'loss' in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            eval_loss.append(entry['eval_loss'])\n",
    "            eval_f1.append(entry.get('eval_f1_macro', 0))\n",
    "            eval_steps.append(entry['step'])\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(steps, train_loss, label='Training Loss', color='blue', linewidth=2)\n",
    "    axes[0].plot(eval_steps, eval_loss, label='Validation Loss', color='orange', linewidth=2)\n",
    "    axes[0].set_xlabel('Training Steps')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 score plot\n",
    "    axes[1].plot(eval_steps, eval_f1, label='Validation F1 (Macro)', color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Training Steps')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].set_title('Validation F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training history plot saved to {config.OUTPUT_DIR / 'training_history.png'}\")\n",
    "else:\n",
    "    print(\"Training log not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9305ba4",
   "metadata": {},
   "source": [
    "## 18. Model Summary and Export Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE - MODEL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📁 Model Location: {config.OUTPUT_DIR}\")\n",
    "print(f\"\\n📊 Performance Metrics:\")\n",
    "if test_dataset:\n",
    "    print(f\"  - Test Accuracy: {test_results.get('eval_accuracy', 0):.4f}\")\n",
    "    print(f\"  - Test F1 (Macro): {test_results.get('eval_f1_macro', 0):.4f}\")\n",
    "    print(f\"  - Test F1 (Weighted): {test_results.get('eval_f1_weighted', 0):.4f}\")\n",
    "\n",
    "print(f\"\\n🔧 Model Configuration:\")\n",
    "print(f\"  - Base Model: {config.MODEL_NAME}\")\n",
    "print(f\"  - Number of Labels: {len(config.ROLES)}\")\n",
    "print(f\"  - Max Sequence Length: {config.MAX_LENGTH}\")\n",
    "print(f\"  - Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n📝 Usage Instructions:\")\n",
    "print(f\"\"\"\\nTo load this model for inference:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"{config.OUTPUT_DIR}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Inference\n",
    "text = \"Your legal sentence here\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length={config.MAX_LENGTH})\n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "# Load label mappings\n",
    "import json\n",
    "with open(\"{config.OUTPUT_DIR}/label_mappings.json\", 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "    id2label = mappings['id2label']\n",
    "\n",
    "print(f\"Predicted role: {{id2label[str(predicted_class)]}}\")\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n✅ All outputs saved to: {config.OUTPUT_DIR}\")\n",
    "print(f\"   - Model weights: pytorch_model.bin\")\n",
    "print(f\"   - Tokenizer: tokenizer_config.json, vocab.txt\")\n",
    "print(f\"   - Label mappings: label_mappings.json\")\n",
    "print(f\"   - Training config: training_config.json\")\n",
    "print(f\"   - Test results: test_results.json\")\n",
    "print(f\"   - Classification report: classification_report.txt\")\n",
    "print(f\"   - Confusion matrix: confusion_matrix.png\")\n",
    "print(f\"   - Training history: training_history.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎉 Training pipeline completed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
