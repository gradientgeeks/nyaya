{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a10f3a3",
   "metadata": {},
   "source": [
    "# Legal Document Role Classifier Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive guide to train your own rhetorical role classifier for legal documents using your existing dataset.\n",
    "\n",
    "## Dataset Structure\n",
    "Your data should be in the format:\n",
    "```\n",
    "sentence1\\trole1\n",
    "sentence2\\trole2\n",
    "\\n\n",
    "sentence1\\trole1  # New document\n",
    "sentence2\\trole2\n",
    "```\n",
    "\n",
    "## Supported Roles\n",
    "- Facts\n",
    "- Issue\n",
    "- Arguments of Petitioner\n",
    "- Arguments of Respondent\n",
    "- Reasoning\n",
    "- Decision\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e628df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install torch transformers scikit-learn pandas matplotlib seaborn spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf95d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add your project path\n",
    "PROJECT_ROOT = \"/home/uttam/B.Tech Major Project/nyaya/server\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\", \"models\", \"training\"))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "try:\n",
    "    from train import RoleClassifierTrainer\n",
    "    from data_loader import create_data_loaders, LegalDocumentDataset\n",
    "    from evaluate import ModelEvaluator\n",
    "    from src.models.role_classifier import RoleClassifier, RhetoricalRole\n",
    "    print(\"‚úÖ Successfully imported training modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure you're running from the correct directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859ed69",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data paths - Update these to match your dataset location\n",
    "    \"train_data\": \"/home/uttam/B.Tech Major Project/nyaya/Hier_BiLSTM_CRF-20250827T113256Z-1-001/Hier_BiLSTM_CRF/train\",\n",
    "    \"val_data\": \"/home/uttam/B.Tech Major Project/nyaya/Hier_BiLSTM_CRF-20250827T113256Z-1-001/Hier_BiLSTM_CRF/val\", \n",
    "    \"test_data\": \"/home/uttam/B.Tech Major Project/nyaya/Hier_BiLSTM_CRF-20250827T113256Z-1-001/Hier_BiLSTM_CRF/test\",\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_type\": \"inlegalbert\",  # Options: \"inlegalbert\", \"bilstm_crf\"\n",
    "    \"model_name\": \"law-ai/InLegalBERT\",  # Pre-trained model\n",
    "    \"context_mode\": \"prev\",  # Options: \"single\", \"prev\", \"prev_two\", \"surrounding\"\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 10,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_length\": 512,\n",
    "    \"warmup_steps\": 500,\n",
    "    \n",
    "    # Device and output\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"./trained_models\",\n",
    "    \"save_best_model\": True\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127ac1a",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset\n",
    "def explore_dataset(data_path):\n",
    "    \"\"\"Explore the structure and statistics of your dataset\"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ùå Data path does not exist: {data_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Exploring dataset: {data_path}\")\n",
    "    \n",
    "    if data_path.is_file():\n",
    "        files = [data_path]\n",
    "    else:\n",
    "        files = list(data_path.glob(\"*.txt\"))\n",
    "    \n",
    "    print(f\"üìÑ Found {len(files)} files\")\n",
    "    \n",
    "    total_sentences = 0\n",
    "    total_documents = 0\n",
    "    role_counts = {}\n",
    "    \n",
    "    for file_path in files[:5]:  # Check first 5 files\n",
    "        print(f\"\\nüìù File: {file_path.name}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        lines = content.split('\\n')\n",
    "        doc_sentences = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if doc_sentences > 0:\n",
    "                    total_documents += 1\n",
    "                    doc_sentences = 0\n",
    "                continue\n",
    "            \n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                sentence = parts[0].strip()\n",
    "                role = parts[1].strip()\n",
    "                \n",
    "                total_sentences += 1\n",
    "                doc_sentences += 1\n",
    "                role_counts[role] = role_counts.get(role, 0) + 1\n",
    "        \n",
    "        if doc_sentences > 0:\n",
    "            total_documents += 1\n",
    "        \n",
    "        print(f\"  Sentences in this file: {doc_sentences}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  Total files: {len(files)}\")\n",
    "    print(f\"  Total documents: {total_documents}\")\n",
    "    print(f\"  Total sentences: {total_sentences}\")\n",
    "    print(f\"  Average sentences per document: {total_sentences/max(total_documents, 1):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è Role Distribution:\")\n",
    "    for role, count in sorted(role_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_sentences) * 100\n",
    "        print(f\"  {role}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return role_counts\n",
    "\n",
    "# Explore training data\n",
    "print(\"üîç Exploring Training Data\")\n",
    "train_role_counts = explore_dataset(config[\"train_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize role distribution\n",
    "if train_role_counts:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    roles = list(train_role_counts.keys())\n",
    "    counts = list(train_role_counts.values())\n",
    "    \n",
    "    plt.bar(roles, counts, color='skyblue', alpha=0.7)\n",
    "    plt.title('Role Distribution in Training Data', fontsize=16)\n",
    "    plt.xlabel('Rhetorical Role', fontsize=12)\n",
    "    plt.ylabel('Number of Sentences', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + max(counts)*0.01, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie(counts, labels=roles, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Role Distribution (Percentage)', fontsize=16)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b23a0",
   "metadata": {},
   "source": [
    "## Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "print(\"üîÑ Testing Data Loading...\")\n",
    "\n",
    "try:\n",
    "    # Create data loaders\n",
    "    data_loaders = create_data_loaders(\n",
    "        train_path=config[\"train_data\"],\n",
    "        val_path=config[\"val_data\"],\n",
    "        test_path=config[\"test_data\"],\n",
    "        tokenizer_name=config[\"model_name\"],\n",
    "        context_mode=config[\"context_mode\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_length=config[\"max_length\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Data loaders created successfully!\")\n",
    "    print(f\"üì¶ Training batches: {len(data_loaders['train'])}\")\n",
    "    print(f\"üì¶ Validation batches: {len(data_loaders['val'])}\")\n",
    "    if 'test' in data_loaders:\n",
    "        print(f\"üì¶ Test batches: {len(data_loaders['test'])}\")\n",
    "    \n",
    "    # Check a sample batch\n",
    "    sample_batch = next(iter(data_loaders['train']))\n",
    "    print(f\"\\nüîç Sample Batch Shape:\")\n",
    "    print(f\"  Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "    print(f\"  Attention Mask: {sample_batch['attention_mask'].shape}\")\n",
    "    print(f\"  Labels: {sample_batch['labels'].shape}\")\n",
    "    print(f\"  Unique labels in batch: {torch.unique(sample_batch['labels'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please check your data paths and format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some examples from the dataset\n",
    "try:\n",
    "    sample_batch = next(iter(data_loaders['train']))\n",
    "    \n",
    "    print(\"üìù Sample Training Examples:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show first 3 examples\n",
    "    for i in range(min(3, len(sample_batch['text']))):\n",
    "        text = sample_batch['text'][i]\n",
    "        label_id = sample_batch['labels'][i].item()\n",
    "        \n",
    "        # Map label ID to role name\n",
    "        role_names = [role.value for role in RhetoricalRole]\n",
    "        role_name = role_names[label_id] if label_id < len(role_names) else \"Unknown\"\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text[:200]}{'...' if len(text) > 200 else ''}\")\n",
    "        print(f\"Role: {role_name} (ID: {label_id})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error sampling examples: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b03cd",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d27ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"üöÄ Initializing Role Classifier Trainer...\")\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(config[\"output_dir\"]) / f\"{config['model_type']}_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Output directory: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    trainer = RoleClassifierTrainer(\n",
    "        model_type=config[\"model_type\"],\n",
    "        model_name=config[\"model_name\"],\n",
    "        device=config[\"device\"],\n",
    "        output_dir=str(output_dir),\n",
    "        num_labels=7  # 7 rhetorical roles\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized successfully!\")\n",
    "    print(f\"üñ•Ô∏è  Using device: {config['device']}\")\n",
    "    print(f\"ü§ñ Model type: {config['model_type']}\")\n",
    "    print(f\"üìè Model parameters: {sum(p.numel() for p in trainer.model.parameters()):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing trainer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üéØ Starting Training...\")\n",
    "print(f\"‚è±Ô∏è  Training for {config['num_epochs']} epochs\")\n",
    "print(f\"üìö Batch size: {config['batch_size']}\")\n",
    "print(f\"üß† Learning rate: {config['learning_rate']}\")\n",
    "print(f\"üìù Context mode: {config['context_mode']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train(\n",
    "        train_data_path=config[\"train_data\"],\n",
    "        val_data_path=config[\"val_data\"],\n",
    "        test_data_path=config[\"test_data\"],\n",
    "        context_mode=config[\"context_mode\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        save_best_model=config[\"save_best_model\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76074cc",
   "metadata": {},
   "source": [
    "## Training Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c96c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display training history\n",
    "history_path = output_dir / \"training_history.json\"\n",
    "\n",
    "if history_path.exists():\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    print(\"üìà Training History:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display final metrics\n",
    "    if history:\n",
    "        final_train_loss = history['train_loss'][-1] if history['train_loss'] else 'N/A'\n",
    "        final_val_loss = history['val_loss'][-1] if history['val_loss'] else 'N/A'\n",
    "        final_train_f1 = history['train_f1'][-1] if history['train_f1'] else 'N/A'\n",
    "        final_val_f1 = history['val_f1'][-1] if history['val_f1'] else 'N/A'\n",
    "        \n",
    "        print(f\"Final Training Loss: {final_train_loss:.4f}\" if isinstance(final_train_loss, float) else f\"Final Training Loss: {final_train_loss}\")\n",
    "        print(f\"Final Validation Loss: {final_val_loss:.4f}\" if isinstance(final_val_loss, float) else f\"Final Validation Loss: {final_val_loss}\")\n",
    "        print(f\"Final Training F1: {final_train_f1:.4f}\" if isinstance(final_train_f1, float) else f\"Final Training F1: {final_train_f1}\")\n",
    "        print(f\"Final Validation F1: {final_val_f1:.4f}\" if isinstance(final_val_f1, float) else f\"Final Validation F1: {final_val_f1}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if all(key in history and history[key] for key in ['epoch', 'train_loss', 'val_loss', 'train_f1', 'val_f1']):\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Loss plot\n",
    "            ax1.plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o')\n",
    "            ax1.plot(history['epoch'], history['val_loss'], label='Val Loss', marker='s')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # F1 Score plot\n",
    "            ax2.plot(history['epoch'], history['train_f1'], label='Train F1', marker='o')\n",
    "            ax2.plot(history['epoch'], history['val_f1'], label='Val F1', marker='s')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('F1 Score')\n",
    "            ax2.set_title('Training and Validation F1 Score')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Training history not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf2209",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b43b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"üîç Evaluating Trained Model...\")\n",
    "\n",
    "# Path to the best model\n",
    "best_model_path = output_dir / \"best_model.pt\"\n",
    "\n",
    "if best_model_path.exists():\n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = ModelEvaluator(\n",
    "            model_path=str(best_model_path),\n",
    "            device=config[\"device\"]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Evaluator initialized\")\n",
    "        \n",
    "        # Create evaluation output directory\n",
    "        eval_output_dir = output_dir / \"evaluation_results\"\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        metrics = evaluator.evaluate_dataset(\n",
    "            test_data_path=config[\"test_data\"],\n",
    "            context_mode=config[\"context_mode\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            output_dir=str(eval_output_dir)\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüéØ Evaluation Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"üìä Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "        print(f\"üìä Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "        print(f\"üìä Weighted Precision: {metrics['weighted_precision']:.4f}\")\n",
    "        print(f\"üìä Weighted Recall: {metrics['weighted_recall']:.4f}\")\n",
    "        \n",
    "        # Display per-class metrics\n",
    "        if 'per_class' in metrics:\n",
    "            print(\"\\nüìà Per-Class Metrics:\")\n",
    "            print(\"-\" * 60)\n",
    "            for role, class_metrics in metrics['per_class'].items():\n",
    "                print(f\"{role:20} | F1: {class_metrics['f1']:.3f} | Prec: {class_metrics['precision']:.3f} | Rec: {class_metrics['recall']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"‚ùå Best model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4fd72",
   "metadata": {},
   "source": [
    "## Test Single Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single predictions\n",
    "if 'evaluator' in locals():\n",
    "    print(\"üß™ Testing Single Predictions...\")\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"The petitioner filed a writ petition challenging the constitutional validity of Section 377.\",\n",
    "        \"The main issue in this case is whether Section 377 violates fundamental rights.\",\n",
    "        \"The petitioner argues that Section 377 is discriminatory and violates Article 14.\",\n",
    "        \"The respondent contends that Section 377 is constitutionally valid and necessary.\",\n",
    "        \"The court finds that Section 377 infringes upon the right to privacy and equality.\",\n",
    "        \"Therefore, Section 377 is hereby declared unconstitutional and is struck down.\"\n",
    "    ]\n",
    "    \n",
    "    expected_roles = [\"Facts\", \"Issue\", \"Arguments of Petitioner\", \"Arguments of Respondent\", \"Reasoning\", \"Decision\"]\n",
    "    \n",
    "    print(\"\\nüìù Prediction Results:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i, (sentence, expected) in enumerate(zip(test_sentences, expected_roles)):\n",
    "        result = evaluator.predict_single(sentence, context_mode=config[\"context_mode\"])\n",
    "        \n",
    "        predicted_role = result['predicted_role']\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        is_correct = predicted_role == expected\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "        \n",
    "        print(f\"\\n{status} Example {i+1}:\")\n",
    "        print(f\"Text: {sentence[:80]}{'...' if len(sentence) > 80 else ''}\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Predicted: {predicted_role} (Confidence: {confidence:.3f})\")\n",
    "        \n",
    "        # Show top predictions\n",
    "        print(\"Top 3 predictions:\")\n",
    "        for j, pred in enumerate(result['top_predictions'][:3]):\n",
    "            print(f\"  {j+1}. {pred['role']}: {pred['confidence']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    accuracy = correct_predictions / len(test_sentences)\n",
    "    print(f\"\\nüéØ Test Accuracy: {correct_predictions}/{len(test_sentences)} ({accuracy:.1%})\")\n",
    "else:\n",
    "    print(\"‚ùå Evaluator not available. Please complete the evaluation step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e3560",
   "metadata": {},
   "source": [
    "## Save and Load Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how to save and load the model for production use\n",
    "print(\"üíæ Model Save/Load for Production\")\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"\\nüìÇ Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    # Show how to load the model in production\n",
    "    print(\"\\nüîß To use this model in your Nyaya system:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    production_code = f'''\n",
    "# In your production code (e.g., in role_classifier.py):\n",
    "from src.models.role_classifier import RoleClassifier\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = RoleClassifier(\n",
    "    model_type=\"{config['model_type']}\",\n",
    "    device=\"{config['device']}\"\n",
    ")\n",
    "\n",
    "# Load your trained weights\n",
    "classifier.load_pretrained_weights(\"{best_model_path}\")\n",
    "\n",
    "# Use for classification\n",
    "results = classifier.classify_document(\n",
    "    document_text=\"Your legal document text here...\",\n",
    "    context_mode=\"{config['context_mode']}\"\n",
    ")\n",
    "'''\n",
    "    \n",
    "    print(production_code)\n",
    "    \n",
    "    # Save production instructions\n",
    "    instructions_path = output_dir / \"production_usage.py\"\n",
    "    with open(instructions_path, 'w') as f:\n",
    "        f.write(production_code)\n",
    "    \n",
    "    print(f\"\\nüìÑ Production usage instructions saved to: {instructions_path}\")\n",
    "    \n",
    "    # Model info\n",
    "    model_info = {\n",
    "        \"model_type\": config[\"model_type\"],\n",
    "        \"model_name\": config[\"model_name\"],\n",
    "        \"context_mode\": config[\"context_mode\"],\n",
    "        \"training_config\": config,\n",
    "        \"model_path\": str(best_model_path),\n",
    "        \"evaluation_metrics\": metrics if 'metrics' in locals() else None,\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    \n",
    "    info_path = output_dir / \"model_info.json\"\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(model_info, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìã Model information saved to: {info_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No trained model found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f38ca6",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1baa98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"üéä TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'metrics' in locals():\n",
    "    print(f\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"üìä Final Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"üìä Final Test F1 Score: {metrics['weighted_f1']:.4f}\")\n",
    "    print(f\"üìÇ Model saved at: {best_model_path}\")\n",
    "    print(f\"üìÇ Results saved at: {output_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Training may not have completed successfully.\")\n",
    "    print(\"Please check the error messages above.\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"1. üìã Review the evaluation results and confusion matrix\")\n",
    "print(\"2. üîß Integrate the trained model into your Nyaya system\")\n",
    "print(\"3. üß™ Test with real legal documents\")\n",
    "print(\"4. üìà Consider further fine-tuning if needed\")\n",
    "print(\"5. üîÑ Update the role_classifier.py to use your trained weights\")\n",
    "\n",
    "print(\"\\nüìö FILES GENERATED:\")\n",
    "if output_dir.exists():\n",
    "    generated_files = list(output_dir.rglob(\"*\"))\n",
    "    for file_path in generated_files:\n",
    "        if file_path.is_file():\n",
    "            print(f\"  üìÑ {file_path.relative_to(output_dir)}\")\n",
    "else:\n",
    "    print(\"  ‚ùå No output directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bdc88",
   "metadata": {},
   "source": [
    "## Optional: Hyperparameter Tuning\n",
    "\n",
    "If you want to experiment with different hyperparameters, you can modify the configuration and re-run the training cells above. Consider trying:\n",
    "\n",
    "- Different context modes: `\"single\"`, `\"prev_two\"`, `\"surrounding\"`\n",
    "- Different learning rates: `1e-5`, `3e-5`, `5e-5`\n",
    "- Different batch sizes: `8`, `32` (depending on your GPU memory)\n",
    "- More epochs for better convergence\n",
    "- Different model types: `\"bilstm_crf\"` for sequence modeling\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "1. **CUDA Out of Memory**: Reduce batch size or max_length\n",
    "2. **Low Accuracy**: Try more epochs, different context modes, or data augmentation\n",
    "3. **Import Errors**: Check file paths and ensure all dependencies are installed\n",
    "4. **Data Format Issues**: Ensure your data follows the sentence\\trole format with proper encoding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
