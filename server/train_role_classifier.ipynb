{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9383811d",
   "metadata": {},
   "source": [
    "# Legal Document Rhetorical Role Classifier Training\n",
    "\n",
    "This notebook trains a rhetorical role classifier for legal documents using InLegalBERT.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `/kaggle/input/dataset/Hier_BiLSTM_CRF/train/` - Training data\n",
    "- `/kaggle/input/dataset/Hier_BiLSTM_CRF/val/` - Validation data\n",
    "- `/kaggle/input/dataset/Hier_BiLSTM_CRF/test/` - Test data\n",
    "\n",
    "**Model Output:**\n",
    "- Saved to `/kaggle/working/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de11f8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b74ea6b",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76906f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uninstall existing transformers to avoid conflicts\n",
    "!pip uninstall -y transformers\n",
    "\n",
    "# Install compatible versions\n",
    "!pip install -q transformers==4.35.2\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn tqdm\n",
    "!pip install -q accelerate sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec366790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06a3fa",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = \"/kaggle/input/dataset/Hier_BiLSTM_CRF\"\n",
    "    TRAIN_PATH = os.path.join(DATA_PATH, \"train\")\n",
    "    VAL_PATH = os.path.join(DATA_PATH, \"val/val\")\n",
    "    TEST_PATH = os.path.join(DATA_PATH, \"test\")\n",
    "    OUTPUT_PATH = \"/kaggle/working\"\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "    MAX_LENGTH = 256  # Reduced for memory optimization\n",
    "    CONTEXT_MODE = \"prev\"  # Options: \"single\", \"prev\", \"prev_two\", \"surrounding\"\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    \n",
    "    # Data sampling (for memory constraints)\n",
    "    TRAIN_SAMPLE_RATIO = 1.0  # Use 100% of training data\n",
    "    VAL_SAMPLE_RATIO = 1.0\n",
    "    TEST_SAMPLE_RATIO = 1.0\n",
    "    \n",
    "    # Optimization\n",
    "    USE_AMP = True  # Automatic Mixed Precision\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    \n",
    "    # Rhetorical roles\n",
    "    ROLES = [\n",
    "        \"None\",\n",
    "        \"Facts\",\n",
    "        \"Issue\",\n",
    "        \"Arguments of Petitioner\",\n",
    "        \"Arguments of Respondent\",\n",
    "        \"Reasoning\",\n",
    "        \"Decision\"\n",
    "    ]\n",
    "    NUM_LABELS = len(ROLES)\n",
    "    ROLE2ID = {role: idx for idx, role in enumerate(ROLES)}\n",
    "    ID2ROLE = {idx: role for idx, role in enumerate(ROLES)}\n",
    "\n",
    "config = Config()\n",
    "print(f\"Configuration loaded. Training on {config.CONTEXT_MODE} context mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c0eee",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentDataset(Dataset):\n",
    "    \"\"\"Optimized dataset for legal document role classification\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tokenizer, config, split='train'):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.data = []\n",
    "        \n",
    "        # Load and process data\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load data from text files\"\"\"\n",
    "        logger.info(f\"Loading {self.split} data from {self.data_path}\")\n",
    "        \n",
    "        # Get all text files\n",
    "        txt_files = sorted(list(self.data_path.glob(\"*.txt\")))\n",
    "        \n",
    "        # Sample files if needed\n",
    "        if self.split == 'train' and self.config.TRAIN_SAMPLE_RATIO < 1.0:\n",
    "            num_files = int(len(txt_files) * self.config.TRAIN_SAMPLE_RATIO)\n",
    "            txt_files = random.sample(txt_files, num_files)\n",
    "        elif self.split == 'val' and self.config.VAL_SAMPLE_RATIO < 1.0:\n",
    "            num_files = int(len(txt_files) * self.config.VAL_SAMPLE_RATIO)\n",
    "            txt_files = random.sample(txt_files, num_files)\n",
    "        elif self.split == 'test' and self.config.TEST_SAMPLE_RATIO < 1.0:\n",
    "            num_files = int(len(txt_files) * self.config.TEST_SAMPLE_RATIO)\n",
    "            txt_files = random.sample(txt_files, num_files)\n",
    "        \n",
    "        logger.info(f\"Processing {len(txt_files)} files...\")\n",
    "        \n",
    "        for file_path in tqdm(txt_files, desc=f\"Loading {self.split}\"):\n",
    "            document = self._load_file(file_path)\n",
    "            if document:\n",
    "                self._process_document(document)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.data)} sentences from {self.split} set\")\n",
    "        self._print_statistics()\n",
    "    \n",
    "    def _load_file(self, file_path):\n",
    "        \"\"\"Load a single file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            document = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if '\\t' in line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        sentence, role = parts\n",
    "                        if role in self.config.ROLE2ID:\n",
    "                            document.append({'sentence': sentence.strip(), 'role': role})\n",
    "            \n",
    "            return document\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _process_document(self, document):\n",
    "        \"\"\"Process document with context\"\"\"\n",
    "        for idx, item in enumerate(document):\n",
    "            context_text = self._create_context(document, idx)\n",
    "            self.data.append({\n",
    "                'text': context_text,\n",
    "                'label': self.config.ROLE2ID[item['role']]\n",
    "            })\n",
    "    \n",
    "    def _create_context(self, document, idx):\n",
    "        \"\"\"Create context based on mode\"\"\"\n",
    "        current = document[idx]['sentence']\n",
    "        \n",
    "        if self.config.CONTEXT_MODE == \"single\":\n",
    "            return current\n",
    "        elif self.config.CONTEXT_MODE == \"prev\":\n",
    "            if idx > 0:\n",
    "                prev = document[idx-1]['sentence']\n",
    "                return f\"{prev} [SEP] {current}\"\n",
    "            return current\n",
    "        elif self.config.CONTEXT_MODE == \"prev_two\":\n",
    "            context = []\n",
    "            if idx > 1:\n",
    "                context.append(document[idx-2]['sentence'])\n",
    "            if idx > 0:\n",
    "                context.append(document[idx-1]['sentence'])\n",
    "            context.append(current)\n",
    "            return \" [SEP] \".join(context)\n",
    "        elif self.config.CONTEXT_MODE == \"surrounding\":\n",
    "            context = []\n",
    "            if idx > 0:\n",
    "                context.append(document[idx-1]['sentence'])\n",
    "            context.append(current)\n",
    "            if idx < len(document) - 1:\n",
    "                context.append(document[idx+1]['sentence'])\n",
    "            return \" [SEP] \".join(context)\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        labels = [item['label'] for item in self.data]\n",
    "        label_counts = Counter(labels)\n",
    "        \n",
    "        print(f\"\\n{self.split.upper()} Dataset Statistics:\")\n",
    "        print(f\"Total samples: {len(self.data)}\")\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        for label_id, count in sorted(label_counts.items()):\n",
    "            role_name = self.config.ID2ROLE[label_id]\n",
    "            percentage = (count / len(self.data)) * 100\n",
    "            print(f\"  {role_name}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            max_length=self.config.MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(item['label'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6440c",
   "metadata": {},
   "source": [
    "## 4. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InLegalBERTClassifier(nn.Module):\n",
    "    \"\"\"InLegalBERT-based classifier for rhetorical role classification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763e85b",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if config.USE_AMP:\n",
    "            with autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item() * config.GRADIENT_ACCUMULATION_STEPS\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': total_loss / (step + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            if config.USE_AMP:\n",
    "                with autocast():\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                    loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1_macro, f1_weighted, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57349f8",
   "metadata": {},
   "source": [
    "## 6. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a80fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(f\"Loading tokenizer: {config.MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LegalDocumentDataset(config.TRAIN_PATH, tokenizer, config, split='train')\n",
    "val_dataset = LegalDocumentDataset(config.VAL_PATH, tokenizer, config, split='val')\n",
    "test_dataset = LegalDocumentDataset(config.TEST_PATH, tokenizer, config, split='test')\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nDataloaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366e44f",
   "metadata": {},
   "source": [
    "## 7. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc838f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"Initializing model: {config.MODEL_NAME}\")\n",
    "model = InLegalBERTClassifier(config.MODEL_NAME, config.NUM_LABELS)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "total_steps = len(train_loader) * config.NUM_EPOCHS // config.GRADIENT_ACCUMULATION_STEPS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.USE_AMP else None\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {config.WARMUP_STEPS}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Mixed precision: {config.USE_AMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4bdd3",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917844f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_f1_macro': [],\n",
    "    'val_f1_weighted': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, scaler, device, config)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1_macro, val_f1_weighted, _, _ = evaluate(model, val_loader, device, config)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1_macro'].append(val_f1_macro)\n",
    "    history['val_f1_weighted'].append(val_f1_weighted)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Val F1 (Macro): {val_f1_macro:.4f} | Val F1 (Weighted): {val_f1_weighted:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1_weighted > best_val_f1:\n",
    "        best_val_f1 = val_f1_weighted\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(config.OUTPUT_PATH, 'best_model.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_f1_weighted': val_f1_weighted,\n",
    "            'config': config\n",
    "        }, model_path)\n",
    "        \n",
    "        print(f\"  ✓ New best model saved! (F1: {val_f1_weighted:.4f})\")\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training completed! Best epoch: {best_epoch} (F1: {best_val_f1:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bff89",
   "metadata": {},
   "source": [
    "## 9. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for testing...\")\n",
    "checkpoint = torch.load(os.path.join(config.OUTPUT_PATH, 'best_model.pt'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc, test_f1_macro, test_f1_weighted, test_predictions, test_labels = evaluate(\n",
    "    model, test_loader, device, config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Set Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1 (Macro): {test_f1_macro:.4f}\")\n",
    "print(f\"Test F1 (Weighted): {test_f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae27697",
   "metadata": {},
   "source": [
    "## 10. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc36938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "report = classification_report(\n",
    "    test_labels,\n",
    "    test_predictions,\n",
    "    target_names=config.ROLES,\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(os.path.join(config.OUTPUT_PATH, 'classification_report.txt'), 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cf031",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=config.ROLES,\n",
    "    yticklabels=config.ROLES\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.OUTPUT_PATH, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8bbdd",
   "metadata": {},
   "source": [
    "## 12. Training History Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot([acc*100 for acc in history['train_acc']], label='Train Acc', marker='o')\n",
    "axes[0, 1].plot([acc*100 for acc in history['val_acc']], label='Val Acc', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Scores\n",
    "axes[1, 0].plot(history['val_f1_macro'], label='Val F1 (Macro)', marker='o')\n",
    "axes[1, 0].plot(history['val_f1_weighted'], label='Val F1 (Weighted)', marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_title('Validation F1 Scores')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Summary table\n",
    "axes[1, 1].axis('off')\n",
    "summary_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Best Epoch', f\"{best_epoch}\"],\n",
    "    ['Best Val F1', f\"{best_val_f1:.4f}\"],\n",
    "    ['Test Accuracy', f\"{test_acc*100:.2f}%\"],\n",
    "    ['Test F1 (Macro)', f\"{test_f1_macro:.4f}\"],\n",
    "    ['Test F1 (Weighted)', f\"{test_f1_weighted:.4f}\"],\n",
    "    ['Context Mode', config.CONTEXT_MODE],\n",
    "    ['Batch Size', f\"{config.BATCH_SIZE}\"],\n",
    "]\n",
    "table = axes[1, 1].table(cellText=summary_data, cellLoc='left', loc='center',\n",
    "                         colWidths=[0.5, 0.5])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "axes[1, 1].set_title('Training Summary', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.OUTPUT_PATH, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e7b92",
   "metadata": {},
   "source": [
    "## 13. Save Final Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer_path = os.path.join(config.OUTPUT_PATH, 'tokenizer')\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "\n",
    "# Save model in PyTorch format\n",
    "final_model_path = os.path.join(config.OUTPUT_PATH, 'role_classifier_final.pt')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'model_name': config.MODEL_NAME,\n",
    "        'num_labels': config.NUM_LABELS,\n",
    "        'max_length': config.MAX_LENGTH,\n",
    "        'context_mode': config.CONTEXT_MODE,\n",
    "        'roles': config.ROLES,\n",
    "        'role2id': config.ROLE2ID,\n",
    "        'id2role': config.ID2ROLE\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'accuracy': test_acc,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'f1_weighted': test_f1_weighted\n",
    "    }\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv(os.path.join(config.OUTPUT_PATH, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All artifacts saved successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSaved files in {config.OUTPUT_PATH}:\")\n",
    "print(\"  - best_model.pt (checkpoint with optimizer state)\")\n",
    "print(\"  - role_classifier_final.pt (final model for inference)\")\n",
    "print(\"  - tokenizer/ (tokenizer files)\")\n",
    "print(\"  - classification_report.txt\")\n",
    "print(\"  - confusion_matrix.png\")\n",
    "print(\"  - training_history.png\")\n",
    "print(\"  - training_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82aa367",
   "metadata": {},
   "source": [
    "## 14. Test Inference (Sample Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3562c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference function\n",
    "def predict_role(text, model, tokenizer, config, device):\n",
    "    \"\"\"Predict rhetorical role for a given text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    pred_label = config.ID2ROLE[prediction.item()]\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return pred_label, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Predictions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_texts = [\n",
    "    \"The petitioner filed a writ petition challenging the constitutional validity of Section 377.\",\n",
    "    \"The main issue in this case is whether Section 377 violates fundamental rights.\",\n",
    "    \"The petitioner argues that Section 377 is discriminatory and violates Article 14.\",\n",
    "    \"The respondent contends that Section 377 is constitutionally valid.\",\n",
    "    \"The court finds that Section 377 infringes upon the right to privacy and equality.\",\n",
    "    \"Therefore, Section 377 is hereby declared unconstitutional.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    pred_role, confidence, probs = predict_role(text, model, tokenizer, config, device)\n",
    "    print(f\"\\n{i}. Text: {text[:80]}...\" if len(text) > 80 else f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Predicted Role: {pred_role}\")\n",
    "    print(f\"   Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23682958",
   "metadata": {},
   "source": [
    "## 15. Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, train_dataset, val_dataset, test_dataset\n",
    "del train_loader, val_loader, test_loader\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleaned up successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20065ae8",
   "metadata": {},
   "source": [
    "## 16. Summary\n",
    "\n",
    "### Training Complete!\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Loaded the legal document dataset from `/kaggle/input/dataset/Hier_BiLSTM_CRF/`\n",
    "2. ✅ Preprocessed data with configurable context modes\n",
    "3. ✅ Trained an InLegalBERT-based classifier for rhetorical role classification\n",
    "4. ✅ Evaluated the model on validation and test sets\n",
    "5. ✅ Generated classification reports and visualizations\n",
    "6. ✅ Saved all artifacts to `/kaggle/working/`\n",
    "\n",
    "### Optimizations Implemented:\n",
    "- Mixed Precision Training (AMP)\n",
    "- Gradient Accumulation\n",
    "- Memory-efficient data loading\n",
    "- Configurable dataset sampling\n",
    "- Automatic garbage collection\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the trained model from `/kaggle/working/`\n",
    "2. Integrate the model into your FastAPI backend\n",
    "3. Use the model for document segmentation in your RAG pipeline\n",
    "4. Fine-tune hyperparameters if needed (learning rate, batch size, context mode)\n",
    "\n",
    "### Model Files:\n",
    "- `best_model.pt` - Complete checkpoint for resuming training\n",
    "- `role_classifier_final.pt` - Final model for inference\n",
    "- `tokenizer/` - Tokenizer for preprocessing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
