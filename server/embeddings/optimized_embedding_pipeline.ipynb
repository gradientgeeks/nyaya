{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52057f9c",
   "metadata": {},
   "source": [
    "# Optimized InLegalBERT Embedding Pipeline\n",
    "\n",
    "This notebook implements an **efficient streaming approach** that:\n",
    "- Processes files in batches instead of loading everything into memory\n",
    "- Generates embeddings and saves them immediately\n",
    "- Uses minimal memory footprint\n",
    "- Supports resume functionality for interrupted processing\n",
    "\n",
    "## Key Improvements:\n",
    "- **Memory Efficient**: Only keeps current batch in memory\n",
    "- **Resumable**: Can continue from where it left off\n",
    "- **Incremental Saving**: Saves results as they're generated\n",
    "- **Progress Tracking**: Real-time progress monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19510dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"/home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF\"\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "test_path = os.path.join(base_path, \"test\") \n",
    "val_path = os.path.join(base_path, \"val\", \"val\")\n",
    "\n",
    "# Output path for embeddings\n",
    "embeddings_output_path = \"/home/uttam/B.Tech Major Project/nyaya/server/embeddings\"\n",
    "os.makedirs(embeddings_output_path, exist_ok=True)\n",
    "\n",
    "# Processing configuration\n",
    "FILES_PER_BATCH = 50  # Process 50 files at a time\n",
    "EMBEDDING_BATCH_SIZE = 8 if device.type == 'cuda' else 4\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Files per batch: {FILES_PER_BATCH}\")\n",
    "print(f\"  Embedding batch size: {EMBEDDING_BATCH_SIZE}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"  Output directory: {embeddings_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model once and reuse\n",
    "print(\"Loading InLegalBERT model and tokenizer...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì InLegalBERT loaded successfully!\")\n",
    "    print(f\"‚úì Hidden size: {model.config.hidden_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading InLegalBERT: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea34283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_batch(texts, tokenizer, model, device, max_length=512, batch_size=8):\n",
    "    \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {key: val.to(device) for key, val in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            batch_embeddings = cls_embeddings.cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    if embeddings:\n",
    "        return np.vstack(embeddings)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def append_to_json_file(data, file_path):\n",
    "    \"\"\"Append data to JSON file (create if doesn't exist)\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Read existing data\n",
    "        with open(file_path, 'r') as f:\n",
    "            try:\n",
    "                existing_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "        \n",
    "        # Append new data\n",
    "        existing_data.extend(data)\n",
    "    else:\n",
    "        existing_data = data\n",
    "    \n",
    "    # Write back to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=2)\n",
    "    \n",
    "    return len(existing_data)\n",
    "\n",
    "\n",
    "def get_progress_file(dataset_name):\n",
    "    \"\"\"Get progress tracking file for resuming\"\"\"\n",
    "    return os.path.join(embeddings_output_path, f\"{dataset_name}_progress.json\")\n",
    "\n",
    "\n",
    "def save_progress(dataset_name, processed_files):\n",
    "    \"\"\"Save processing progress\"\"\"\n",
    "    progress_file = get_progress_file(dataset_name)\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump({\"processed_files\": processed_files}, f)\n",
    "\n",
    "\n",
    "def load_progress(dataset_name):\n",
    "    \"\"\"Load processing progress\"\"\"\n",
    "    progress_file = get_progress_file(dataset_name)\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f).get(\"processed_files\", [])\n",
    "    return []\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc133b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_streaming(dataset_path, output_filename, dataset_name, has_labels=True, \n",
    "                            label_mapping=None, files_per_batch=50):\n",
    "    \"\"\"\n",
    "    Process dataset in streaming fashion: load batch -> generate embeddings -> save -> repeat\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset directory\n",
    "        output_filename: Output JSON filename\n",
    "        dataset_name: Name for progress tracking\n",
    "        has_labels: Whether dataset has labels\n",
    "        label_mapping: Dictionary mapping label names to numbers\n",
    "        files_per_batch: Number of files to process in each batch\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ö†Ô∏è Dataset path not found: {dataset_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = [f for f in os.listdir(dataset_path) if f.endswith('.txt')]\n",
    "    print(f\"üìÅ Found {len(all_files)} files in {dataset_name}\")\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"‚ö†Ô∏è No .txt files found in {dataset_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Load progress and filter already processed files\n",
    "    processed_files = load_progress(dataset_name)\n",
    "    remaining_files = [f for f in all_files if f not in processed_files]\n",
    "    \n",
    "    print(f\"üìã Progress: {len(processed_files)} already processed, {len(remaining_files)} remaining\")\n",
    "    \n",
    "    if not remaining_files:\n",
    "        print(f\"‚úÖ All files already processed for {dataset_name}\")\n",
    "        return len(processed_files)\n",
    "    \n",
    "    output_file_path = os.path.join(embeddings_output_path, output_filename)\n",
    "    total_processed = len(processed_files)\n",
    "    \n",
    "    # Process files in batches\n",
    "    for batch_start in tqdm(range(0, len(remaining_files), files_per_batch), \n",
    "                           desc=f\"Processing {dataset_name} batches\"):\n",
    "        \n",
    "        batch_files = remaining_files[batch_start:batch_start + files_per_batch]\n",
    "        print(f\"\\nüîÑ Processing batch {batch_start//files_per_batch + 1}: {len(batch_files)} files\")\n",
    "        \n",
    "        # Load current batch\n",
    "        batch_texts = []\n",
    "        batch_labels = []\n",
    "        batch_label_numbers = []\n",
    "        \n",
    "        for filename in batch_files:\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            try:\n",
    "                if has_labels:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "                    if not df.empty:\n",
    "                        # Process labels\n",
    "                        df[\"label\"] = df[\"label\"].fillna(\"None\").astype(str).str.strip()\n",
    "                        df[\"label\"] = df[\"label\"].replace({\"none\": \"None\", \"NONE\": \"None\"})\n",
    "                        \n",
    "                        batch_texts.extend(df[\"text\"].tolist())\n",
    "                        labels = df[\"label\"].tolist()\n",
    "                        batch_labels.extend(labels)\n",
    "                        \n",
    "                        # Map labels to numbers\n",
    "                        if label_mapping:\n",
    "                            label_nums = [label_mapping.get(label, 6) for label in labels]  # 6 for unknown\n",
    "                            batch_label_numbers.extend(label_nums)\n",
    "                        else:\n",
    "                            batch_label_numbers.extend([0] * len(labels))  # Default\n",
    "                else:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\"])\n",
    "                    if not df.empty:\n",
    "                        batch_texts.extend(df[\"text\"].astype(str).str.strip().tolist())\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {filename}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_texts:\n",
    "            print(f\"‚ö†Ô∏è No valid texts in current batch\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   Loaded {len(batch_texts)} text samples\")\n",
    "        \n",
    "        # Generate embeddings for current batch\n",
    "        print(f\"   üß† Generating embeddings...\")\n",
    "        batch_embeddings = get_bert_embeddings_batch(\n",
    "            batch_texts, tokenizer, model, device, MAX_LENGTH, EMBEDDING_BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        if batch_embeddings.size == 0:\n",
    "            print(f\"‚ö†Ô∏è Failed to generate embeddings for batch\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   ‚úì Generated embeddings shape: {batch_embeddings.shape}\")\n",
    "        \n",
    "        # Create JSON data for current batch\n",
    "        batch_json_data = []\n",
    "        for i in range(len(batch_texts)):\n",
    "            data_point = {\n",
    "                \"text\": batch_texts[i],\n",
    "                \"vector\": batch_embeddings[i].tolist()\n",
    "            }\n",
    "            \n",
    "            if has_labels and batch_labels and batch_label_numbers:\n",
    "                data_point[\"classname\"] = batch_labels[i] if i < len(batch_labels) else None\n",
    "                data_point[\"classnumber\"] = int(batch_label_numbers[i]) if i < len(batch_label_numbers) else None\n",
    "            else:\n",
    "                data_point[\"classname\"] = None\n",
    "                data_point[\"classnumber\"] = None\n",
    "            \n",
    "            batch_json_data.append(data_point)\n",
    "        \n",
    "        # Save current batch results\n",
    "        print(f\"   üíæ Saving {len(batch_json_data)} samples...\")\n",
    "        total_samples = append_to_json_file(batch_json_data, output_file_path)\n",
    "        \n",
    "        # Update progress\n",
    "        processed_files.extend(batch_files)\n",
    "        save_progress(dataset_name, processed_files)\n",
    "        total_processed += len(batch_files)\n",
    "        \n",
    "        print(f\"   ‚úÖ Batch saved. Total samples in file: {total_samples}\")\n",
    "        \n",
    "        # Free memory\n",
    "        del batch_texts, batch_embeddings, batch_json_data\n",
    "        if has_labels:\n",
    "            del batch_labels, batch_label_numbers\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {dataset_name} processing completed!\")\n",
    "    print(f\"   Total files processed: {total_processed}\")\n",
    "    \n",
    "    # Clean up progress file\n",
    "    progress_file = get_progress_file(dataset_name)\n",
    "    if os.path.exists(progress_file):\n",
    "        os.remove(progress_file)\n",
    "    \n",
    "    return total_processed\n",
    "\n",
    "print(\"‚úì Streaming processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07320234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label_to_num = {\n",
    "    'Facts': 0,\n",
    "    'Reasoning': 1, \n",
    "    'Arguments of Respondent': 2,\n",
    "    'Arguments of Petitioner': 3,\n",
    "    'Decision': 4,\n",
    "    'Issue': 5,\n",
    "    'None': 6\n",
    "}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, num in label_to_num.items():\n",
    "    print(f\"  {num}: {label}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_file_path = os.path.join(embeddings_output_path, 'label_mapping_inlegalbert.json')\n",
    "with open(label_file_path, 'w') as f:\n",
    "    json.dump({v: k for k, v in label_to_num.items()}, f, indent=2)\n",
    "print(f\"\\n‚úì Label mapping saved to: {label_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all datasets using streaming approach with NPZ format\n",
    "print(\"=\"*80)\n",
    "print(\"PROCESSING ALL DATASETS WITH STREAMING APPROACH (NPZ FORMAT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process training data (with labels)\n",
    "print(\"\\nüéØ Processing Training Data...\")\n",
    "train_count = process_dataset_streaming_npz(\n",
    "    dataset_path=train_path,\n",
    "    output_filename='train_embeddings_inlegalbert.npz',\n",
    "    dataset_name='train',\n",
    "    has_labels=True,\n",
    "    label_mapping=label_to_num,\n",
    "    files_per_batch=FILES_PER_BATCH\n",
    ")\n",
    "\n",
    "# Process test data (no labels)\n",
    "print(\"\\nüéØ Processing Test Data...\")\n",
    "test_count = process_dataset_streaming_npz(\n",
    "    dataset_path=test_path,\n",
    "    output_filename='test_embeddings_inlegalbert.npz',\n",
    "    dataset_name='test',\n",
    "    has_labels=False,\n",
    "    files_per_batch=FILES_PER_BATCH\n",
    ")\n",
    "\n",
    "# Process validation data (no labels)\n",
    "print(\"\\nüéØ Processing Validation Data...\")\n",
    "val_count = process_dataset_streaming_npz(\n",
    "    dataset_path=val_path,\n",
    "    output_filename='val_embeddings_inlegalbert.npz',\n",
    "    dataset_name='val',\n",
    "    has_labels=False,\n",
    "    files_per_batch=FILES_PER_BATCH\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_files = train_count + test_count + val_count\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Training files processed: {train_count}\")\n",
    "print(f\"‚úÖ Test files processed: {test_count}\")\n",
    "print(f\"‚úÖ Validation files processed: {val_count}\")\n",
    "print(f\"‚úÖ Total files processed: {total_files}\")\n",
    "print(f\"‚úÖ Total processing time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"‚úÖ Average time per file: {total_time/total_files:.3f} seconds\")\n",
    "print(f\"‚úÖ Output directory: {embeddings_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final file information\n",
    "print(\"üìÇ Generated Files:\")\n",
    "output_files = [\n",
    "    'train_embeddings_inlegalbert.npz',\n",
    "    'test_embeddings_inlegalbert.npz', \n",
    "    'val_embeddings_inlegalbert.npz',\n",
    "    'label_mapping_inlegalbert.json'\n",
    "]\n",
    "\n",
    "total_size_mb = 0\n",
    "for filename in output_files:\n",
    "    filepath = os.path.join(embeddings_output_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024*1024)\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"  ‚úì {filename}: {size_mb:.1f} MB\")\n",
    "        \n",
    "        # Show sample info for NPZ files\n",
    "        if filename.endswith('.npz'):\n",
    "            with np.load(filepath, allow_pickle=True) as data:\n",
    "                vectors = data['vectors']\n",
    "                texts = data['texts']\n",
    "                metadata = data['metadata'].item()\n",
    "                print(f\"    üìä Samples: {len(vectors)}, Embedding dim: {vectors.shape[1]}\")\n",
    "                print(f\"    üìÖ Created: {metadata.get('created_date', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {filename}: Not found\")\n",
    "\n",
    "print(f\"\\nüìä Total output size: {total_size_mb:.1f} MB\")\n",
    "print(f\"üíæ NPZ format saves ~90% space compared to JSON!\")\n",
    "\n",
    "# Memory cleanup\n",
    "print(\"\\nüßπ Final cleanup...\")\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"‚úÖ Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìñ How to Load and Use NPZ Files\n",
    "print(\"=\" * 60)\n",
    "print(\"HOW TO LOAD AND USE THE GENERATED NPZ FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def load_embeddings_from_npz(npz_file_path):\n",
    "    \"\"\"Load embeddings from NPZ file\"\"\"\n",
    "    if not os.path.exists(npz_file_path):\n",
    "        print(f\"File not found: {npz_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    with np.load(npz_file_path, allow_pickle=True) as data:\n",
    "        result = {\n",
    "            'vectors': data['vectors'],\n",
    "            'texts': data['texts'],\n",
    "            'metadata': data['metadata'].item()\n",
    "        }\n",
    "        \n",
    "        # Add labels if they exist\n",
    "        if 'labels' in data:\n",
    "            result['labels'] = data['labels']\n",
    "            result['label_numbers'] = data['label_numbers']\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "print(\"üîç Example: Loading training embeddings...\")\n",
    "train_npz_path = os.path.join(embeddings_output_path, 'train_embeddings_inlegalbert.npz')\n",
    "\n",
    "if os.path.exists(train_npz_path):\n",
    "    # Load the data\n",
    "    train_data = load_embeddings_from_npz(train_npz_path)\n",
    "    \n",
    "    print(f\"‚úì Loaded training data:\")\n",
    "    print(f\"  üìä Vectors shape: {train_data['vectors'].shape}\")\n",
    "    print(f\"  üìä Number of texts: {len(train_data['texts'])}\")\n",
    "    print(f\"  üìä Has labels: {'labels' in train_data}\")\n",
    "    print(f\"  üìä Embedding dimension: {train_data['vectors'].shape[1]}\")\n",
    "    \n",
    "    if 'labels' in train_data:\n",
    "        print(f\"  üìä Unique labels: {np.unique(train_data['labels'])}\")\n",
    "        print(f\"  üìä Label distribution:\")\n",
    "        unique_labels, counts = np.unique(train_data['labels'], return_counts=True)\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f\"    {label}: {count} samples\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nüìù Sample data:\")\n",
    "    print(f\"  Text: {train_data['texts'][0][:100]}...\")\n",
    "    print(f\"  Vector (first 5 values): {train_data['vectors'][0][:5]}\")\n",
    "    if 'labels' in train_data:\n",
    "        print(f\"  Label: {train_data['labels'][0]}\")\n",
    "        print(f\"  Label number: {train_data['label_numbers'][0]}\")\n",
    "    \n",
    "    print(f\"\\nüí° Usage examples:\")\n",
    "    print(f\"  # Get all embeddings: embeddings = train_data['vectors']\")\n",
    "    print(f\"  # Get all texts: texts = train_data['texts']\")\n",
    "    print(f\"  # Get all labels: labels = train_data['labels']\")\n",
    "    print(f\"  # Convert to PyTorch tensor: torch.tensor(train_data['vectors'])\")\n",
    "    print(f\"  # Use for sklearn: from sklearn.model_selection import train_test_split\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training NPZ file not found. Run the processing cells first.\")\n",
    "\n",
    "print(f\"\\nüìÅ File locations:\")\n",
    "for dataset in ['train', 'test', 'val']:\n",
    "    npz_path = os.path.join(embeddings_output_path, f'{dataset}_embeddings_inlegalbert.npz')\n",
    "    if os.path.exists(npz_path):\n",
    "        size_mb = os.path.getsize(npz_path) / (1024*1024)\n",
    "        print(f\"  ‚úì {dataset}: {npz_path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {dataset}: Not generated yet\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea2465",
   "metadata": {},
   "source": [
    "# üîç Understanding the Numbers in InLegalBERT Processing\n",
    "\n",
    "This section clarifies the different numerical parameters used in the embedding pipeline.\n",
    "\n",
    "## Key Numbers Explained:\n",
    "\n",
    "### 1. **512 = Max Sequence Length (Token Limit)**\n",
    "- **What it is**: Maximum number of tokens (words/subwords) InLegalBERT can process at once\n",
    "- **Where used**: In tokenization step\n",
    "- **Why 512**: BERT models have a fixed context window limit\n",
    "- **Effect**: Longer texts get truncated to 512 tokens\n",
    "\n",
    "### 2. **768 = Embedding Dimension (Vector Size)**  \n",
    "- **What it is**: Size of the output embedding vector for each text\n",
    "- **Where used**: Output of InLegalBERT model\n",
    "- **Why 768**: Built into InLegalBERT architecture (BERT-base standard)\n",
    "- **Effect**: Each text becomes a 768-dimensional vector\n",
    "\n",
    "### 3. **50 = Files Per Batch**\n",
    "- **What it is**: Number of files processed together in each batch\n",
    "- **Where used**: Streaming processing to manage memory\n",
    "- **Why 50**: Balance between memory usage and efficiency\n",
    "- **Effect**: Processes 50 files ‚Üí generates embeddings ‚Üí saves ‚Üí repeats\n",
    "\n",
    "### 4. **8/4 = Embedding Batch Size**\n",
    "- **What it is**: Number of texts processed simultaneously for embedding generation\n",
    "- **Where used**: Inside embedding generation function\n",
    "- **Why 8/4**: GPU memory management (8 for CUDA, 4 for CPU)\n",
    "- **Effect**: Processes multiple texts in parallel for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc814d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Practical Demonstration of the 512 Token Limit\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: 512 TOKEN LIMIT vs 768 EMBEDDING SIZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample legal text to demonstrate tokenization\n",
    "sample_legal_text = \"\"\"\n",
    "The Hon'ble Supreme Court of India in the landmark case of Kesavananda Bharati vs State of Kerala \n",
    "held that the basic structure of the Constitution cannot be altered by Parliament through constitutional \n",
    "amendments. This doctrine ensures that fundamental principles like democracy, secularism, federalism, \n",
    "and judicial review remain intact. The Court emphasized that while Parliament has wide powers to amend \n",
    "the Constitution under Article 368, these powers are not unlimited and cannot be used to destroy the \n",
    "very essence of the constitutional framework that gives it legitimacy.\n",
    "\"\"\"\n",
    "\n",
    "# If model and tokenizer are available, demonstrate tokenization\n",
    "if 'tokenizer' in globals():\n",
    "    print(\"üî§ Tokenizing sample legal text...\")\n",
    "    \n",
    "    # Tokenize the sample text\n",
    "    tokens = tokenizer.tokenize(sample_legal_text)\n",
    "    encoded = tokenizer(\n",
    "        sample_legal_text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,  # This is our MAX_LENGTH\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Original text length: {len(sample_legal_text)} characters\")\n",
    "    print(f\"üìä Number of tokens: {len(tokens)}\")\n",
    "    print(f\"üìä Encoded input_ids shape: {encoded['input_ids'].shape}\")\n",
    "    print(f\"üìä Max allowed tokens: 512\")\n",
    "    \n",
    "    # Show first few tokens\n",
    "    print(f\"\\nüî§ First 10 tokens: {tokens[:10]}\")\n",
    "    print(f\"üî§ Last 10 tokens: {tokens[-10:]}\")\n",
    "    \n",
    "    # If we had a model, this would show embedding dimension\n",
    "    if 'model' in globals():\n",
    "        print(f\"\\nüß† If we generate embeddings:\")\n",
    "        print(f\"   Input shape: (1, {encoded['input_ids'].shape[1]}) tokens\")\n",
    "        print(f\"   Output shape: (1, 768) embedding vector\")\n",
    "        print(f\"   ‚Ü≥ 512 is INPUT limit, 768 is OUTPUT size\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insight:\")\n",
    "    print(f\"   ‚Ä¢ 512 = Maximum INPUT tokens (text length limit)\")\n",
    "    print(f\"   ‚Ä¢ 768 = OUTPUT embedding dimension (vector size)\")\n",
    "    print(f\"   ‚Ä¢ These are completely different concepts!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer not available (run previous cells first)\")\n",
    "    print(\"\\nüìù Explanation without demonstration:\")\n",
    "    print(\"   ‚Ä¢ MAX_LENGTH = 512 means text longer than 512 tokens gets truncated\")\n",
    "    print(\"   ‚Ä¢ Embedding dimension = 768 means output vector has 768 numbers\")\n",
    "    print(\"   ‚Ä¢ Example: 'This is a very long legal document...' ‚Üí [0.1, -0.2, 0.3, ... 768 numbers]\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748fd17",
   "metadata": {},
   "source": [
    "# üíæ NPZ Format: Space-Efficient Storage\n",
    "\n",
    "**Problem**: JSON format is very space-inefficient for large numerical arrays\n",
    "**Solution**: Use NumPy's NPZ format for compressed binary storage\n",
    "\n",
    "## NPZ vs JSON Comparison:\n",
    "\n",
    "| Format | File Size | Load Speed | Memory Usage | Human Readable |\n",
    "|--------|-----------|------------|--------------|----------------|\n",
    "| **JSON** | ~10x larger | Slow | High | ‚úÖ Yes |\n",
    "| **NPZ** | Compact | Fast | Low | ‚ùå No |\n",
    "\n",
    "## NPZ File Structure:\n",
    "Each NPZ file will contain:\n",
    "- `vectors`: 2D array of embeddings (samples √ó 768)\n",
    "- `texts`: Array of text strings  \n",
    "- `labels`: Array of class names (for training data)\n",
    "- `label_numbers`: Array of class numbers (for training data)\n",
    "- `metadata`: Dictionary with dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Updated Functions for NPZ Format\n",
    "def append_to_npz_file(batch_vectors, batch_texts, batch_labels, batch_label_numbers, file_path, dataset_name):\n",
    "    \"\"\"Append data to NPZ file (create if doesn't exist)\"\"\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load existing data\n",
    "        with np.load(file_path, allow_pickle=True) as existing:\n",
    "            existing_vectors = existing['vectors']\n",
    "            existing_texts = existing['texts']\n",
    "            existing_labels = existing.get('labels', np.array([]))\n",
    "            existing_label_numbers = existing.get('label_numbers', np.array([]))\n",
    "        \n",
    "        # Concatenate new data\n",
    "        all_vectors = np.vstack([existing_vectors, batch_vectors])\n",
    "        all_texts = np.concatenate([existing_texts, batch_texts])\n",
    "        \n",
    "        if len(existing_labels) > 0 and len(batch_labels) > 0:\n",
    "            all_labels = np.concatenate([existing_labels, batch_labels])\n",
    "            all_label_numbers = np.concatenate([existing_label_numbers, batch_label_numbers])\n",
    "        elif len(batch_labels) > 0:\n",
    "            all_labels = np.array(batch_labels)\n",
    "            all_label_numbers = np.array(batch_label_numbers)\n",
    "        else:\n",
    "            all_labels = existing_labels\n",
    "            all_label_numbers = existing_label_numbers\n",
    "    else:\n",
    "        # First time - create new arrays\n",
    "        all_vectors = batch_vectors\n",
    "        all_texts = np.array(batch_texts)\n",
    "        all_labels = np.array(batch_labels) if batch_labels else np.array([])\n",
    "        all_label_numbers = np.array(batch_label_numbers) if batch_label_numbers else np.array([])\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'embedding_model': 'law-ai/InLegalBERT',\n",
    "        'embedding_dim': all_vectors.shape[1],\n",
    "        'total_samples': len(all_vectors),\n",
    "        'has_labels': len(all_labels) > 0,\n",
    "        'created_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Save to NPZ format\n",
    "    if len(all_labels) > 0:\n",
    "        np.savez_compressed(\n",
    "            file_path,\n",
    "            vectors=all_vectors,\n",
    "            texts=all_texts,\n",
    "            labels=all_labels,\n",
    "            label_numbers=all_label_numbers,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        np.savez_compressed(\n",
    "            file_path,\n",
    "            vectors=all_vectors,\n",
    "            texts=all_texts,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    \n",
    "    return len(all_vectors)\n",
    "\n",
    "\n",
    "def save_labels_to_json(label_mapping, file_path):\n",
    "    \"\"\"Save label mapping to JSON for reference\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "\n",
    "print(\"‚úì NPZ helper functions defined\")\n",
    "print(\"‚úì NPZ format will save ~90% storage space compared to JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f78555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Updated Streaming Processing Function for NPZ Format\n",
    "def process_dataset_streaming_npz(dataset_path, output_filename, dataset_name, has_labels=True, \n",
    "                                label_mapping=None, files_per_batch=50):\n",
    "    \"\"\"\n",
    "    Process dataset in streaming fashion and save to NPZ format\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset directory\n",
    "        output_filename: Output NPZ filename  \n",
    "        dataset_name: Name for progress tracking\n",
    "        has_labels: Whether dataset has labels\n",
    "        label_mapping: Dictionary mapping label names to numbers\n",
    "        files_per_batch: Number of files to process in each batch\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ö†Ô∏è Dataset path not found: {dataset_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Get all files\n",
    "    all_files = [f for f in os.listdir(dataset_path) if f.endswith('.txt')]\n",
    "    print(f\"üìÅ Found {len(all_files)} files in {dataset_name}\")\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"‚ö†Ô∏è No .txt files found in {dataset_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Load progress and filter already processed files\n",
    "    processed_files = load_progress(dataset_name)\n",
    "    remaining_files = [f for f in all_files if f not in processed_files]\n",
    "    \n",
    "    print(f\"üìã Progress: {len(processed_files)} already processed, {len(remaining_files)} remaining\")\n",
    "    \n",
    "    if not remaining_files:\n",
    "        print(f\"‚úÖ All files already processed for {dataset_name}\")\n",
    "        return len(processed_files)\n",
    "    \n",
    "    output_file_path = os.path.join(embeddings_output_path, output_filename)\n",
    "    total_processed = len(processed_files)\n",
    "    \n",
    "    # Process files in batches\n",
    "    for batch_start in tqdm(range(0, len(remaining_files), files_per_batch), \n",
    "                           desc=f\"Processing {dataset_name} batches\"):\n",
    "        \n",
    "        batch_files = remaining_files[batch_start:batch_start + files_per_batch]\n",
    "        print(f\"\\\\nüîÑ Processing batch {batch_start//files_per_batch + 1}: {len(batch_files)} files\")\n",
    "        \n",
    "        # Load current batch\n",
    "        batch_texts = []\n",
    "        batch_labels = []\n",
    "        batch_label_numbers = []\n",
    "        \n",
    "        for filename in batch_files:\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            try:\n",
    "                if has_labels:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\\\t\", header=None, names=[\"text\", \"label\"])\n",
    "                    if not df.empty:\n",
    "                        # Process labels\n",
    "                        df[\"label\"] = df[\"label\"].fillna(\"None\").astype(str).str.strip()\n",
    "                        df[\"label\"] = df[\"label\"].replace({\"none\": \"None\", \"NONE\": \"None\"})\n",
    "                        \n",
    "                        batch_texts.extend(df[\"text\"].tolist())\n",
    "                        labels = df[\"label\"].tolist()\n",
    "                        batch_labels.extend(labels)\n",
    "                        \n",
    "                        # Map labels to numbers\n",
    "                        if label_mapping:\n",
    "                            label_nums = [label_mapping.get(label, 6) for label in labels]  # 6 for unknown\n",
    "                            batch_label_numbers.extend(label_nums)\n",
    "                        else:\n",
    "                            batch_label_numbers.extend([0] * len(labels))  # Default\n",
    "                else:\n",
    "                    df = pd.read_csv(file_path, sep=\"\\\\t\", header=None, names=[\"text\"])\n",
    "                    if not df.empty:\n",
    "                        batch_texts.extend(df[\"text\"].astype(str).str.strip().tolist())\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {filename}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_texts:\n",
    "            print(f\"‚ö†Ô∏è No valid texts in current batch\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   Loaded {len(batch_texts)} text samples\")\n",
    "        \n",
    "        # Generate embeddings for current batch\n",
    "        print(f\"   üß† Generating embeddings...\")\n",
    "        batch_embeddings = get_bert_embeddings_batch(\n",
    "            batch_texts, tokenizer, model, device, MAX_LENGTH, EMBEDDING_BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        if batch_embeddings.size == 0:\n",
    "            print(f\"‚ö†Ô∏è Failed to generate embeddings for batch\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   ‚úì Generated embeddings shape: {batch_embeddings.shape}\")\n",
    "        \n",
    "        # Save current batch results to NPZ\n",
    "        print(f\"   üíæ Saving {len(batch_texts)} samples to NPZ...\")\n",
    "        total_samples = append_to_npz_file(\n",
    "            batch_embeddings, \n",
    "            batch_texts, \n",
    "            batch_labels if has_labels else [], \n",
    "            batch_label_numbers if has_labels else [],\n",
    "            output_file_path,\n",
    "            dataset_name\n",
    "        )\n",
    "        \n",
    "        # Update progress\n",
    "        processed_files.extend(batch_files)\n",
    "        save_progress(dataset_name, processed_files)\n",
    "        total_processed += len(batch_files)\n",
    "        \n",
    "        print(f\"   ‚úÖ Batch saved to NPZ. Total samples in file: {total_samples}\")\n",
    "        \n",
    "        # Show file size comparison\n",
    "        if os.path.exists(output_file_path):\n",
    "            npz_size_mb = os.path.getsize(output_file_path) / (1024*1024)\n",
    "            print(f\"   üìä Current NPZ file size: {npz_size_mb:.1f} MB\")\n",
    "        \n",
    "        # Free memory\n",
    "        del batch_texts, batch_embeddings\n",
    "        if has_labels:\n",
    "            del batch_labels, batch_label_numbers\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ {dataset_name} processing completed!\")\n",
    "    print(f\"   Total files processed: {total_processed}\")\n",
    "    \n",
    "    # Clean up progress file\n",
    "    progress_file = get_progress_file(dataset_name)\n",
    "    if os.path.exists(progress_file):\n",
    "        os.remove(progress_file)\n",
    "    \n",
    "    return total_processed\n",
    "\n",
    "print(\"‚úì NPZ streaming processing function defined\")\n",
    "print(\"‚úì This will save embeddings in compressed NumPy format\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
