{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12921034,"sourceType":"datasetVersion","datasetId":8175930}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# InLegalBERT Embedding Generation for Legal Text Classification\n\nThis notebook generates embeddings for legal text classification using **InLegalBERT** - a BERT model specifically pre-trained on Indian legal documents.\n\n## Dataset Structure\n- **Train**: Files with text and labels (Facts, Reasoning, Arguments of Respondent, Arguments of Petitioner, Decision, Issue)\n- **Test**: Files with only text (no labels)\n- **Val**: Files with only text (no labels)\n\n## Output\n- JSON files with text, embeddings, class names, and class numbers\n- Saved in the embeddings folder","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport os\nimport numpy as np\nimport json\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:03:52.112009Z","iopub.execute_input":"2025-09-02T16:03:52.112274Z","iopub.status.idle":"2025-09-02T16:03:52.197062Z","shell.execute_reply.started":"2025-09-02T16:03:52.112253Z","shell.execute_reply":"2025-09-02T16:03:52.195790Z"}},"outputs":[{"name":"stdout","text":"Torch version: 2.6.0+cu124\nCUDA available: True\nDevice: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set device for GPU acceleration if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Dataset paths (adjusted for local structure)\nbase_path = \"/kaggle/input/dataset/Hier_BiLSTM_CRF\"\ntrain_path = os.path.join(base_path, \"train\")\ntest_path = os.path.join(base_path, \"test\") \nval_path = os.path.join(base_path, \"val\", \"val\")\n\n# Output path for embeddings\nembeddings_output_path = \"/kaggle/working/\"\n\nprint(f\"Train path: {train_path}\")\nprint(f\"Test path: {test_path}\")\nprint(f\"Val path: {val_path}\")\nprint(f\"Output path: {embeddings_output_path}\")\n\n# Verify paths exist\nfor path_name, path in [(\"Train\", train_path), (\"Test\", test_path), (\"Val\", val_path)]:\n    if os.path.exists(path):\n        file_count = len([f for f in os.listdir(path) if f.endswith('.txt')])\n        print(f\"‚úì {path_name} directory exists with {file_count} files\")\n    else:\n        print(f\"‚úó {path_name} directory not found: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:03:55.315401Z","iopub.execute_input":"2025-09-02T16:03:55.316066Z","iopub.status.idle":"2025-09-02T16:03:55.399546Z","shell.execute_reply.started":"2025-09-02T16:03:55.316028Z","shell.execute_reply":"2025-09-02T16:03:55.398763Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain path: /kaggle/input/dataset/Hier_BiLSTM_CRF/train\nTest path: /kaggle/input/dataset/Hier_BiLSTM_CRF/test\nVal path: /kaggle/input/dataset/Hier_BiLSTM_CRF/val/val\nOutput path: /kaggle/working/\n‚úì Train directory exists with 4994 files\n‚úì Test directory exists with 712 files\n‚úì Val directory exists with 1424 files\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_train_files(directory_path):\n    \"\"\"Load training files with labels\"\"\"\n    all_dfs = []\n    \n    print(f\"Loading training files from: {directory_path}\")\n    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n    print(f\"Found {len(files)} files\")\n    \n    for file_name in tqdm(files, desc=\"Loading train files\"):\n        file_path = os.path.join(directory_path, file_name)\n        try:\n            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n            if not df.empty:\n                # üîß Replace NaN with \"None\"\n                df[\"label\"] = df[\"label\"].fillna(\"None\")\n                # üîß Normalize label values\n                df[\"label\"] = df[\"label\"].astype(str).str.strip()\n                df[\"label\"] = df[\"label\"].replace(\n                    {\"none\": \"None\", \"NONE\": \"None\"}  # unify casing\n                )\n                all_dfs.append(df)\n        except Exception as e:\n            print(f\"Error loading {file_name}: {e}\")\n    \n    if all_dfs:\n        result_df = pd.concat(all_dfs, ignore_index=True)\n        print(f\"‚úÖ Successfully loaded {len(result_df)} training samples\")\n        return result_df\n    else:\n        print(\"‚ö†Ô∏è No valid training data found\")\n        return pd.DataFrame(columns=[\"text\", \"label\"])\n\n\ndef load_test_val_files(directory_path):\n    \"\"\"Load test/val files with only text (no labels)\"\"\"\n    all_dfs = []\n    \n    print(f\"Loading test/val files from: {directory_path}\")\n    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n    print(f\"Found {len(files)} files\")\n    \n    for file_name in tqdm(files, desc=\"Loading test/val files\"):\n        file_path = os.path.join(directory_path, file_name)\n        try:\n            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\"])\n            if not df.empty:\n                # üîß Normalize text (strip whitespace)\n                df[\"text\"] = df[\"text\"].astype(str).str.strip()\n                all_dfs.append(df)\n        except Exception as e:\n            print(f\"Error loading {file_name}: {e}\")\n    \n    if all_dfs:\n        result_df = pd.concat(all_dfs, ignore_index=True)\n        print(f\"‚úÖ Successfully loaded {len(result_df)} samples\")\n        return result_df\n    else:\n        print(\"‚ö†Ô∏è No valid test/val data found\")\n        return pd.DataFrame(columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:03:58.730649Z","iopub.execute_input":"2025-09-02T16:03:58.731388Z","iopub.status.idle":"2025-09-02T16:03:58.740588Z","shell.execute_reply.started":"2025-09-02T16:03:58.731360Z","shell.execute_reply":"2025-09-02T16:03:58.739704Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load datasets\nprint(\"Loading datasets...\")\ndf_train = load_train_files(train_path)  # has text + label\ndf_test = load_test_val_files(test_path)  # only text  \ndf_val = load_test_val_files(val_path)    # only text\n\n# Show results\nprint(f\"\\nDataset Summary:\")\nprint(f\"Train: {len(df_train)} rows\")\nprint(f\"Test: {len(df_test)} rows\")\nprint(f\"Val: {len(df_val)} rows\")\n\nif not df_train.empty:\n    print(f\"\\nTrain labels distribution:\")\n    print(df_train[\"label\"].value_counts())\n    \n    print(f\"\\nSample train data:\")\n    print(df_train.head())\nelse:\n    print(\"No training data loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:04:00.014437Z","iopub.execute_input":"2025-09-02T16:04:00.014699Z","iopub.status.idle":"2025-09-02T16:04:39.965864Z","shell.execute_reply.started":"2025-09-02T16:04:00.014679Z","shell.execute_reply":"2025-09-02T16:04:39.965103Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nLoading training files from: /kaggle/input/dataset/Hier_BiLSTM_CRF/train\nFound 4994 files\n","output_type":"stream"},{"name":"stderr","text":"Loading train files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4994/4994 [00:28<00:00, 173.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded 1123832 training samples\nLoading test/val files from: /kaggle/input/dataset/Hier_BiLSTM_CRF/test\nFound 712 files\n","output_type":"stream"},{"name":"stderr","text":"Loading test/val files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 712/712 [00:03<00:00, 199.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded 149868 samples\nLoading test/val files from: /kaggle/input/dataset/Hier_BiLSTM_CRF/val/val\nFound 1424 files\n","output_type":"stream"},{"name":"stderr","text":"Loading test/val files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1424/1424 [00:07<00:00, 199.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded 293408 samples\n\nDataset Summary:\nTrain: 1123832 rows\nTest: 149868 rows\nVal: 293408 rows\n\nTrain labels distribution:\nlabel\nNone                       603585\nReasoning                  202593\nFacts                      170068\nArguments of Petitioner     65032\nArguments of Respondent     50137\nDecision                    19599\nIssue                       12818\nName: count, dtype: int64\n\nSample train data:\n                                                text  label\n0   M. JOSEPH, J. The appeal is directed against ...  Issue\n1  The chargesheet came to be filed on the basis ...  Facts\n2  The appellant was Director of Mines and Geolog...  Facts\n3  There was a partnership firm by the name M s A...  Facts\n4  The offences are alleged to revolve around the...  Facts\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Label encoding for training data\nif not df_train.empty:\n    # Manual mapping (similar to your original code)\n    label_to_num = {\n        'Facts': 0,\n        'Reasoning': 1, \n        'Arguments of Respondent': 2,\n        'Arguments of Petitioner': 3,\n        'Decision': 4,\n        'Issue': 5,\n        'None': 6\n    }\n    \n    print(\"Creating label mappings...\")\n    \n    # Check if all labels in data are in our mapping\n    unique_labels = df_train['label'].unique()\n    print(f\"Unique labels in dataset: {unique_labels}\")\n    \n    missing_labels = [label for label in unique_labels if label not in label_to_num]\n    if missing_labels:\n        print(f\"Warning: Missing labels in mapping: {missing_labels}\")\n        \n        # Use LabelEncoder as fallback for missing labels\n        label_encoder = LabelEncoder()\n        df_train['label_encoded'] = label_encoder.fit_transform(df_train['label'])\n        \n        # Create updated mapping\n        label_mapping = {}\n        for i, label in enumerate(label_encoder.classes_):\n            label_mapping[i] = label\n            \n        train_labels = df_train['label'].tolist()\n        train_label_numbers = df_train['label_encoded'].tolist()\n        \n        print(\"Using LabelEncoder mapping:\")\n        for i, label in label_mapping.items():\n            print(f\"{i}: {label}\")\n    else:\n        # Use manual mapping\n        df_train['label_numeric'] = df_train['label'].map(label_to_num)\n        \n        label_mapping = {v: k for k, v in label_to_num.items()}  # Reverse mapping\n        train_labels = df_train['label'].tolist()\n        train_label_numbers = df_train['label_numeric'].tolist()\n        \n        print(\"Using manual mapping:\")\n        for num, label in label_mapping.items():\n            print(f\"{num}: {label}\")\n            \n    print(f\"\\nLabel distribution by number:\")\n    label_counts = {}\n    for label_num in train_label_numbers:\n        label_counts[label_num] = label_counts.get(label_num, 0) + 1\n    for num, count in sorted(label_counts.items()):\n        print(f\"{num} ({label_mapping[num]}): {count}\")\nelse:\n    print(\"No training data available for label encoding\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:05:04.903994Z","iopub.execute_input":"2025-09-02T16:05:04.904286Z","iopub.status.idle":"2025-09-02T16:05:05.211067Z","shell.execute_reply.started":"2025-09-02T16:05:04.904263Z","shell.execute_reply":"2025-09-02T16:05:05.210306Z"}},"outputs":[{"name":"stdout","text":"Creating label mappings...\nUnique labels in dataset: ['Issue' 'Facts' 'None' 'Arguments of Petitioner'\n 'Arguments of Respondent' 'Reasoning' 'Decision']\nUsing manual mapping:\n0: Facts\n1: Reasoning\n2: Arguments of Respondent\n3: Arguments of Petitioner\n4: Decision\n5: Issue\n6: None\n\nLabel distribution by number:\n0 (Facts): 170068\n1 (Reasoning): 202593\n2 (Arguments of Respondent): 50137\n3 (Arguments of Petitioner): 65032\n4 (Decision): 19599\n5 (Issue): 12818\n6 (None): 603585\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load InLegalBERT model and tokenizer\nprint(\"Loading InLegalBERT model and tokenizer...\")\nprint(\"This may take a few minutes on first run...\")\n\ntry:\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n    model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n    \n    # Move model to device (GPU if available)\n    model = model.to(device)\n    model.eval()  # Set to evaluation mode\n    \n    print(f\"‚úì InLegalBERT loaded successfully!\")\n    print(f\"‚úì Model moved to: {device}\")\n    print(f\"‚úì Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n    print(f\"‚úì Model max position embeddings: {model.config.max_position_embeddings}\")\n    print(f\"‚úì Hidden size: {model.config.hidden_size}\")\n    \nexcept Exception as e:\n    print(f\"‚úó Error loading InLegalBERT: {e}\")\n    print(\"Please ensure you have internet connection and transformers library installed\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:05:08.218830Z","iopub.execute_input":"2025-09-02T16:05:08.219571Z","iopub.status.idle":"2025-09-02T16:05:38.340137Z","shell.execute_reply.started":"2025-09-02T16:05:08.219538Z","shell.execute_reply":"2025-09-02T16:05:38.339300Z"}},"outputs":[{"name":"stdout","text":"Loading InLegalBERT model and tokenizer...\nThis may take a few minutes on first run...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa58fc464c414b009123d2b8cd7bf6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83c2690b4f3b4af89ef842930a205d39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db94765d0ac4028a4a764ee2ee6a328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4608319ba2c145dd81e7c20b847aede4"}},"metadata":{}},{"name":"stderr","text":"2025-09-02 16:05:20.035920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756829120.226958      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756829120.284460      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/534M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2bfc606d5ea4599a15071efe9877753"}},"metadata":{}},{"name":"stdout","text":"‚úì InLegalBERT loaded successfully!\n‚úì Model moved to: cuda\n‚úì Tokenizer vocabulary size: 30522\n‚úì Model max position embeddings: 512\n‚úì Hidden size: 768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/534M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eadaffe986ca4b2583a16a35d0bd071c"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def get_bert_embeddings(texts, tokenizer, model, device, max_length=512, batch_size=8):\n    \"\"\"\n    Generate embeddings using InLegalBERT\n    \n    Args:\n        texts: List of text strings\n        tokenizer: InLegalBERT tokenizer\n        model: InLegalBERT model\n        device: torch device (cuda/cpu)\n        max_length: Maximum sequence length for BERT\n        batch_size: Batch size for processing\n    \n    Returns:\n        numpy array of embeddings (texts x hidden_size)\n    \"\"\"\n    embeddings = []\n    \n    # Process in batches to manage memory\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n        batch_texts = texts[i:i + batch_size]\n        \n        # Tokenize batch\n        encoded = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        \n        # Move to device\n        encoded = {key: val.to(device) for key, val in encoded.items()}\n        \n        # Generate embeddings\n        with torch.no_grad():\n            outputs = model(**encoded)\n            # Use [CLS] token embedding (first token) as sentence representation\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n            \n            # Move back to CPU and convert to numpy\n            batch_embeddings = cls_embeddings.cpu().numpy()\n            embeddings.append(batch_embeddings)\n    \n    # Concatenate all batches\n    all_embeddings = np.vstack(embeddings)\n    return all_embeddings\n\nprint(\"‚úì Embedding function defined\")\nprint(\"This function will use the [CLS] token representation as sentence embeddings\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:05:57.012621Z","iopub.execute_input":"2025-09-02T16:05:57.013670Z","iopub.status.idle":"2025-09-02T16:05:57.020690Z","shell.execute_reply.started":"2025-09-02T16:05:57.013636Z","shell.execute_reply":"2025-09-02T16:05:57.019901Z"}},"outputs":[{"name":"stdout","text":"‚úì Embedding function defined\nThis function will use the [CLS] token representation as sentence embeddings\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def create_json_data_bert(texts, embeddings, labels=None, label_numbers=None, dataset_name=\"\"):\n    \"\"\"Create JSON data with text, BERT embeddings, classname, classnumber\"\"\"\n    print(f\"Creating JSON data for {dataset_name}...\")\n    json_data = []\n    \n    for i in range(len(texts)):\n        data_point = {\n            \"text\": texts[i],\n            \"vector\": embeddings[i].tolist(),  # Convert numpy array to list for JSON\n        }\n        \n        if labels is not None and label_numbers is not None:\n            data_point[\"classname\"] = labels[i]\n            data_point[\"classnumber\"] = int(label_numbers[i])\n        else:\n            data_point[\"classname\"] = None\n            data_point[\"classnumber\"] = None\n            \n        json_data.append(data_point)\n        \n        # Progress indicator\n        if (i + 1) % 1000 == 0:\n            print(f\"  Processed {i + 1}/{len(texts)} samples\")\n    \n    print(f\"‚úì Created JSON data for {len(json_data)} samples\")\n    return json_data\n\nprint(\"‚úì JSON creation function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:06:00.095607Z","iopub.execute_input":"2025-09-02T16:06:00.096442Z","iopub.status.idle":"2025-09-02T16:06:00.104098Z","shell.execute_reply.started":"2025-09-02T16:06:00.096406Z","shell.execute_reply":"2025-09-02T16:06:00.103172Z"}},"outputs":[{"name":"stdout","text":"‚úì JSON creation function defined\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Generate embeddings for all datasets\nprint(\"=\"*60)\nprint(\"GENERATING INLEGALBERT EMBEDDINGS\")\nprint(\"=\"*60)\n\n# Prepare text data\ntrain_texts = df_train[\"text\"].tolist() if not df_train.empty else []\ntest_texts = df_test[\"text\"].tolist() if not df_test.empty else []\nval_texts = df_val[\"text\"].tolist() if not df_val.empty else []\n\nprint(f\"Texts to process:\")\nprint(f\"  Train: {len(train_texts)} texts\")\nprint(f\"  Test: {len(test_texts)} texts\") \nprint(f\"  Val: {len(val_texts)} texts\")\nprint(f\"  Total: {len(train_texts) + len(test_texts) + len(val_texts)} texts\")\n\n# Configuration for embedding generation\nMAX_LENGTH = 512  # BERT's typical max length\nBATCH_SIZE = 4 if device.type == 'cuda' else 2  # Smaller batch size to avoid memory issues\n\nprint(f\"\\nEmbedding configuration:\")\nprint(f\"  Max length: {MAX_LENGTH}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Device: {device}\")\n\n# Start timing\nimport time\nstart_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:06:03.397597Z","iopub.execute_input":"2025-09-02T16:06:03.398223Z","iopub.status.idle":"2025-09-02T16:06:03.505926Z","shell.execute_reply.started":"2025-09-02T16:06:03.398195Z","shell.execute_reply":"2025-09-02T16:06:03.505209Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING INLEGALBERT EMBEDDINGS\n============================================================\nTexts to process:\n  Train: 1123832 texts\n  Test: 149868 texts\n  Val: 293408 texts\n  Total: 1567108 texts\n\nEmbedding configuration:\n  Max length: 512\n  Batch size: 4\n  Device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# DEMO: Generate embeddings for a small sample first (for testing)\nprint(\"üß™ DEMO MODE: Processing small sample first\")\nprint(\"=\"*60)\n\nDEMO_SIZE = 10  # Process only 10 samples for demo\ndemo_train_texts = train_texts[:DEMO_SIZE]\ndemo_train_labels = train_labels[:DEMO_SIZE] \ndemo_train_label_numbers = train_label_numbers[:DEMO_SIZE]\n\nprint(f\"Demo sample size: {DEMO_SIZE}\")\nprint(\"Demo texts:\")\nfor i, (text, label) in enumerate(zip(demo_train_texts, demo_train_labels)):\n    preview = text[:100] + \"...\" if len(text) > 100 else text\n    print(f\"{i+1}. [{label}] {preview}\")\n\nprint(f\"\\nüîÑ Generating embeddings for demo sample...\")\ndemo_embeddings = get_bert_embeddings(\n    demo_train_texts, \n    tokenizer, \n    model, \n    device, \n    max_length=MAX_LENGTH, \n    batch_size=2\n)\n\nprint(f\"‚úÖ Demo embeddings generated!\")\nprint(f\"üìä Shape: {demo_embeddings.shape}\")\nprint(f\"üìä Embedding dimension: {demo_embeddings.shape[1]}\")\nprint(f\"üìä Sample embedding (first 5 values): {demo_embeddings[0][:5]}\")\n\n# Create demo JSON\ndemo_json_data = create_json_data_bert(\n    demo_train_texts, \n    demo_embeddings, \n    demo_train_labels, \n    demo_train_label_numbers, \n    \"demo set\"\n)\n\nprint(f\"‚úÖ Demo JSON data created with {len(demo_json_data)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T12:29:28.882740Z","iopub.execute_input":"2025-08-31T12:29:28.883295Z","iopub.status.idle":"2025-08-31T12:29:29.353634Z","shell.execute_reply.started":"2025-08-31T12:29:28.883269Z","shell.execute_reply":"2025-08-31T12:29:29.352860Z"}},"outputs":[{"name":"stdout","text":"üß™ DEMO MODE: Processing small sample first\n============================================================\nDemo sample size: 10\nDemo texts:\n1. [Issue]  M. JOSEPH, J. The appeal is directed against the Order of the High Court setting aside the Order pa...\n2. [Facts] The chargesheet came to be filed on the basis of a FIR dated 01.10.2011.\n3. [Facts] The appellant was Director of Mines and Geology in the State of Karnataka at the relevant time.\n4. [Facts] There was a partnership firm by the name M s Associated Mineral Company (AMC, for short).\n5. [Facts] The offences are alleged to revolve around the affairs of the said firm.\n6. [Facts] First accused is the husband of the second accused.\n7. [Facts] They became partners of the firm (AMC) in 2009.\n8. [Facts] Appellant was arrayed as the third accused.\n9. [Facts] There was reference in the chargesheet to a conspiracy between the first accused and the second accu...\n10. [Facts] It is alleged, inter alia, that they obtained an undated letter from one Shri K.M. Vishwanath, the E...\n\nüîÑ Generating embeddings for demo sample...\n","output_type":"stream"},{"name":"stderr","text":"\nGenerating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\nGenerating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 10.96it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"‚úÖ Demo embeddings generated!\nüìä Shape: (10, 768)\nüìä Embedding dimension: 768\nüìä Sample embedding (first 5 values): [ 0.4694947  -0.09240394  0.19579029  0.00546941 -0.14047483]\nCreating JSON data for demo set...\n‚úì Created JSON data for 10 samples\n‚úÖ Demo JSON data created with 10 samples\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Generate embeddings for training data\nif train_texts:\n    print(f\"\\n Processing training data ({len(train_texts)} samples)...\")\n    train_embeddings = get_bert_embeddings(\n        train_texts, \n        tokenizer, \n        model, \n        device, \n        max_length=MAX_LENGTH, \n        batch_size=BATCH_SIZE\n    )\n    print(f\"‚úì Train embeddings shape: {train_embeddings.shape}\")\n    \n    # Create JSON data for training\n    train_json_data = create_json_data_bert(\n        train_texts, \n        train_embeddings, \n        train_labels, \n        train_label_numbers, \n        \"train set\"\n    )\nelse:\n    print(\"No training data available\")\n    train_json_data = []\n\nprint(f\"\\n Training data processing time: {time.time() - start_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T16:06:12.888385Z","iopub.execute_input":"2025-09-02T16:06:12.888659Z"}},"outputs":[{"name":"stdout","text":"\n Processing training data (1123832 samples)...\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 280958/280958 [1:03:40<00:00, 73.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úì Train embeddings shape: (1123832, 768)\nCreating JSON data for train set...\n  Processed 1000/1123832 samples\n  Processed 2000/1123832 samples\n  Processed 3000/1123832 samples\n  Processed 4000/1123832 samples\n  Processed 5000/1123832 samples\n  Processed 6000/1123832 samples\n  Processed 7000/1123832 samples\n  Processed 8000/1123832 samples\n  Processed 9000/1123832 samples\n  Processed 10000/1123832 samples\n  Processed 11000/1123832 samples\n  Processed 12000/1123832 samples\n  Processed 13000/1123832 samples\n  Processed 14000/1123832 samples\n  Processed 15000/1123832 samples\n  Processed 16000/1123832 samples\n  Processed 17000/1123832 samples\n  Processed 18000/1123832 samples\n  Processed 19000/1123832 samples\n  Processed 20000/1123832 samples\n  Processed 21000/1123832 samples\n  Processed 22000/1123832 samples\n  Processed 23000/1123832 samples\n  Processed 24000/1123832 samples\n  Processed 25000/1123832 samples\n  Processed 26000/1123832 samples\n  Processed 27000/1123832 samples\n  Processed 28000/1123832 samples\n  Processed 29000/1123832 samples\n  Processed 30000/1123832 samples\n  Processed 31000/1123832 samples\n  Processed 32000/1123832 samples\n  Processed 33000/1123832 samples\n  Processed 34000/1123832 samples\n  Processed 35000/1123832 samples\n  Processed 36000/1123832 samples\n  Processed 37000/1123832 samples\n  Processed 38000/1123832 samples\n  Processed 39000/1123832 samples\n  Processed 40000/1123832 samples\n  Processed 41000/1123832 samples\n  Processed 42000/1123832 samples\n  Processed 43000/1123832 samples\n  Processed 44000/1123832 samples\n  Processed 45000/1123832 samples\n  Processed 46000/1123832 samples\n  Processed 47000/1123832 samples\n  Processed 48000/1123832 samples\n  Processed 49000/1123832 samples\n  Processed 50000/1123832 samples\n  Processed 51000/1123832 samples\n  Processed 52000/1123832 samples\n  Processed 53000/1123832 samples\n  Processed 54000/1123832 samples\n  Processed 55000/1123832 samples\n  Processed 56000/1123832 samples\n  Processed 57000/1123832 samples\n  Processed 58000/1123832 samples\n  Processed 59000/1123832 samples\n  Processed 60000/1123832 samples\n  Processed 61000/1123832 samples\n  Processed 62000/1123832 samples\n  Processed 63000/1123832 samples\n  Processed 64000/1123832 samples\n  Processed 65000/1123832 samples\n  Processed 66000/1123832 samples\n  Processed 67000/1123832 samples\n  Processed 68000/1123832 samples\n  Processed 69000/1123832 samples\n  Processed 70000/1123832 samples\n  Processed 71000/1123832 samples\n  Processed 72000/1123832 samples\n  Processed 73000/1123832 samples\n  Processed 74000/1123832 samples\n  Processed 75000/1123832 samples\n  Processed 76000/1123832 samples\n  Processed 77000/1123832 samples\n  Processed 78000/1123832 samples\n  Processed 79000/1123832 samples\n  Processed 80000/1123832 samples\n  Processed 81000/1123832 samples\n  Processed 82000/1123832 samples\n  Processed 83000/1123832 samples\n  Processed 84000/1123832 samples\n  Processed 85000/1123832 samples\n  Processed 86000/1123832 samples\n  Processed 87000/1123832 samples\n  Processed 88000/1123832 samples\n  Processed 89000/1123832 samples\n  Processed 90000/1123832 samples\n  Processed 91000/1123832 samples\n  Processed 92000/1123832 samples\n  Processed 93000/1123832 samples\n  Processed 94000/1123832 samples\n  Processed 95000/1123832 samples\n  Processed 96000/1123832 samples\n  Processed 97000/1123832 samples\n  Processed 98000/1123832 samples\n  Processed 99000/1123832 samples\n  Processed 100000/1123832 samples\n  Processed 101000/1123832 samples\n  Processed 102000/1123832 samples\n  Processed 103000/1123832 samples\n  Processed 104000/1123832 samples\n  Processed 105000/1123832 samples\n  Processed 106000/1123832 samples\n  Processed 107000/1123832 samples\n  Processed 108000/1123832 samples\n  Processed 109000/1123832 samples\n  Processed 110000/1123832 samples\n  Processed 111000/1123832 samples\n  Processed 112000/1123832 samples\n  Processed 113000/1123832 samples\n  Processed 114000/1123832 samples\n  Processed 115000/1123832 samples\n  Processed 116000/1123832 samples\n  Processed 117000/1123832 samples\n  Processed 118000/1123832 samples\n  Processed 119000/1123832 samples\n  Processed 120000/1123832 samples\n  Processed 121000/1123832 samples\n  Processed 122000/1123832 samples\n  Processed 123000/1123832 samples\n  Processed 124000/1123832 samples\n  Processed 125000/1123832 samples\n  Processed 126000/1123832 samples\n  Processed 127000/1123832 samples\n  Processed 128000/1123832 samples\n  Processed 129000/1123832 samples\n  Processed 130000/1123832 samples\n  Processed 131000/1123832 samples\n  Processed 132000/1123832 samples\n  Processed 133000/1123832 samples\n  Processed 134000/1123832 samples\n  Processed 135000/1123832 samples\n  Processed 136000/1123832 samples\n  Processed 137000/1123832 samples\n  Processed 138000/1123832 samples\n  Processed 139000/1123832 samples\n  Processed 140000/1123832 samples\n  Processed 141000/1123832 samples\n  Processed 142000/1123832 samples\n  Processed 143000/1123832 samples\n  Processed 144000/1123832 samples\n  Processed 145000/1123832 samples\n  Processed 146000/1123832 samples\n  Processed 147000/1123832 samples\n  Processed 148000/1123832 samples\n  Processed 149000/1123832 samples\n  Processed 150000/1123832 samples\n  Processed 151000/1123832 samples\n  Processed 152000/1123832 samples\n  Processed 153000/1123832 samples\n  Processed 154000/1123832 samples\n  Processed 155000/1123832 samples\n  Processed 156000/1123832 samples\n  Processed 157000/1123832 samples\n  Processed 158000/1123832 samples\n  Processed 159000/1123832 samples\n  Processed 160000/1123832 samples\n  Processed 161000/1123832 samples\n  Processed 162000/1123832 samples\n  Processed 163000/1123832 samples\n  Processed 164000/1123832 samples\n  Processed 165000/1123832 samples\n  Processed 166000/1123832 samples\n  Processed 167000/1123832 samples\n  Processed 168000/1123832 samples\n  Processed 169000/1123832 samples\n  Processed 170000/1123832 samples\n  Processed 171000/1123832 samples\n  Processed 172000/1123832 samples\n  Processed 173000/1123832 samples\n  Processed 174000/1123832 samples\n  Processed 175000/1123832 samples\n  Processed 176000/1123832 samples\n  Processed 177000/1123832 samples\n  Processed 178000/1123832 samples\n  Processed 179000/1123832 samples\n  Processed 180000/1123832 samples\n  Processed 181000/1123832 samples\n  Processed 182000/1123832 samples\n  Processed 183000/1123832 samples\n  Processed 184000/1123832 samples\n  Processed 185000/1123832 samples\n  Processed 186000/1123832 samples\n  Processed 187000/1123832 samples\n  Processed 188000/1123832 samples\n  Processed 189000/1123832 samples\n  Processed 190000/1123832 samples\n  Processed 191000/1123832 samples\n  Processed 192000/1123832 samples\n  Processed 193000/1123832 samples\n  Processed 194000/1123832 samples\n  Processed 195000/1123832 samples\n  Processed 196000/1123832 samples\n  Processed 197000/1123832 samples\n  Processed 198000/1123832 samples\n  Processed 199000/1123832 samples\n  Processed 200000/1123832 samples\n  Processed 201000/1123832 samples\n  Processed 202000/1123832 samples\n  Processed 203000/1123832 samples\n  Processed 204000/1123832 samples\n  Processed 205000/1123832 samples\n  Processed 206000/1123832 samples\n  Processed 207000/1123832 samples\n  Processed 208000/1123832 samples\n  Processed 209000/1123832 samples\n  Processed 210000/1123832 samples\n  Processed 211000/1123832 samples\n  Processed 212000/1123832 samples\n  Processed 213000/1123832 samples\n  Processed 214000/1123832 samples\n  Processed 215000/1123832 samples\n  Processed 216000/1123832 samples\n  Processed 217000/1123832 samples\n  Processed 218000/1123832 samples\n  Processed 219000/1123832 samples\n  Processed 220000/1123832 samples\n  Processed 221000/1123832 samples\n  Processed 222000/1123832 samples\n  Processed 223000/1123832 samples\n  Processed 224000/1123832 samples\n  Processed 225000/1123832 samples\n  Processed 226000/1123832 samples\n  Processed 227000/1123832 samples\n  Processed 228000/1123832 samples\n  Processed 229000/1123832 samples\n  Processed 230000/1123832 samples\n  Processed 231000/1123832 samples\n  Processed 232000/1123832 samples\n  Processed 233000/1123832 samples\n  Processed 234000/1123832 samples\n  Processed 235000/1123832 samples\n  Processed 236000/1123832 samples\n  Processed 237000/1123832 samples\n  Processed 238000/1123832 samples\n  Processed 239000/1123832 samples\n  Processed 240000/1123832 samples\n  Processed 241000/1123832 samples\n  Processed 242000/1123832 samples\n  Processed 243000/1123832 samples\n  Processed 244000/1123832 samples\n  Processed 245000/1123832 samples\n  Processed 246000/1123832 samples\n  Processed 247000/1123832 samples\n  Processed 248000/1123832 samples\n  Processed 249000/1123832 samples\n  Processed 250000/1123832 samples\n  Processed 251000/1123832 samples\n  Processed 252000/1123832 samples\n  Processed 253000/1123832 samples\n  Processed 254000/1123832 samples\n  Processed 255000/1123832 samples\n  Processed 256000/1123832 samples\n  Processed 257000/1123832 samples\n  Processed 258000/1123832 samples\n  Processed 259000/1123832 samples\n  Processed 260000/1123832 samples\n  Processed 261000/1123832 samples\n  Processed 262000/1123832 samples\n  Processed 263000/1123832 samples\n  Processed 264000/1123832 samples\n  Processed 265000/1123832 samples\n  Processed 266000/1123832 samples\n  Processed 267000/1123832 samples\n  Processed 268000/1123832 samples\n  Processed 269000/1123832 samples\n  Processed 270000/1123832 samples\n  Processed 271000/1123832 samples\n  Processed 272000/1123832 samples\n  Processed 273000/1123832 samples\n  Processed 274000/1123832 samples\n  Processed 275000/1123832 samples\n  Processed 276000/1123832 samples\n  Processed 277000/1123832 samples\n  Processed 278000/1123832 samples\n  Processed 279000/1123832 samples\n  Processed 280000/1123832 samples\n  Processed 281000/1123832 samples\n  Processed 282000/1123832 samples\n  Processed 283000/1123832 samples\n  Processed 284000/1123832 samples\n  Processed 285000/1123832 samples\n  Processed 286000/1123832 samples\n  Processed 287000/1123832 samples\n  Processed 288000/1123832 samples\n  Processed 289000/1123832 samples\n  Processed 290000/1123832 samples\n  Processed 291000/1123832 samples\n  Processed 292000/1123832 samples\n  Processed 293000/1123832 samples\n  Processed 294000/1123832 samples\n  Processed 295000/1123832 samples\n  Processed 296000/1123832 samples\n  Processed 297000/1123832 samples\n  Processed 298000/1123832 samples\n  Processed 299000/1123832 samples\n  Processed 300000/1123832 samples\n  Processed 301000/1123832 samples\n  Processed 302000/1123832 samples\n  Processed 303000/1123832 samples\n  Processed 304000/1123832 samples\n  Processed 305000/1123832 samples\n  Processed 306000/1123832 samples\n  Processed 307000/1123832 samples\n  Processed 308000/1123832 samples\n  Processed 309000/1123832 samples\n  Processed 310000/1123832 samples\n  Processed 311000/1123832 samples\n  Processed 312000/1123832 samples\n  Processed 313000/1123832 samples\n  Processed 314000/1123832 samples\n  Processed 315000/1123832 samples\n  Processed 316000/1123832 samples\n  Processed 317000/1123832 samples\n  Processed 318000/1123832 samples\n  Processed 319000/1123832 samples\n  Processed 320000/1123832 samples\n  Processed 321000/1123832 samples\n  Processed 322000/1123832 samples\n  Processed 323000/1123832 samples\n  Processed 324000/1123832 samples\n  Processed 325000/1123832 samples\n  Processed 326000/1123832 samples\n  Processed 327000/1123832 samples\n  Processed 328000/1123832 samples\n  Processed 329000/1123832 samples\n  Processed 330000/1123832 samples\n  Processed 331000/1123832 samples\n  Processed 332000/1123832 samples\n  Processed 333000/1123832 samples\n  Processed 334000/1123832 samples\n  Processed 335000/1123832 samples\n  Processed 336000/1123832 samples\n  Processed 337000/1123832 samples\n  Processed 338000/1123832 samples\n  Processed 339000/1123832 samples\n  Processed 340000/1123832 samples\n  Processed 341000/1123832 samples\n  Processed 342000/1123832 samples\n  Processed 343000/1123832 samples\n  Processed 344000/1123832 samples\n  Processed 345000/1123832 samples\n  Processed 346000/1123832 samples\n  Processed 347000/1123832 samples\n  Processed 348000/1123832 samples\n  Processed 349000/1123832 samples\n  Processed 350000/1123832 samples\n  Processed 351000/1123832 samples\n  Processed 352000/1123832 samples\n  Processed 353000/1123832 samples\n  Processed 354000/1123832 samples\n  Processed 355000/1123832 samples\n  Processed 356000/1123832 samples\n  Processed 357000/1123832 samples\n  Processed 358000/1123832 samples\n  Processed 359000/1123832 samples\n  Processed 360000/1123832 samples\n  Processed 361000/1123832 samples\n  Processed 362000/1123832 samples\n  Processed 363000/1123832 samples\n  Processed 364000/1123832 samples\n  Processed 365000/1123832 samples\n  Processed 366000/1123832 samples\n  Processed 367000/1123832 samples\n  Processed 368000/1123832 samples\n  Processed 369000/1123832 samples\n  Processed 370000/1123832 samples\n  Processed 371000/1123832 samples\n  Processed 372000/1123832 samples\n  Processed 373000/1123832 samples\n  Processed 374000/1123832 samples\n  Processed 375000/1123832 samples\n  Processed 376000/1123832 samples\n  Processed 377000/1123832 samples\n  Processed 378000/1123832 samples\n  Processed 379000/1123832 samples\n  Processed 380000/1123832 samples\n  Processed 381000/1123832 samples\n  Processed 382000/1123832 samples\n  Processed 383000/1123832 samples\n  Processed 384000/1123832 samples\n  Processed 385000/1123832 samples\n  Processed 386000/1123832 samples\n  Processed 387000/1123832 samples\n  Processed 388000/1123832 samples\n  Processed 389000/1123832 samples\n  Processed 390000/1123832 samples\n  Processed 391000/1123832 samples\n  Processed 392000/1123832 samples\n  Processed 393000/1123832 samples\n  Processed 394000/1123832 samples\n  Processed 395000/1123832 samples\n  Processed 396000/1123832 samples\n  Processed 397000/1123832 samples\n  Processed 398000/1123832 samples\n  Processed 399000/1123832 samples\n  Processed 400000/1123832 samples\n  Processed 401000/1123832 samples\n  Processed 402000/1123832 samples\n  Processed 403000/1123832 samples\n  Processed 404000/1123832 samples\n  Processed 405000/1123832 samples\n  Processed 406000/1123832 samples\n  Processed 407000/1123832 samples\n  Processed 408000/1123832 samples\n  Processed 409000/1123832 samples\n  Processed 410000/1123832 samples\n  Processed 411000/1123832 samples\n  Processed 412000/1123832 samples\n  Processed 413000/1123832 samples\n  Processed 414000/1123832 samples\n  Processed 415000/1123832 samples\n  Processed 416000/1123832 samples\n  Processed 417000/1123832 samples\n  Processed 418000/1123832 samples\n  Processed 419000/1123832 samples\n  Processed 420000/1123832 samples\n  Processed 421000/1123832 samples\n  Processed 422000/1123832 samples\n  Processed 423000/1123832 samples\n  Processed 424000/1123832 samples\n  Processed 425000/1123832 samples\n  Processed 426000/1123832 samples\n  Processed 427000/1123832 samples\n  Processed 428000/1123832 samples\n  Processed 429000/1123832 samples\n  Processed 430000/1123832 samples\n  Processed 431000/1123832 samples\n  Processed 432000/1123832 samples\n  Processed 433000/1123832 samples\n  Processed 434000/1123832 samples\n  Processed 435000/1123832 samples\n  Processed 436000/1123832 samples\n  Processed 437000/1123832 samples\n  Processed 438000/1123832 samples\n  Processed 439000/1123832 samples\n  Processed 440000/1123832 samples\n  Processed 441000/1123832 samples\n  Processed 442000/1123832 samples\n  Processed 443000/1123832 samples\n  Processed 444000/1123832 samples\n  Processed 445000/1123832 samples\n  Processed 446000/1123832 samples\n  Processed 447000/1123832 samples\n  Processed 448000/1123832 samples\n  Processed 449000/1123832 samples\n  Processed 450000/1123832 samples\n  Processed 451000/1123832 samples\n  Processed 452000/1123832 samples\n  Processed 453000/1123832 samples\n  Processed 454000/1123832 samples\n  Processed 455000/1123832 samples\n  Processed 456000/1123832 samples\n  Processed 457000/1123832 samples\n  Processed 458000/1123832 samples\n  Processed 459000/1123832 samples\n  Processed 460000/1123832 samples\n  Processed 461000/1123832 samples\n  Processed 462000/1123832 samples\n  Processed 463000/1123832 samples\n  Processed 464000/1123832 samples\n  Processed 465000/1123832 samples\n  Processed 466000/1123832 samples\n  Processed 467000/1123832 samples\n  Processed 468000/1123832 samples\n  Processed 469000/1123832 samples\n  Processed 470000/1123832 samples\n  Processed 471000/1123832 samples\n  Processed 472000/1123832 samples\n  Processed 473000/1123832 samples\n  Processed 474000/1123832 samples\n  Processed 475000/1123832 samples\n  Processed 476000/1123832 samples\n  Processed 477000/1123832 samples\n  Processed 478000/1123832 samples\n  Processed 479000/1123832 samples\n  Processed 480000/1123832 samples\n  Processed 481000/1123832 samples\n  Processed 482000/1123832 samples\n  Processed 483000/1123832 samples\n  Processed 484000/1123832 samples\n  Processed 485000/1123832 samples\n  Processed 486000/1123832 samples\n  Processed 487000/1123832 samples\n  Processed 488000/1123832 samples\n  Processed 489000/1123832 samples\n  Processed 490000/1123832 samples\n  Processed 491000/1123832 samples\n  Processed 492000/1123832 samples\n  Processed 493000/1123832 samples\n  Processed 494000/1123832 samples\n  Processed 495000/1123832 samples\n  Processed 496000/1123832 samples\n  Processed 497000/1123832 samples\n  Processed 498000/1123832 samples\n  Processed 499000/1123832 samples\n  Processed 500000/1123832 samples\n  Processed 501000/1123832 samples\n  Processed 502000/1123832 samples\n  Processed 503000/1123832 samples\n  Processed 504000/1123832 samples\n  Processed 505000/1123832 samples\n  Processed 506000/1123832 samples\n  Processed 507000/1123832 samples\n  Processed 508000/1123832 samples\n  Processed 509000/1123832 samples\n  Processed 510000/1123832 samples\n  Processed 511000/1123832 samples\n  Processed 512000/1123832 samples\n  Processed 513000/1123832 samples\n  Processed 514000/1123832 samples\n  Processed 515000/1123832 samples\n  Processed 516000/1123832 samples\n  Processed 517000/1123832 samples\n  Processed 518000/1123832 samples\n  Processed 519000/1123832 samples\n  Processed 520000/1123832 samples\n  Processed 521000/1123832 samples\n  Processed 522000/1123832 samples\n  Processed 523000/1123832 samples\n  Processed 524000/1123832 samples\n  Processed 525000/1123832 samples\n  Processed 526000/1123832 samples\n  Processed 527000/1123832 samples\n  Processed 528000/1123832 samples\n  Processed 529000/1123832 samples\n  Processed 530000/1123832 samples\n  Processed 531000/1123832 samples\n  Processed 532000/1123832 samples\n  Processed 533000/1123832 samples\n  Processed 534000/1123832 samples\n  Processed 535000/1123832 samples\n  Processed 536000/1123832 samples\n  Processed 537000/1123832 samples\n  Processed 538000/1123832 samples\n  Processed 539000/1123832 samples\n  Processed 540000/1123832 samples\n  Processed 541000/1123832 samples\n  Processed 542000/1123832 samples\n  Processed 543000/1123832 samples\n  Processed 544000/1123832 samples\n  Processed 545000/1123832 samples\n  Processed 546000/1123832 samples\n  Processed 547000/1123832 samples\n  Processed 548000/1123832 samples\n  Processed 549000/1123832 samples\n  Processed 550000/1123832 samples\n  Processed 551000/1123832 samples\n  Processed 552000/1123832 samples\n  Processed 553000/1123832 samples\n  Processed 554000/1123832 samples\n  Processed 555000/1123832 samples\n  Processed 556000/1123832 samples\n  Processed 557000/1123832 samples\n  Processed 558000/1123832 samples\n  Processed 559000/1123832 samples\n  Processed 560000/1123832 samples\n  Processed 561000/1123832 samples\n  Processed 562000/1123832 samples\n  Processed 563000/1123832 samples\n  Processed 564000/1123832 samples\n  Processed 565000/1123832 samples\n  Processed 566000/1123832 samples\n  Processed 567000/1123832 samples\n  Processed 568000/1123832 samples\n  Processed 569000/1123832 samples\n  Processed 570000/1123832 samples\n  Processed 571000/1123832 samples\n  Processed 572000/1123832 samples\n  Processed 573000/1123832 samples\n  Processed 574000/1123832 samples\n  Processed 575000/1123832 samples\n  Processed 576000/1123832 samples\n  Processed 577000/1123832 samples\n  Processed 578000/1123832 samples\n  Processed 579000/1123832 samples\n  Processed 580000/1123832 samples\n  Processed 581000/1123832 samples\n  Processed 582000/1123832 samples\n  Processed 583000/1123832 samples\n  Processed 584000/1123832 samples\n  Processed 585000/1123832 samples\n  Processed 586000/1123832 samples\n  Processed 587000/1123832 samples\n  Processed 588000/1123832 samples\n  Processed 589000/1123832 samples\n  Processed 590000/1123832 samples\n  Processed 591000/1123832 samples\n  Processed 592000/1123832 samples\n  Processed 593000/1123832 samples\n  Processed 594000/1123832 samples\n  Processed 595000/1123832 samples\n  Processed 596000/1123832 samples\n  Processed 597000/1123832 samples\n  Processed 598000/1123832 samples\n  Processed 599000/1123832 samples\n  Processed 600000/1123832 samples\n  Processed 601000/1123832 samples\n  Processed 602000/1123832 samples\n  Processed 603000/1123832 samples\n  Processed 604000/1123832 samples\n  Processed 605000/1123832 samples\n  Processed 606000/1123832 samples\n  Processed 607000/1123832 samples\n  Processed 608000/1123832 samples\n  Processed 609000/1123832 samples\n  Processed 610000/1123832 samples\n  Processed 611000/1123832 samples\n  Processed 612000/1123832 samples\n  Processed 613000/1123832 samples\n  Processed 614000/1123832 samples\n  Processed 615000/1123832 samples\n  Processed 616000/1123832 samples\n  Processed 617000/1123832 samples\n  Processed 618000/1123832 samples\n  Processed 619000/1123832 samples\n  Processed 620000/1123832 samples\n  Processed 621000/1123832 samples\n  Processed 622000/1123832 samples\n  Processed 623000/1123832 samples\n  Processed 624000/1123832 samples\n  Processed 625000/1123832 samples\n  Processed 626000/1123832 samples\n  Processed 627000/1123832 samples\n  Processed 628000/1123832 samples\n  Processed 629000/1123832 samples\n  Processed 630000/1123832 samples\n  Processed 631000/1123832 samples\n  Processed 632000/1123832 samples\n  Processed 633000/1123832 samples\n  Processed 634000/1123832 samples\n  Processed 635000/1123832 samples\n  Processed 636000/1123832 samples\n  Processed 637000/1123832 samples\n  Processed 638000/1123832 samples\n  Processed 639000/1123832 samples\n  Processed 640000/1123832 samples\n  Processed 641000/1123832 samples\n  Processed 642000/1123832 samples\n  Processed 643000/1123832 samples\n  Processed 644000/1123832 samples\n  Processed 645000/1123832 samples\n  Processed 646000/1123832 samples\n  Processed 647000/1123832 samples\n  Processed 648000/1123832 samples\n  Processed 649000/1123832 samples\n  Processed 650000/1123832 samples\n  Processed 651000/1123832 samples\n  Processed 652000/1123832 samples\n  Processed 653000/1123832 samples\n  Processed 654000/1123832 samples\n  Processed 655000/1123832 samples\n  Processed 656000/1123832 samples\n  Processed 657000/1123832 samples\n  Processed 658000/1123832 samples\n  Processed 659000/1123832 samples\n  Processed 660000/1123832 samples\n  Processed 661000/1123832 samples\n  Processed 662000/1123832 samples\n  Processed 663000/1123832 samples\n  Processed 664000/1123832 samples\n  Processed 665000/1123832 samples\n  Processed 666000/1123832 samples\n  Processed 667000/1123832 samples\n  Processed 668000/1123832 samples\n  Processed 669000/1123832 samples\n  Processed 670000/1123832 samples\n  Processed 671000/1123832 samples\n  Processed 672000/1123832 samples\n  Processed 673000/1123832 samples\n  Processed 674000/1123832 samples\n  Processed 675000/1123832 samples\n  Processed 676000/1123832 samples\n  Processed 677000/1123832 samples\n  Processed 678000/1123832 samples\n  Processed 679000/1123832 samples\n  Processed 680000/1123832 samples\n  Processed 681000/1123832 samples\n  Processed 682000/1123832 samples\n  Processed 683000/1123832 samples\n  Processed 684000/1123832 samples\n  Processed 685000/1123832 samples\n  Processed 686000/1123832 samples\n  Processed 687000/1123832 samples\n  Processed 688000/1123832 samples\n  Processed 689000/1123832 samples\n  Processed 690000/1123832 samples\n  Processed 691000/1123832 samples\n  Processed 692000/1123832 samples\n  Processed 693000/1123832 samples\n  Processed 694000/1123832 samples\n  Processed 695000/1123832 samples\n  Processed 696000/1123832 samples\n  Processed 697000/1123832 samples\n  Processed 698000/1123832 samples\n  Processed 699000/1123832 samples\n  Processed 700000/1123832 samples\n  Processed 701000/1123832 samples\n  Processed 702000/1123832 samples\n  Processed 703000/1123832 samples\n  Processed 704000/1123832 samples\n  Processed 705000/1123832 samples\n  Processed 706000/1123832 samples\n  Processed 707000/1123832 samples\n  Processed 708000/1123832 samples\n  Processed 709000/1123832 samples\n  Processed 710000/1123832 samples\n  Processed 711000/1123832 samples\n  Processed 712000/1123832 samples\n  Processed 713000/1123832 samples\n  Processed 714000/1123832 samples\n  Processed 715000/1123832 samples\n  Processed 716000/1123832 samples\n  Processed 717000/1123832 samples\n  Processed 718000/1123832 samples\n  Processed 719000/1123832 samples\n  Processed 720000/1123832 samples\n  Processed 721000/1123832 samples\n  Processed 722000/1123832 samples\n  Processed 723000/1123832 samples\n  Processed 724000/1123832 samples\n  Processed 725000/1123832 samples\n  Processed 726000/1123832 samples\n  Processed 727000/1123832 samples\n  Processed 728000/1123832 samples\n  Processed 729000/1123832 samples\n  Processed 730000/1123832 samples\n  Processed 731000/1123832 samples\n  Processed 732000/1123832 samples\n  Processed 733000/1123832 samples\n  Processed 734000/1123832 samples\n  Processed 735000/1123832 samples\n  Processed 736000/1123832 samples\n  Processed 737000/1123832 samples\n  Processed 738000/1123832 samples\n  Processed 739000/1123832 samples\n  Processed 740000/1123832 samples\n  Processed 741000/1123832 samples\n  Processed 742000/1123832 samples\n  Processed 743000/1123832 samples\n  Processed 744000/1123832 samples\n  Processed 745000/1123832 samples\n  Processed 746000/1123832 samples\n  Processed 747000/1123832 samples\n  Processed 748000/1123832 samples\n  Processed 749000/1123832 samples\n  Processed 750000/1123832 samples\n  Processed 751000/1123832 samples\n  Processed 752000/1123832 samples\n  Processed 753000/1123832 samples\n  Processed 754000/1123832 samples\n  Processed 755000/1123832 samples\n  Processed 756000/1123832 samples\n  Processed 757000/1123832 samples\n  Processed 758000/1123832 samples\n  Processed 759000/1123832 samples\n  Processed 760000/1123832 samples\n  Processed 761000/1123832 samples\n  Processed 762000/1123832 samples\n  Processed 763000/1123832 samples\n  Processed 764000/1123832 samples\n  Processed 765000/1123832 samples\n  Processed 766000/1123832 samples\n  Processed 767000/1123832 samples\n  Processed 768000/1123832 samples\n  Processed 769000/1123832 samples\n  Processed 770000/1123832 samples\n  Processed 771000/1123832 samples\n  Processed 772000/1123832 samples\n  Processed 773000/1123832 samples\n  Processed 774000/1123832 samples\n  Processed 775000/1123832 samples\n  Processed 776000/1123832 samples\n  Processed 777000/1123832 samples\n  Processed 778000/1123832 samples\n  Processed 779000/1123832 samples\n  Processed 780000/1123832 samples\n  Processed 781000/1123832 samples\n  Processed 782000/1123832 samples\n  Processed 783000/1123832 samples\n  Processed 784000/1123832 samples\n  Processed 785000/1123832 samples\n  Processed 786000/1123832 samples\n  Processed 787000/1123832 samples\n  Processed 788000/1123832 samples\n  Processed 789000/1123832 samples\n  Processed 790000/1123832 samples\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Generate embeddings for test data\nif test_texts:\n    print(f\"\\n Processing test data ({len(test_texts)} samples)...\")\n    test_start_time = time.time()\n    \n    test_embeddings = get_bert_embeddings(\n        test_texts, \n        tokenizer, \n        model, \n        device, \n        max_length=MAX_LENGTH, \n        batch_size=BATCH_SIZE\n    )\n    print(f\"‚úì Test embeddings shape: {test_embeddings.shape}\")\n    \n    # Create JSON data for test (no labels)\n    test_json_data = create_json_data_bert(\n        test_texts, \n        test_embeddings, \n        dataset_name=\"test set\"\n    )\n    \n    print(f\" Test data processing time: {time.time() - test_start_time:.2f} seconds\")\nelse:\n    print(\" No test data available\")\n    test_json_data = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate embeddings for validation data\nif val_texts:\n    print(f\"\\nProcessing validation data ({len(val_texts)} samples)...\")\n    val_start_time = time.time()\n    \n    val_embeddings = get_bert_embeddings(\n        val_texts, \n        tokenizer, \n        model, \n        device, \n        max_length=MAX_LENGTH, \n        batch_size=BATCH_SIZE\n    )\n    print(f\"‚úì Val embeddings shape: {val_embeddings.shape}\")\n    \n    # Create JSON data for validation (no labels)\n    val_json_data = create_json_data_bert(\n        val_texts, \n        val_embeddings, \n        dataset_name=\"validation set\"\n    )\n    \n    print(f\"Validation data processing time: {time.time() - val_start_time:.2f} seconds\")\nelse:\n    print(\" No validation data available\")\n    val_json_data = []\n\ntotal_time = time.time() - start_time\nprint(f\"\\n Total embedding generation time: {total_time:.2f} seconds\")\nprint(f\" Average time per sample: {total_time / (len(train_texts) + len(test_texts) + len(val_texts)):.3f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save embeddings to JSON files\nprint(\"=\"*60)\nprint(\"SAVING EMBEDDINGS TO JSON FILES\")\nprint(\"=\"*60)\n\n# Ensure output directory exists\nos.makedirs(embeddings_output_path, exist_ok=True)\nprint(f\"üìÅ Output directory: {embeddings_output_path}\")\n\n# Save training embeddings\nif train_json_data:\n    train_file_path = os.path.join(embeddings_output_path, 'train_embeddings_inlegalbert.json')\n    with open(train_file_path, 'w') as f:\n        json.dump(train_json_data, f, indent=2)\n    print(f\"Saved train_embeddings_inlegalbert.json with {len(train_json_data)} samples\")\n    print(f\"   File size: {os.path.getsize(train_file_path) / (1024*1024):.1f} MB\")\n\n# # Save test embeddings  \n# if test_json_data:\n#     test_file_path = os.path.join(embeddings_output_path, 'test_embeddings_inlegalbert.json')\n#     with open(test_file_path, 'w') as f:\n#         json.dump(test_json_data, f, indent=2)\n#     print(f\"‚úÖ Saved test_embeddings_inlegalbert.json with {len(test_json_data)} samples\")\n#     print(f\"   File size: {os.path.getsize(test_file_path) / (1024*1024):.1f} MB\")\n\n# # Save validation embeddings\n# if val_json_data:\n#     val_file_path = os.path.join(embeddings_output_path, 'val_embeddings_inlegalbert.json')\n#     with open(val_file_path, 'w') as f:\n#         json.dump(val_json_data, f, indent=2)\n#     print(f\"‚úÖ Saved val_embeddings_inlegalbert.json with {len(val_json_data)} samples\")\n#     print(f\"   File size: {os.path.getsize(val_file_path) / (1024*1024):.1f} MB\")\n\n# Save label mapping for reference (if available)\nif 'label_mapping' in locals():\n    label_file_path = os.path.join(embeddings_output_path, 'label_mapping_inlegalbert.json')\n    with open(label_file_path, 'w') as f:\n        json.dump(label_mapping, f, indent=2)\n    print(f\"‚úÖ Saved label_mapping_inlegalbert.json\")\n\nprint(f\"\\nüìÇ All files saved in: {os.path.abspath(embeddings_output_path)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}