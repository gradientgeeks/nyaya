{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ce1cc6",
   "metadata": {},
   "source": [
    "# InLegalBERT Embedding Generation for Legal Text Classification\n",
    "\n",
    "This notebook generates embeddings for legal text classification using **InLegalBERT** - a BERT model specifically pre-trained on Indian legal documents.\n",
    "\n",
    "## Dataset Structure\n",
    "- **Train**: Files with text and labels (Facts, Reasoning, Arguments of Respondent, Arguments of Petitioner, Decision, Issue)\n",
    "- **Test**: Files with only text (no labels)\n",
    "- **Val**: Files with only text (no labels)\n",
    "\n",
    "## Output\n",
    "- JSON files with text, embeddings, class names, and class numbers\n",
    "- Saved in the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdd3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uttam/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0f6912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/train\n",
      "Test path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/test\n",
      "Val path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/val/val\n",
      "Output path: /home/uttam/B.Tech Major Project/nyaya/server/embeddings\n",
      "✓ Train directory exists with 4994 files\n",
      "✓ Test directory exists with 712 files\n",
      "✓ Val directory exists with 1424 files\n"
     ]
    }
   ],
   "source": [
    "# Set device for GPU acceleration if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset paths (adjusted for local structure)\n",
    "base_path = \"/home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF\"\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "test_path = os.path.join(base_path, \"test\") \n",
    "val_path = os.path.join(base_path, \"val\", \"val\")\n",
    "\n",
    "# Output path for embeddings\n",
    "embeddings_output_path = \"/home/uttam/B.Tech Major Project/nyaya/server/embeddings\"\n",
    "\n",
    "print(f\"Train path: {train_path}\")\n",
    "print(f\"Test path: {test_path}\")\n",
    "print(f\"Val path: {val_path}\")\n",
    "print(f\"Output path: {embeddings_output_path}\")\n",
    "\n",
    "# Verify paths exist\n",
    "for path_name, path in [(\"Train\", train_path), (\"Test\", test_path), (\"Val\", val_path)]:\n",
    "    if os.path.exists(path):\n",
    "        file_count = len([f for f in os.listdir(path) if f.endswith('.txt')])\n",
    "        print(f\"✓ {path_name} directory exists with {file_count} files\")\n",
    "    else:\n",
    "        print(f\"✗ {path_name} directory not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b166ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_files(directory_path):\n",
    "    \"\"\"Load training files with labels\"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(f\"Loading training files from: {directory_path}\")\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(files)} files\")\n",
    "    \n",
    "    for file_name in tqdm(files, desc=\"Loading train files\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "            df.dropna(subset=[\"label\"], inplace=True)\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        result_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(result_df)} training samples\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"No valid training data found\")\n",
    "        return pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "\n",
    "def load_test_val_files(directory_path):\n",
    "    \"\"\"Load test/val files with only text (no labels)\"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(f\"Loading test/val files from: {directory_path}\")\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(files)} files\")\n",
    "    \n",
    "    for file_name in tqdm(files, desc=\"Loading test/val files\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\"])\n",
    "            df.dropna(subset=[\"text\"], inplace=True)\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        result_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(result_df)} samples\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"No valid test/val data found\")\n",
    "        return pd.DataFrame(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa175939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading training files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/train\n",
      "Found 4994 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train files: 100%|██████████| 4994/4994 [00:19<00:00, 260.67it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 520247 training samples\n",
      "Loading test/val files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/test\n",
      "Found 712 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test/val files: 100%|██████████| 712/712 [00:03<00:00, 227.54it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 149868 samples\n",
      "Loading test/val files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/val/val\n",
      "Found 1424 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test/val files: 100%|██████████| 1424/1424 [00:05<00:00, 271.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 293408 samples\n",
      "\n",
      "Dataset Summary:\n",
      "Train: 520247 rows\n",
      "Test: 149868 rows\n",
      "Val: 293408 rows\n",
      "\n",
      "Train labels distribution:\n",
      "label\n",
      "Reasoning                  202593\n",
      "Facts                      170068\n",
      "Arguments of Petitioner     65032\n",
      "Arguments of Respondent     50137\n",
      "Decision                    19599\n",
      "Issue                       12818\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample train data:\n",
      "                                                text  label\n",
      "0   K. Mathur, J. This appeal is directed against...  Issue\n",
      "1  Brief facts giving rise to this appeal areThe ...  Facts\n",
      "2  The case of the complainant respondent was tha...  Facts\n",
      "3  The respondent complainant held a valid Fire P...  Facts\n",
      "4  This policy also endorsed to cover risk of flood.  Facts\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "df_train = load_train_files(train_path)  # has text + label\n",
    "df_test = load_test_val_files(test_path)  # only text  \n",
    "df_val = load_test_val_files(val_path)    # only text\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Train: {len(df_train)} rows\")\n",
    "print(f\"Test: {len(df_test)} rows\")\n",
    "print(f\"Val: {len(df_val)} rows\")\n",
    "\n",
    "if not df_train.empty:\n",
    "    print(f\"\\nTrain labels distribution:\")\n",
    "    print(df_train[\"label\"].value_counts())\n",
    "    \n",
    "    print(f\"\\nSample train data:\")\n",
    "    print(df_train.head())\n",
    "else:\n",
    "    print(\"No training data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a98b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating label mappings...\n",
      "Unique labels in dataset: ['Issue' 'Facts' 'Arguments of Petitioner' 'Arguments of Respondent'\n",
      " 'Reasoning' 'Decision']\n",
      "Using manual mapping:\n",
      "0: Facts\n",
      "1: Reasoning\n",
      "2: Arguments of Respondent\n",
      "3: Arguments of Petitioner\n",
      "4: Decision\n",
      "5: Issue\n",
      "6: None\n",
      "\n",
      "Label distribution by number:\n",
      "0 (Facts): 170068\n",
      "1 (Reasoning): 202593\n",
      "2 (Arguments of Respondent): 50137\n",
      "3 (Arguments of Petitioner): 65032\n",
      "4 (Decision): 19599\n",
      "5 (Issue): 12818\n"
     ]
    }
   ],
   "source": [
    "# Label encoding for training data\n",
    "if not df_train.empty:\n",
    "    # Manual mapping (similar to your original code)\n",
    "    label_to_num = {\n",
    "        'Facts': 0,\n",
    "        'Reasoning': 1, \n",
    "        'Arguments of Respondent': 2,\n",
    "        'Arguments of Petitioner': 3,\n",
    "        'Decision': 4,\n",
    "        'Issue': 5,\n",
    "        'None': 6\n",
    "    }\n",
    "    \n",
    "    print(\"Creating label mappings...\")\n",
    "    \n",
    "    # Check if all labels in data are in our mapping\n",
    "    unique_labels = df_train['label'].unique()\n",
    "    print(f\"Unique labels in dataset: {unique_labels}\")\n",
    "    \n",
    "    missing_labels = [label for label in unique_labels if label not in label_to_num]\n",
    "    if missing_labels:\n",
    "        print(f\"Warning: Missing labels in mapping: {missing_labels}\")\n",
    "        \n",
    "        # Use LabelEncoder as fallback for missing labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_train['label_encoded'] = label_encoder.fit_transform(df_train['label'])\n",
    "        \n",
    "        # Create updated mapping\n",
    "        label_mapping = {}\n",
    "        for i, label in enumerate(label_encoder.classes_):\n",
    "            label_mapping[i] = label\n",
    "            \n",
    "        train_labels = df_train['label'].tolist()\n",
    "        train_label_numbers = df_train['label_encoded'].tolist()\n",
    "        \n",
    "        print(\"Using LabelEncoder mapping:\")\n",
    "        for i, label in label_mapping.items():\n",
    "            print(f\"{i}: {label}\")\n",
    "    else:\n",
    "        # Use manual mapping\n",
    "        df_train['label_numeric'] = df_train['label'].map(label_to_num)\n",
    "        \n",
    "        label_mapping = {v: k for k, v in label_to_num.items()}  # Reverse mapping\n",
    "        train_labels = df_train['label'].tolist()\n",
    "        train_label_numbers = df_train['label_numeric'].tolist()\n",
    "        \n",
    "        print(\"Using manual mapping:\")\n",
    "        for num, label in label_mapping.items():\n",
    "            print(f\"{num}: {label}\")\n",
    "            \n",
    "    print(f\"\\nLabel distribution by number:\")\n",
    "    label_counts = {}\n",
    "    for label_num in train_label_numbers:\n",
    "        label_counts[label_num] = label_counts.get(label_num, 0) + 1\n",
    "    for num, count in sorted(label_counts.items()):\n",
    "        print(f\"{num} ({label_mapping[num]}): {count}\")\n",
    "else:\n",
    "    print(\"No training data available for label encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee33144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading InLegalBERT model and tokenizer...\n",
      "This may take a few minutes on first run...\n",
      "✓ InLegalBERT loaded successfully!\n",
      "✓ Model moved to: cpu\n",
      "✓ Tokenizer vocabulary size: 30522\n",
      "✓ Model max position embeddings: 512\n",
      "✓ Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "# Load InLegalBERT model and tokenizer\n",
    "print(\"Loading InLegalBERT model and tokenizer...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    \n",
    "    # Move model to device (GPU if available)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"✓ InLegalBERT loaded successfully!\")\n",
    "    print(f\"✓ Model moved to: {device}\")\n",
    "    print(f\"✓ Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"✓ Model max position embeddings: {model.config.max_position_embeddings}\")\n",
    "    print(f\"✓ Hidden size: {model.config.hidden_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading InLegalBERT: {e}\")\n",
    "    print(\"Please ensure you have internet connection and transformers library installed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46994adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding function defined\n",
      "This function will use the [CLS] token representation as sentence embeddings\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(texts, tokenizer, model, device, max_length=512, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate embeddings using InLegalBERT\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        tokenizer: InLegalBERT tokenizer\n",
    "        model: InLegalBERT model\n",
    "        device: torch device (cuda/cpu)\n",
    "        max_length: Maximum sequence length for BERT\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (texts x hidden_size)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {key: val.to(device) for key, val in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use [CLS] token embedding (first token) as sentence representation\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Move back to CPU and convert to numpy\n",
    "            batch_embeddings = cls_embeddings.cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = np.vstack(embeddings)\n",
    "    return all_embeddings\n",
    "\n",
    "print(\"✓ Embedding function defined\")\n",
    "print(\"This function will use the [CLS] token representation as sentence embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6c487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JSON creation function defined\n"
     ]
    }
   ],
   "source": [
    "def create_json_data_bert(texts, embeddings, labels=None, label_numbers=None, dataset_name=\"\"):\n",
    "    \"\"\"Create JSON data with text, BERT embeddings, classname, classnumber\"\"\"\n",
    "    print(f\"Creating JSON data for {dataset_name}...\")\n",
    "    json_data = []\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        data_point = {\n",
    "            \"text\": texts[i],\n",
    "            \"vector\": embeddings[i].tolist(),  # Convert numpy array to list for JSON\n",
    "        }\n",
    "        \n",
    "        if labels is not None and label_numbers is not None:\n",
    "            data_point[\"classname\"] = labels[i]\n",
    "            data_point[\"classnumber\"] = int(label_numbers[i])\n",
    "        else:\n",
    "            data_point[\"classname\"] = None\n",
    "            data_point[\"classnumber\"] = None\n",
    "            \n",
    "        json_data.append(data_point)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} samples\")\n",
    "    \n",
    "    print(f\"✓ Created JSON data for {len(json_data)} samples\")\n",
    "    return json_data\n",
    "\n",
    "print(\"✓ JSON creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f7bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING INLEGALBERT EMBEDDINGS\n",
      "============================================================\n",
      "Texts to process:\n",
      "  Train: 520247 texts\n",
      "  Test: 149868 texts\n",
      "  Val: 293408 texts\n",
      "  Total: 963523 texts\n",
      "\n",
      "Embedding configuration:\n",
      "  Max length: 512\n",
      "  Batch size: 2\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all datasets\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING INLEGALBERT EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare text data\n",
    "train_texts = df_train[\"text\"].tolist() if not df_train.empty else []\n",
    "test_texts = df_test[\"text\"].tolist() if not df_test.empty else []\n",
    "val_texts = df_val[\"text\"].tolist() if not df_val.empty else []\n",
    "\n",
    "print(f\"Texts to process:\")\n",
    "print(f\"  Train: {len(train_texts)} texts\")\n",
    "print(f\"  Test: {len(test_texts)} texts\") \n",
    "print(f\"  Val: {len(val_texts)} texts\")\n",
    "print(f\"  Total: {len(train_texts) + len(test_texts) + len(val_texts)} texts\")\n",
    "\n",
    "# Configuration for embedding generation\n",
    "MAX_LENGTH = 512  # BERT's typical max length\n",
    "BATCH_SIZE = 4 if device.type == 'cuda' else 2  # Smaller batch size to avoid memory issues\n",
    "\n",
    "print(f\"\\nEmbedding configuration:\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Start timing\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26920832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 DEMO MODE: Processing small sample first\n",
      "============================================================\n",
      "Demo sample size: 10\n",
      "Demo texts:\n",
      "1. [Issue]  K. Mathur, J. This appeal is directed against the order passed by the National Consumer Disputes Re...\n",
      "2. [Facts] Brief facts giving rise to this appeal areThe respondent complainant M s Kiran Combers Spinners file...\n",
      "3. [Facts] The case of the complainant respondent was that they got their building and stock insured from the U...\n",
      "4. [Facts] The respondent complainant held a valid Fire Policy for its stock (Building Rs. 25 lakhs, Machinery ...\n",
      "5. [Facts] This policy also endorsed to cover risk of flood.\n",
      "6. [Facts] On account of heavy rains and floods in the city, insured property was affected by floods on 24th Ju...\n",
      "7. [Facts] This incident was reported to the Company on 25th July, 1993 and an FIR was lodged on 27th July, 199...\n",
      "8. [Facts] The respondentclaimant claimed Rs.20,03,842/ in July, 1993 from the Company.\n",
      "9. [Facts] Surveyor, namely, M s Vij Engineers Enterprise appointed by the Company carried out its preliminary ...\n",
      "10. [Facts] Second Surveyor M s Mita Marine and General Survey Agencies Pvt.\n",
      "\n",
      "🔄 Generating embeddings for demo sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Demo embeddings generated!\n",
      "📊 Shape: (10, 768)\n",
      "📊 Embedding dimension: 768\n",
      "📊 Sample embedding (first 5 values): [-0.05631871 -0.25117683  0.42925707 -0.5550534  -0.05736984]\n",
      "Creating JSON data for demo set...\n",
      "✓ Created JSON data for 10 samples\n",
      "✅ Demo JSON data created with 10 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DEMO: Generate embeddings for a small sample first (for testing)\n",
    "print(\"🧪 DEMO MODE: Processing small sample first\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "DEMO_SIZE = 10  # Process only 10 samples for demo\n",
    "demo_train_texts = train_texts[:DEMO_SIZE]\n",
    "demo_train_labels = train_labels[:DEMO_SIZE] \n",
    "demo_train_label_numbers = train_label_numbers[:DEMO_SIZE]\n",
    "\n",
    "print(f\"Demo sample size: {DEMO_SIZE}\")\n",
    "print(\"Demo texts:\")\n",
    "for i, (text, label) in enumerate(zip(demo_train_texts, demo_train_labels)):\n",
    "    preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "    print(f\"{i+1}. [{label}] {preview}\")\n",
    "\n",
    "print(f\"\\n🔄 Generating embeddings for demo sample...\")\n",
    "demo_embeddings = get_bert_embeddings(\n",
    "    demo_train_texts, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    device, \n",
    "    max_length=MAX_LENGTH, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "print(f\"✅ Demo embeddings generated!\")\n",
    "print(f\"📊 Shape: {demo_embeddings.shape}\")\n",
    "print(f\"📊 Embedding dimension: {demo_embeddings.shape[1]}\")\n",
    "print(f\"📊 Sample embedding (first 5 values): {demo_embeddings[0][:5]}\")\n",
    "\n",
    "# Create demo JSON\n",
    "demo_json_data = create_json_data_bert(\n",
    "    demo_train_texts, \n",
    "    demo_embeddings, \n",
    "    demo_train_labels, \n",
    "    demo_train_label_numbers, \n",
    "    \"demo set\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Demo JSON data created with {len(demo_json_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e927807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for test data\n",
    "if test_texts:\n",
    "    print(f\"\\n Processing test data ({len(test_texts)} samples)...\")\n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    test_embeddings = get_bert_embeddings(\n",
    "        test_texts, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        device, \n",
    "        max_length=MAX_LENGTH, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    print(f\"✓ Test embeddings shape: {test_embeddings.shape}\")\n",
    "    \n",
    "    # Create JSON data for test (no labels)\n",
    "    test_json_data = create_json_data_bert(\n",
    "        test_texts, \n",
    "        test_embeddings, \n",
    "        dataset_name=\"test set\"\n",
    "    )\n",
    "    \n",
    "    print(f\" Test data processing time: {time.time() - test_start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\" No test data available\")\n",
    "    test_json_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for validation data\n",
    "if val_texts:\n",
    "    print(f\"\\nProcessing validation data ({len(val_texts)} samples)...\")\n",
    "    val_start_time = time.time()\n",
    "    \n",
    "    val_embeddings = get_bert_embeddings(\n",
    "        val_texts, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        device, \n",
    "        max_length=MAX_LENGTH, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    print(f\"✓ Val embeddings shape: {val_embeddings.shape}\")\n",
    "    \n",
    "    # Create JSON data for validation (no labels)\n",
    "    val_json_data = create_json_data_bert(\n",
    "        val_texts, \n",
    "        val_embeddings, \n",
    "        dataset_name=\"validation set\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation data processing time: {time.time() - val_start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\" No validation data available\")\n",
    "    val_json_data = []\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n Total embedding generation time: {total_time:.2f} seconds\")\n",
    "print(f\" Average time per sample: {total_time / (len(train_texts) + len(test_texts) + len(val_texts)):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6855790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to JSON files\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING EMBEDDINGS TO JSON FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(embeddings_output_path, exist_ok=True)\n",
    "print(f\"📁 Output directory: {embeddings_output_path}\")\n",
    "\n",
    "# Save training embeddings\n",
    "if train_json_data:\n",
    "    train_file_path = os.path.join(embeddings_output_path, 'train_embeddings_inlegalbert.json')\n",
    "    with open(train_file_path, 'w') as f:\n",
    "        json.dump(train_json_data, f, indent=2)\n",
    "    print(f\"Saved train_embeddings_inlegalbert.json with {len(train_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(train_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save test embeddings  \n",
    "if test_json_data:\n",
    "    test_file_path = os.path.join(embeddings_output_path, 'test_embeddings_inlegalbert.json')\n",
    "    with open(test_file_path, 'w') as f:\n",
    "        json.dump(test_json_data, f, indent=2)\n",
    "    print(f\"✅ Saved test_embeddings_inlegalbert.json with {len(test_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(test_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save validation embeddings\n",
    "if val_json_data:\n",
    "    val_file_path = os.path.join(embeddings_output_path, 'val_embeddings_inlegalbert.json')\n",
    "    with open(val_file_path, 'w') as f:\n",
    "        json.dump(val_json_data, f, indent=2)\n",
    "    print(f\"✅ Saved val_embeddings_inlegalbert.json with {len(val_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(val_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save label mapping for reference (if available)\n",
    "if 'label_mapping' in locals():\n",
    "    label_file_path = os.path.join(embeddings_output_path, 'label_mapping_inlegalbert.json')\n",
    "    with open(label_file_path, 'w') as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    print(f\"✅ Saved label_mapping_inlegalbert.json\")\n",
    "\n",
    "print(f\"\\n📂 All files saved in: {os.path.abspath(embeddings_output_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample JSON structure and summary\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY AND SAMPLE OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample data structure\n",
    "if train_json_data:\n",
    "    print(\"\\n📋 Sample JSON structure (train data):\")\n",
    "    sample = train_json_data[0].copy()\n",
    "    \n",
    "    # Show only first 5 vector elements for readability\n",
    "    if 'vector' in sample and len(sample['vector']) > 5:\n",
    "        original_length = len(sample['vector'])\n",
    "        sample['vector'] = sample['vector'][:5] + [f'... ({original_length-5} more values)']\n",
    "    \n",
    "    print(json.dumps(sample, indent=2))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📊 FINAL SUMMARY:\")\n",
    "print(f\"✅ InLegalBERT Model: law-ai/InLegalBERT\")\n",
    "print(f\"✅ Embedding dimension: {model.config.hidden_size}\")\n",
    "print(f\"✅ Device used: {device}\")\n",
    "\n",
    "if train_json_data:\n",
    "    print(f\"✅ train_embeddings_inlegalbert.json: {len(train_json_data)} samples with labels\")\n",
    "if test_json_data:\n",
    "    print(f\"✅ test_embeddings_inlegalbert.json: {len(test_json_data)} samples without labels\")\n",
    "if val_json_data:\n",
    "    print(f\"✅ val_embeddings_inlegalbert.json: {len(val_json_data)} samples without labels\")\n",
    "if 'label_mapping' in locals():\n",
    "    print(f\"✅ label_mapping_inlegalbert.json: Label number to name mapping\")\n",
    "\n",
    "print(f\"\\n🎉 InLegalBERT embedding generation completed successfully!\")\n",
    "print(f\"📁 Files are saved in: {embeddings_output_path}\")\n",
    "\n",
    "# Cleanup to free memory\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(f\"🧹 Memory cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
