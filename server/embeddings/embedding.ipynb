{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ce1cc6",
   "metadata": {},
   "source": [
    "# InLegalBERT Embedding Generation for Legal Text Classification\n",
    "\n",
    "This notebook generates embeddings for legal text classification using **InLegalBERT** - a BERT model specifically pre-trained on Indian legal documents.\n",
    "\n",
    "## Dataset Structure\n",
    "- **Train**: Files with text and labels (Facts, Reasoning, Arguments of Respondent, Arguments of Petitioner, Decision, Issue)\n",
    "- **Test**: Files with only text (no labels)\n",
    "- **Val**: Files with only text (no labels)\n",
    "\n",
    "## Output\n",
    "- JSON files with text, embeddings, class names, and class numbers\n",
    "- Saved in the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdd3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uttam/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0f6912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/train\n",
      "Test path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/test\n",
      "Val path: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/val/val\n",
      "Output path: /home/uttam/B.Tech Major Project/nyaya/server/embeddings\n",
      "‚úì Train directory exists with 4994 files\n",
      "‚úì Test directory exists with 712 files\n",
      "‚úì Val directory exists with 1424 files\n"
     ]
    }
   ],
   "source": [
    "# Set device for GPU acceleration if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset paths (adjusted for local structure)\n",
    "base_path = \"/home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF\"\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "test_path = os.path.join(base_path, \"test\") \n",
    "val_path = os.path.join(base_path, \"val\", \"val\")\n",
    "\n",
    "# Output path for embeddings\n",
    "embeddings_output_path = \"/home/uttam/B.Tech Major Project/nyaya/server/embeddings\"\n",
    "\n",
    "print(f\"Train path: {train_path}\")\n",
    "print(f\"Test path: {test_path}\")\n",
    "print(f\"Val path: {val_path}\")\n",
    "print(f\"Output path: {embeddings_output_path}\")\n",
    "\n",
    "# Verify paths exist\n",
    "for path_name, path in [(\"Train\", train_path), (\"Test\", test_path), (\"Val\", val_path)]:\n",
    "    if os.path.exists(path):\n",
    "        file_count = len([f for f in os.listdir(path) if f.endswith('.txt')])\n",
    "        print(f\"‚úì {path_name} directory exists with {file_count} files\")\n",
    "    else:\n",
    "        print(f\"‚úó {path_name} directory not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Dataset Size Configuration - Adjust these to control dataset size\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SIZE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set limits for each dataset split (None = process all files)\n",
    "TRAIN_FILE_LIMIT = 100   # Process only first 100 training files (was 4994)\n",
    "TEST_FILE_LIMIT = 50     # Process only first 50 test files (was 712)  \n",
    "VAL_FILE_LIMIT = 50      # Process only first 50 validation files (was 1424)\n",
    "\n",
    "# Alternative: Use percentage of total dataset\n",
    "# TRAIN_PERCENTAGE = 0.02  # Use 2% of training data\n",
    "# TEST_PERCENTAGE = 0.07   # Use 7% of test data\n",
    "# VAL_PERCENTAGE = 0.04    # Use 4% of validation data\n",
    "\n",
    "print(f\"üìã Dataset Limits Configuration:\")\n",
    "print(f\"   Train files limit: {TRAIN_FILE_LIMIT if TRAIN_FILE_LIMIT else 'No limit (all files)'}\")\n",
    "print(f\"   Test files limit: {TEST_FILE_LIMIT if TEST_FILE_LIMIT else 'No limit (all files)'}\")\n",
    "print(f\"   Val files limit: {VAL_FILE_LIMIT if VAL_FILE_LIMIT else 'No limit (all files)'}\")\n",
    "print(f\"   Expected total files to process: ~{(TRAIN_FILE_LIMIT or 0) + (TEST_FILE_LIMIT or 0) + (VAL_FILE_LIMIT or 0)}\")\n",
    "\n",
    "print(f\"\\nüí° To process full dataset, set all limits to None\")\n",
    "print(f\"üí° Current settings will process ~{((TRAIN_FILE_LIMIT or 0) + (TEST_FILE_LIMIT or 0) + (VAL_FILE_LIMIT or 0)) / 71.3:.1f}% of total dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_files(directory_path, file_limit=None):\n",
    "    \"\"\"Load training files with labels\"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(f\"Loading training files from: {directory_path}\")\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(files)} total files\")\n",
    "    \n",
    "    # Apply file limit if specified\n",
    "    if file_limit is not None:\n",
    "        files = files[:file_limit]\n",
    "        print(f\"üîÑ Processing only first {len(files)} files (limit: {file_limit})\")\n",
    "    else:\n",
    "        print(f\"üîÑ Processing all {len(files)} files\")\n",
    "    \n",
    "    for file_name in tqdm(files, desc=\"Loading train files\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "            if not df.empty:\n",
    "                # üîß Replace NaN with \"None\"\n",
    "                df[\"label\"] = df[\"label\"].fillna(\"None\")\n",
    "                # üîß Normalize label values\n",
    "                df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "                df[\"label\"] = df[\"label\"].replace(\n",
    "                    {\"none\": \"None\", \"NONE\": \"None\"}  # unify casing\n",
    "                )\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        result_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Successfully loaded {len(result_df)} training samples from {len(files)} files\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid training data found\")\n",
    "        return pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "def load_test_val_files(directory_path, file_limit=None):\n",
    "    \"\"\"Load test/val files with only text (no labels)\"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(f\"Loading test/val files from: {directory_path}\")\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(files)} total files\")\n",
    "    \n",
    "    # Apply file limit if specified\n",
    "    if file_limit is not None:\n",
    "        files = files[:file_limit]\n",
    "        print(f\"üîÑ Processing only first {len(files)} files (limit: {file_limit})\")\n",
    "    else:\n",
    "        print(f\"üîÑ Processing all {len(files)} files\")\n",
    "    \n",
    "    for file_name in tqdm(files, desc=\"Loading test/val files\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\"])\n",
    "            if not df.empty:\n",
    "                # üîß Normalize text (strip whitespace)\n",
    "                df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "                all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        result_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Successfully loaded {len(result_df)} samples from {len(files)} files\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid test/val data found\")\n",
    "        return pd.DataFrame(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa175939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading training files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/train\n",
      "Found 4994 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4994/4994 [00:19<00:00, 260.67it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 520247 training samples\n",
      "Loading test/val files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/test\n",
      "Found 712 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test/val files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 712/712 [00:03<00:00, 227.54it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 149868 samples\n",
      "Loading test/val files from: /home/uttam/B.Tech Major Project/nyaya/server/dataset/Hier_BiLSTM_CRF/val/val\n",
      "Found 1424 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test/val files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1424/1424 [00:05<00:00, 271.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 293408 samples\n",
      "\n",
      "Dataset Summary:\n",
      "Train: 520247 rows\n",
      "Test: 149868 rows\n",
      "Val: 293408 rows\n",
      "\n",
      "Train labels distribution:\n",
      "label\n",
      "Reasoning                  202593\n",
      "Facts                      170068\n",
      "Arguments of Petitioner     65032\n",
      "Arguments of Respondent     50137\n",
      "Decision                    19599\n",
      "Issue                       12818\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample train data:\n",
      "                                                text  label\n",
      "0   K. Mathur, J. This appeal is directed against...  Issue\n",
      "1  Brief facts giving rise to this appeal areThe ...  Facts\n",
      "2  The case of the complainant respondent was tha...  Facts\n",
      "3  The respondent complainant held a valid Fire P...  Facts\n",
      "4  This policy also endorsed to cover risk of flood.  Facts\n"
     ]
    }
   ],
   "source": [
    "# Load datasets with file limits\n",
    "print(\"Loading datasets with configured limits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_train = load_train_files(train_path, file_limit=TRAIN_FILE_LIMIT)  # has text + label\n",
    "df_test = load_test_val_files(test_path, file_limit=TEST_FILE_LIMIT)  # only text  \n",
    "df_val = load_test_val_files(val_path, file_limit=VAL_FILE_LIMIT)    # only text\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nüìä Dataset Summary (After Applying Limits):\")\n",
    "print(f\"Train: {len(df_train)} rows\")\n",
    "print(f\"Test: {len(df_test)} rows\")\n",
    "print(f\"Val: {len(df_val)} rows\")\n",
    "print(f\"Total samples: {len(df_train) + len(df_test) + len(df_val)}\")\n",
    "\n",
    "if not df_train.empty:\n",
    "    print(f\"\\nüìã Train labels distribution:\")\n",
    "    print(df_train[\"label\"].value_counts())\n",
    "    \n",
    "    print(f\"\\nüìÑ Sample train data:\")\n",
    "    print(df_train.head(3))\n",
    "    \n",
    "    # Show average text length\n",
    "    avg_length = df_train[\"text\"].str.len().mean()\n",
    "    print(f\"\\nüìè Average text length: {avg_length:.1f} characters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a98b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating label mappings...\n",
      "Unique labels in dataset: ['Issue' 'Facts' 'Arguments of Petitioner' 'Arguments of Respondent'\n",
      " 'Reasoning' 'Decision']\n",
      "Using manual mapping:\n",
      "0: Facts\n",
      "1: Reasoning\n",
      "2: Arguments of Respondent\n",
      "3: Arguments of Petitioner\n",
      "4: Decision\n",
      "5: Issue\n",
      "6: None\n",
      "\n",
      "Label distribution by number:\n",
      "0 (Facts): 170068\n",
      "1 (Reasoning): 202593\n",
      "2 (Arguments of Respondent): 50137\n",
      "3 (Arguments of Petitioner): 65032\n",
      "4 (Decision): 19599\n",
      "5 (Issue): 12818\n"
     ]
    }
   ],
   "source": [
    "# Label encoding for training data\n",
    "if not df_train.empty:\n",
    "    # Manual mapping (similar to your original code)\n",
    "    label_to_num = {\n",
    "        'Facts': 0,\n",
    "        'Reasoning': 1, \n",
    "        'Arguments of Respondent': 2,\n",
    "        'Arguments of Petitioner': 3,\n",
    "        'Decision': 4,\n",
    "        'Issue': 5,\n",
    "        'None': 6\n",
    "    }\n",
    "    \n",
    "    print(\"Creating label mappings...\")\n",
    "    \n",
    "    # Check if all labels in data are in our mapping\n",
    "    unique_labels = df_train['label'].unique()\n",
    "    print(f\"Unique labels in dataset: {unique_labels}\")\n",
    "    \n",
    "    missing_labels = [label for label in unique_labels if label not in label_to_num]\n",
    "    if missing_labels:\n",
    "        print(f\"Warning: Missing labels in mapping: {missing_labels}\")\n",
    "        \n",
    "        # Use LabelEncoder as fallback for missing labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_train['label_encoded'] = label_encoder.fit_transform(df_train['label'])\n",
    "        \n",
    "        # Create updated mapping\n",
    "        label_mapping = {}\n",
    "        for i, label in enumerate(label_encoder.classes_):\n",
    "            label_mapping[i] = label\n",
    "            \n",
    "        train_labels = df_train['label'].tolist()\n",
    "        train_label_numbers = df_train['label_encoded'].tolist()\n",
    "        \n",
    "        print(\"Using LabelEncoder mapping:\")\n",
    "        for i, label in label_mapping.items():\n",
    "            print(f\"{i}: {label}\")\n",
    "    else:\n",
    "        # Use manual mapping\n",
    "        df_train['label_numeric'] = df_train['label'].map(label_to_num)\n",
    "        \n",
    "        label_mapping = {v: k for k, v in label_to_num.items()}  # Reverse mapping\n",
    "        train_labels = df_train['label'].tolist()\n",
    "        train_label_numbers = df_train['label_numeric'].tolist()\n",
    "        \n",
    "        print(\"Using manual mapping:\")\n",
    "        for num, label in label_mapping.items():\n",
    "            print(f\"{num}: {label}\")\n",
    "            \n",
    "    print(f\"\\nLabel distribution by number:\")\n",
    "    label_counts = {}\n",
    "    for label_num in train_label_numbers:\n",
    "        label_counts[label_num] = label_counts.get(label_num, 0) + 1\n",
    "    for num, count in sorted(label_counts.items()):\n",
    "        print(f\"{num} ({label_mapping[num]}): {count}\")\n",
    "else:\n",
    "    print(\"No training data available for label encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee33144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading InLegalBERT model and tokenizer...\n",
      "This may take a few minutes on first run...\n",
      "‚úì InLegalBERT loaded successfully!\n",
      "‚úì Model moved to: cpu\n",
      "‚úì Tokenizer vocabulary size: 30522\n",
      "‚úì Model max position embeddings: 512\n",
      "‚úì Hidden size: 768\n",
      "‚úì InLegalBERT loaded successfully!\n",
      "‚úì Model moved to: cpu\n",
      "‚úì Tokenizer vocabulary size: 30522\n",
      "‚úì Model max position embeddings: 512\n",
      "‚úì Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "# Load InLegalBERT model and tokenizer\n",
    "print(\"Loading InLegalBERT model and tokenizer...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "    \n",
    "    # Move model to device (GPU if available)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"‚úì InLegalBERT loaded successfully!\")\n",
    "    print(f\"‚úì Model moved to: {device}\")\n",
    "    print(f\"‚úì Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"‚úì Model max position embeddings: {model.config.max_position_embeddings}\")\n",
    "    print(f\"‚úì Hidden size: {model.config.hidden_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading InLegalBERT: {e}\")\n",
    "    print(\"Please ensure you have internet connection and transformers library installed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46994adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding function defined\n",
      "This function will use the [CLS] token representation as sentence embeddings\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(texts, tokenizer, model, device, max_length=512, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate embeddings using InLegalBERT\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        tokenizer: InLegalBERT tokenizer\n",
    "        model: InLegalBERT model\n",
    "        device: torch device (cuda/cpu)\n",
    "        max_length: Maximum sequence length for BERT\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (texts x hidden_size)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {key: val.to(device) for key, val in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use [CLS] token embedding (first token) as sentence representation\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Move back to CPU and convert to numpy\n",
    "            batch_embeddings = cls_embeddings.cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = np.vstack(embeddings)\n",
    "    return all_embeddings\n",
    "\n",
    "print(\"‚úì Embedding function defined\")\n",
    "print(\"This function will use the [CLS] token representation as sentence embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6c487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì JSON creation function defined\n"
     ]
    }
   ],
   "source": [
    "def create_json_data_bert(texts, embeddings, labels=None, label_numbers=None, dataset_name=\"\"):\n",
    "    \"\"\"Create JSON data with text, BERT embeddings, classname, classnumber\"\"\"\n",
    "    print(f\"Creating JSON data for {dataset_name}...\")\n",
    "    json_data = []\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        data_point = {\n",
    "            \"text\": texts[i],\n",
    "            \"vector\": embeddings[i].tolist(),  # Convert numpy array to list for JSON\n",
    "        }\n",
    "        \n",
    "        if labels is not None and label_numbers is not None:\n",
    "            data_point[\"classname\"] = labels[i]\n",
    "            data_point[\"classnumber\"] = int(label_numbers[i])\n",
    "        else:\n",
    "            data_point[\"classname\"] = None\n",
    "            data_point[\"classnumber\"] = None\n",
    "            \n",
    "        json_data.append(data_point)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(texts)} samples\")\n",
    "    \n",
    "    print(f\"‚úì Created JSON data for {len(json_data)} samples\")\n",
    "    return json_data\n",
    "\n",
    "print(\"‚úì JSON creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING INLEGALBERT EMBEDDINGS\n",
      "============================================================\n",
      "Texts to process:\n",
      "  Train: 520247 texts\n",
      "  Test: 149868 texts\n",
      "  Val: 293408 texts\n",
      "  Total: 963523 texts\n",
      "\n",
      "Embedding configuration:\n",
      "  Max length: 512\n",
      "  Batch size: 2\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all datasets\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING INLEGALBERT EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare text data\n",
    "train_texts = df_train[\"text\"].tolist() if not df_train.empty else []\n",
    "test_texts = df_test[\"text\"].tolist() if not df_test.empty else []\n",
    "val_texts = df_val[\"text\"].tolist() if not df_val.empty else []\n",
    "\n",
    "total_samples = len(train_texts) + len(test_texts) + len(val_texts)\n",
    "\n",
    "print(f\"üìã Texts to process:\")\n",
    "print(f\"  Train: {len(train_texts)} texts\")\n",
    "print(f\"  Test: {len(test_texts)} texts\") \n",
    "print(f\"  Val: {len(val_texts)} texts\")\n",
    "print(f\"  Total: {total_samples} texts\")\n",
    "\n",
    "# Configuration for embedding generation (optimized for smaller datasets)\n",
    "MAX_LENGTH = 512  # BERT's typical max length\n",
    "\n",
    "# Adjust batch size based on dataset size and device\n",
    "if total_samples <= 500:\n",
    "    BATCH_SIZE = 8 if device.type == 'cuda' else 4  # Larger batches for small datasets\n",
    "elif total_samples <= 2000:\n",
    "    BATCH_SIZE = 6 if device.type == 'cuda' else 3  # Medium batches\n",
    "else:\n",
    "    BATCH_SIZE = 4 if device.type == 'cuda' else 2  # Conservative for large datasets\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Embedding configuration:\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (auto-adjusted based on dataset size)\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Estimate processing time\n",
    "estimated_time = total_samples * 0.1  # Rough estimate: 0.1 seconds per sample\n",
    "print(f\"  Estimated processing time: {estimated_time/60:.1f} minutes\")\n",
    "\n",
    "# Start timing\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26920832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ DEMO MODE: Processing small sample first\n",
      "============================================================\n",
      "Demo sample size: 10\n",
      "Demo texts:\n",
      "1. [Issue]  K. Mathur, J. This appeal is directed against the order passed by the National Consumer Disputes Re...\n",
      "2. [Facts] Brief facts giving rise to this appeal areThe respondent complainant M s Kiran Combers Spinners file...\n",
      "3. [Facts] The case of the complainant respondent was that they got their building and stock insured from the U...\n",
      "4. [Facts] The respondent complainant held a valid Fire Policy for its stock (Building Rs. 25 lakhs, Machinery ...\n",
      "5. [Facts] This policy also endorsed to cover risk of flood.\n",
      "6. [Facts] On account of heavy rains and floods in the city, insured property was affected by floods on 24th Ju...\n",
      "7. [Facts] This incident was reported to the Company on 25th July, 1993 and an FIR was lodged on 27th July, 199...\n",
      "8. [Facts] The respondentclaimant claimed Rs.20,03,842/ in July, 1993 from the Company.\n",
      "9. [Facts] Surveyor, namely, M s Vij Engineers Enterprise appointed by the Company carried out its preliminary ...\n",
      "10. [Facts] Second Surveyor M s Mita Marine and General Survey Agencies Pvt.\n",
      "\n",
      "üîÑ Generating embeddings for demo sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Demo embeddings generated!\n",
      "üìä Shape: (10, 768)\n",
      "üìä Embedding dimension: 768\n",
      "üìä Sample embedding (first 5 values): [-0.05631871 -0.25117683  0.42925707 -0.5550534  -0.05736984]\n",
      "Creating JSON data for demo set...\n",
      "‚úì Created JSON data for 10 samples\n",
      "‚úÖ Demo JSON data created with 10 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Skip demo mode since we're already using a reduced dataset\n",
    "print(\"üöÄ PROCESSING REDUCED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä Dataset is already reduced to manageable size:\")\n",
    "print(f\"   Train samples: {len(train_texts) if 'train_texts' in locals() else 0}\")\n",
    "print(f\"   Test samples: {len(test_texts) if 'test_texts' in locals() else 0}\")\n",
    "print(f\"   Val samples: {len(val_texts) if 'val_texts' in locals() else 0}\")\n",
    "\n",
    "if 'train_texts' in locals() and train_texts:\n",
    "    print(f\"\\nüìÑ Sample training texts:\")\n",
    "    for i, (text, label) in enumerate(zip(train_texts[:3], train_labels[:3])):\n",
    "        preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "        print(f\"{i+1}. [{label}] {preview}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to process full reduced dataset\")\n",
    "print(f\"   No demo mode needed - dataset size is already optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4dc61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing training data (520247 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 786/260124 [02:12<12:07:15,  5.94it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_texts:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Processing training data (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_embeddings = \u001b[43mget_bert_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Train embeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_embeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Create JSON data for training\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mget_bert_embeddings\u001b[39m\u001b[34m(texts, tokenizer, model, device, max_length, batch_size)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Use [CLS] token embedding (first token) as sentence representation\u001b[39;00m\n\u001b[32m     38\u001b[39m     cls_embeddings = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# Shape: (batch_size, hidden_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1006\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1004\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1020\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:653\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    649\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    651\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:562\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    553\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    561\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    571\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:502\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    484\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    485\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    491\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    492\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    493\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    494\u001b[39m         hidden_states,\n\u001b[32m    495\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    500\u001b[39m         cache_position=cache_position,\n\u001b[32m    501\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:444\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    442\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m    443\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/B.Tech Major Project/nyaya/server/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2905\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2897\u001b[39m         layer_norm,\n\u001b[32m   2898\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2903\u001b[39m         eps=eps,\n\u001b[32m   2904\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2906\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2907\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate embeddings for training data\n",
    "if train_texts:\n",
    "    print(f\"\\n Processing training data ({len(train_texts)} samples)...\")\n",
    "    train_embeddings = get_bert_embeddings(\n",
    "        train_texts, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        device, \n",
    "        max_length=MAX_LENGTH, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    print(f\"‚úì Train embeddings shape: {train_embeddings.shape}\")\n",
    "    \n",
    "    # Create JSON data for training\n",
    "    train_json_data = create_json_data_bert(\n",
    "        train_texts, \n",
    "        train_embeddings, \n",
    "        train_labels, \n",
    "        train_label_numbers, \n",
    "        \"train set\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No training data available\")\n",
    "    train_json_data = []\n",
    "\n",
    "print(f\"\\n Training data processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e927807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for test data\n",
    "if test_texts:\n",
    "    print(f\"\\n Processing test data ({len(test_texts)} samples)...\")\n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    test_embeddings = get_bert_embeddings(\n",
    "        test_texts, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        device, \n",
    "        max_length=MAX_LENGTH, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    print(f\"‚úì Test embeddings shape: {test_embeddings.shape}\")\n",
    "    \n",
    "    # Create JSON data for test (no labels)\n",
    "    test_json_data = create_json_data_bert(\n",
    "        test_texts, \n",
    "        test_embeddings, \n",
    "        dataset_name=\"test set\"\n",
    "    )\n",
    "    \n",
    "    print(f\" Test data processing time: {time.time() - test_start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\" No test data available\")\n",
    "    test_json_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for validation data\n",
    "if val_texts:\n",
    "    print(f\"\\nProcessing validation data ({len(val_texts)} samples)...\")\n",
    "    val_start_time = time.time()\n",
    "    \n",
    "    val_embeddings = get_bert_embeddings(\n",
    "        val_texts, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        device, \n",
    "        max_length=MAX_LENGTH, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    print(f\"‚úì Val embeddings shape: {val_embeddings.shape}\")\n",
    "    \n",
    "    # Create JSON data for validation (no labels)\n",
    "    val_json_data = create_json_data_bert(\n",
    "        val_texts, \n",
    "        val_embeddings, \n",
    "        dataset_name=\"validation set\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation data processing time: {time.time() - val_start_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\" No validation data available\")\n",
    "    val_json_data = []\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n Total embedding generation time: {total_time:.2f} seconds\")\n",
    "print(f\" Average time per sample: {total_time / (len(train_texts) + len(test_texts) + len(val_texts)):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6855790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to JSON files\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING EMBEDDINGS TO JSON FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(embeddings_output_path, exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {embeddings_output_path}\")\n",
    "\n",
    "# Save training embeddings\n",
    "if train_json_data:\n",
    "    train_file_path = os.path.join(embeddings_output_path, 'train_embeddings_inlegalbert.json')\n",
    "    with open(train_file_path, 'w') as f:\n",
    "        json.dump(train_json_data, f, indent=2)\n",
    "    print(f\"Saved train_embeddings_inlegalbert.json with {len(train_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(train_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save test embeddings  \n",
    "if test_json_data:\n",
    "    test_file_path = os.path.join(embeddings_output_path, 'test_embeddings_inlegalbert.json')\n",
    "    with open(test_file_path, 'w') as f:\n",
    "        json.dump(test_json_data, f, indent=2)\n",
    "    print(f\"‚úÖ Saved test_embeddings_inlegalbert.json with {len(test_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(test_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save validation embeddings\n",
    "if val_json_data:\n",
    "    val_file_path = os.path.join(embeddings_output_path, 'val_embeddings_inlegalbert.json')\n",
    "    with open(val_file_path, 'w') as f:\n",
    "        json.dump(val_json_data, f, indent=2)\n",
    "    print(f\"‚úÖ Saved val_embeddings_inlegalbert.json with {len(val_json_data)} samples\")\n",
    "    print(f\"   File size: {os.path.getsize(val_file_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Save label mapping for reference (if available)\n",
    "if 'label_mapping' in locals():\n",
    "    label_file_path = os.path.join(embeddings_output_path, 'label_mapping_inlegalbert.json')\n",
    "    with open(label_file_path, 'w') as f:\n",
    "        json.dump(label_mapping, f, indent=2)\n",
    "    print(f\"‚úÖ Saved label_mapping_inlegalbert.json\")\n",
    "\n",
    "print(f\"\\nüìÇ All files saved in: {os.path.abspath(embeddings_output_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample JSON structure and summary\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY AND SAMPLE OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample data structure\n",
    "if train_json_data:\n",
    "    print(\"\\nüìã Sample JSON structure (train data):\")\n",
    "    sample = train_json_data[0].copy()\n",
    "    \n",
    "    # Show only first 5 vector elements for readability\n",
    "    if 'vector' in sample and len(sample['vector']) > 5:\n",
    "        original_length = len(sample['vector'])\n",
    "        sample['vector'] = sample['vector'][:5] + [f'... ({original_length-5} more values)']\n",
    "    \n",
    "    print(json.dumps(sample, indent=2))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä FINAL SUMMARY:\")\n",
    "print(f\"‚úÖ InLegalBERT Model: law-ai/InLegalBERT\")\n",
    "print(f\"‚úÖ Embedding dimension: {model.config.hidden_size}\")\n",
    "print(f\"‚úÖ Device used: {device}\")\n",
    "\n",
    "if train_json_data:\n",
    "    print(f\"‚úÖ train_embeddings_inlegalbert.json: {len(train_json_data)} samples with labels\")\n",
    "if test_json_data:\n",
    "    print(f\"‚úÖ test_embeddings_inlegalbert.json: {len(test_json_data)} samples without labels\")\n",
    "if val_json_data:\n",
    "    print(f\"‚úÖ val_embeddings_inlegalbert.json: {len(val_json_data)} samples without labels\")\n",
    "if 'label_mapping' in locals():\n",
    "    print(f\"‚úÖ label_mapping_inlegalbert.json: Label number to name mapping\")\n",
    "\n",
    "print(f\"\\nüéâ InLegalBERT embedding generation completed successfully!\")\n",
    "print(f\"üìÅ Files are saved in: {embeddings_output_path}\")\n",
    "\n",
    "# Cleanup to free memory\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(f\"üßπ Memory cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
