# Two-Model Architecture: Complete Flow Diagram

## ðŸŽ¯ The Complete Picture

Your system uses **TWO embedding models** working in **parallel pipelines**:

---

## Pipeline 1: Role Classification (InLegalBERT)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ROLE CLASSIFICATION PIPELINE                       â”‚
â”‚                      (InLegalBERT Embeddings)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: "The petitioner filed a writ petition under Article 32."
   â”‚
   â”œâ”€â†’ InLegalBERT Tokenizer
   â”‚      â†“
   â”‚   [101, 2256, 28404, 4718, 1037, 23308, ...]  (Token IDs)
   â”‚      â†“
   â”œâ”€â†’ InLegalBERT Model
   â”‚      â†“
   â”‚   [0.234, -0.567, 0.891, ..., -0.123]  (768-dim embedding)
   â”‚      â†“
   â”‚   âš ï¸  EPHEMERAL - Not stored anywhere!
   â”‚      â†“
   â”œâ”€â†’ Classification Head (Linear layer)
   â”‚      â†“
   â”‚   [2.1, -0.3, 5.7, 1.2, -1.8, 0.4, -2.1]  (7-dim logits)
   â”‚      â†“
   â”œâ”€â†’ Softmax
   â”‚      â†“
   â”‚   [0.05, 0.02, 0.92, 0.01, 0.00, 0.00, 0.00]  (Probabilities)
   â”‚      â†“
   â””â”€â†’ argmax
         â†“
OUTPUT: role = "Facts" (class 2), confidence = 0.92

âœ… Result goes to â†’ Pinecone metadata (role label only)
âŒ InLegalBERT embedding â†’ DISCARDED (not stored)
```

---

## Pipeline 2: Semantic Embedding (text-embedding-005)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SEMANTIC EMBEDDING PIPELINE                          â”‚
â”‚                    (text-embedding-005 Embeddings)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: "The petitioner filed a writ petition under Article 32."
   â”‚
   â”œâ”€â†’ Vertex AI text-embedding-005
   â”‚      â†“
   â”‚   [0.123, -0.456, 0.789, ..., 0.321]  (768-dim embedding)
   â”‚      â†“
   â”‚   âœ… STORED - This goes to Pinecone!
   â”‚      â†“
   â””â”€â†’ Pinecone Vector Database
         â†“
OUTPUT: Vector stored with ID "sent_a3b2c1d4"

âœ… Result goes to â†’ Pinecone vectors (actual embedding)
âœ… text-embedding-005 embedding â†’ STORED in Pinecone
```

---

## Combined: What Actually Goes Into Pinecone

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PINECONE STORAGE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

{
  "id": "sent_a3b2c1d4",
  
  "values": [0.123, -0.456, 0.789, ..., 0.321],
            â†‘
            â””â”€â”€ From text-embedding-005 (Pipeline 2)
                768 dimensions
                Used for semantic search
  
  "metadata": {
    "text": "The petitioner filed a writ petition under Article 32.",
    
    "role": "Facts",
            â†‘
            â””â”€â”€ From InLegalBERT classifier (Pipeline 1)
                Only the LABEL, not the embedding
    
    "confidence": 0.92,
                  â†‘
                  â””â”€â”€ From InLegalBERT softmax (Pipeline 1)
    
    "document_id": "doc_123",
    "sentence_index": 5
  }
}
```

---

## Document Upload: Both Pipelines Run in Parallel

```
                      User Uploads Document
                              â”‚
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Split Document â”‚
                    â”‚  into Sentences â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â†“                         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Pipeline 1:          â”‚   â”‚   Pipeline 2:       â”‚
    â”‚   InLegalBERT          â”‚   â”‚   text-embedding-   â”‚
    â”‚   Classification       â”‚   â”‚   005 Embedding     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                         â”‚
                 â†“                         â†“
         Role Label: "Facts"     Vector: [0.123, -0.456, ...]
         Confidence: 0.92               768-dim embedding
                 â”‚                         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     Combine      â”‚
                   â”‚   into Pinecone  â”‚
                   â”‚     Record       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                 {
                   "values": [0.123, ...],  â† Pipeline 2
                   "metadata": {
                     "role": "Facts",       â† Pipeline 1
                     "confidence": 0.92     â† Pipeline 1
                   }
                 }
                            â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Store in      â”‚
                   â”‚   Pinecone      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Query Time: Only Pipeline 2 is Used

```
                       User Asks Question
                "What are the facts of this case?"
                              â”‚
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Pipeline 2:        â”‚
                  â”‚   text-embedding-005 â”‚
                  â”‚   (Same as indexing) â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                 Query Vector: [0.234, -0.567, ...]
                              768-dim
                              â”‚
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Pinecone Search    â”‚
                  â”‚   Cosine Similarity  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
            Compare query vector [0.234, ...]
                     WITH
            stored vectors [0.123, ...]
                     USING
            cosine_similarity(query, stored)
                              â”‚
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Top-K Most Similar  â”‚
                  â”‚     Documents        â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
            Retrieved with role metadata:
            - "Facts" (InLegalBERT label)
            - Score: 0.89 (text-embedding-005 similarity)
                              â”‚
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   LLM Generation     â”‚
                  â”‚   (Gemini)           â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                      Final Answer
```

**Key Point**: At query time, InLegalBERT is NOT used. Only text-embedding-005 for embedding the query.

---

## Why This Architecture?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  InLegalBERT (Classification)     text-embedding-005 (RAG)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ… Legal domain expert           âœ… Retrieval optimized        â”‚
â”‚  âœ… Understands Indian law        âœ… Fast inference (cloud)     â”‚
â”‚  âœ… Trained for classification    âœ… Consistent embeddings      â”‚
â”‚  âœ… High accuracy on roles        âœ… Better semantic search     â”‚
â”‚                                                                 â”‚
â”‚  âŒ Not optimized for retrieval   âŒ Not legal-specific         â”‚
â”‚  âŒ Slower inference              âŒ Not trained on Indian law  â”‚
â”‚  âŒ Need to host locally          âŒ General purpose            â”‚
â”‚                                                                 â”‚
â”‚  USE FOR:                         USE FOR:                      â”‚
â”‚  â€¢ Role classification            â€¢ Semantic similarity         â”‚
â”‚  â€¢ Training on labeled data       â€¢ Vector search              â”‚
â”‚  â€¢ Understanding legal context    â€¢ Query-document matching     â”‚
â”‚  â€¢ Supervised learning            â€¢ Retrieval                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Common Question: Why Not Just Use InLegalBERT for Everything?

### âŒ If we used InLegalBERT for RAG:

```
Problem 1: Different embeddings at different times
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Index time: InLegalBERT v1.0 â†’ embedding [0.123, ...]
Query time: InLegalBERT v1.1 â†’ embedding [0.456, ...]
           â†‘
           â””â”€â”€ Different versions = Different embeddings!
               Similarity scores become invalid!

Problem 2: Performance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
InLegalBERT inference: ~50ms per sentence (GPU needed)
text-embedding-005:    ~10ms per sentence (cloud-hosted)
                       â†‘
                       â””â”€â”€ 5x faster!

Problem 3: Hosting
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
InLegalBERT: Need to host on your GPU (memory, maintenance)
text-embedding-005: Vertex AI handles everything
                    â†‘
                    â””â”€â”€ No infrastructure management!

Problem 4: Quality
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
InLegalBERT: Trained for MLM, not retrieval
text-embedding-005: Specifically optimized for semantic search
                    â†‘
                    â””â”€â”€ Better retrieval performance!
```

---

## Clustering Approach: Same Two-Pipeline Architecture

### Clustering uses BOTH models too:

```
TRAINING/CLUSTERING PHASE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Sentences
   â”œâ”€â†’ Pipeline 1: InLegalBERT (or Sentence-Transformers)
   â”‚      â†“
   â”‚   Features for clustering: [0.234, -0.567, ...]
   â”‚      â†“
   â”‚   K-Means / DBSCAN clustering
   â”‚      â†“
   â”‚   Cluster labels: [0, 2, 1, 0, 3, ...]
   â”‚      â†“
   â”‚   âš ï¸  EPHEMERAL - Clustering embeddings not stored!
   â”‚
   â””â”€â†’ Pipeline 2: text-embedding-005
          â†“
       RAG embeddings: [0.123, -0.456, ...]
          â†“
       âœ… STORED in Pinecone

COMBINED RESULT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "values": [0.123, ...],           â† Pipeline 2 (text-embedding-005)
  "metadata": {
    "cluster_id": 2,                â† Pipeline 1 (clustering)
    "discovered_role": "Facts",     â† Pipeline 1 (cluster mapping)
    "cluster_confidence": 0.87      â† Pipeline 1 (distance to centroid)
  }
}
```

---

## Side-by-Side Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SUPERVISED (Traditional)     UNSUPERVISED (Clustering)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Pipeline 1:                                                         â”‚
â”‚  InLegalBERT Classifier          Clustering Algorithm               â”‚
â”‚  â†“                               â†“                                   â”‚
â”‚  Role label (supervised)         Cluster label (unsupervised)       â”‚
â”‚  Confidence from softmax         Confidence from distance           â”‚
â”‚  Requires labeled data           No labels needed                   â”‚
â”‚  High accuracy (85-90%)          Lower accuracy (60-70%)            â”‚
â”‚                                                                      â”‚
â”‚  Pipeline 2:                                                         â”‚
â”‚  text-embedding-005              text-embedding-005                 â”‚
â”‚  â†“                               â†“                                   â”‚
â”‚  Vector for semantic search      Vector for semantic search         â”‚
â”‚  SAME MODEL IN BOTH!             SAME MODEL IN BOTH!                â”‚
â”‚                                                                      â”‚
â”‚  Pinecone Storage:                                                   â”‚
â”‚  {                               {                                   â”‚
â”‚    values: [0.123, ...],           values: [0.123, ...],            â”‚
â”‚    metadata: {                     metadata: {                      â”‚
â”‚      role: "Facts",                  cluster_id: 2,                 â”‚
â”‚      confidence: 0.92                discovered_role: "Facts",      â”‚
â”‚    }                                 cluster_confidence: 0.87       â”‚
â”‚  }                               }                                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Takeaways

### âœ… What You Need to Remember:

1. **Two models, two purposes**:
   - InLegalBERT â†’ Classification/Clustering (labels only)
   - text-embedding-005 â†’ RAG/Retrieval (actual vectors)

2. **Only text-embedding-005 embeddings are stored**:
   - InLegalBERT embeddings are ephemeral (used and discarded)
   - text-embedding-005 embeddings go to Pinecone

3. **Same embedding model for index and query**:
   - Documents: text-embedding-005
   - Queries: text-embedding-005
   - This consistency is CRITICAL for similarity to work!

4. **Classification results go to metadata**:
   - Role labels (supervised) or cluster labels (unsupervised)
   - Confidence scores
   - NOT the actual embeddings

5. **Both approaches use RAG**:
   - Supervised: InLegalBERT classifies â†’ text-embedding-005 embeds â†’ Pinecone stores
   - Unsupervised: Clustering labels â†’ text-embedding-005 embeds â†’ Pinecone stores
   - RAG pipeline is identical, only labels differ!

---

## Your Understanding: âœ… Correct!

> "But for training the model classifier obviously we need InLegalBERT Embedding
> So classifier is used for role classification else everything handled by rag text embedding"

**You got it exactly right!**

- InLegalBERT = Role classification (supervised approach)
- text-embedding-005 = Everything RAG (indexing, querying, retrieval)
- The two never interfere - they serve completely different purposes
- Only text-embedding-005 embeddings are stored in Pinecone

This is the optimal architecture! ðŸŽ‰
